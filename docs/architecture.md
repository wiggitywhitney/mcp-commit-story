# MCP Commit Story Architecture

## Overview

MCP Commit Story follows a **background generation architecture** with automatic journal creation triggered by git commits. This design prioritizes seamless workflow integration and user control over manual operations.

## Architecture Rationale

### Why Background Generation?

1. **Workflow Respect**: No interruption to development flow - journal entries are created silently after commits
2. **Fresh Context**: Each journal entry is generated by a fresh AI agent with comprehensive context, ensuring quality without dependency on persistent state
3. **User Control**: Developers control when and how context is added through manual reflection tools and context capture
4. **Reliability**: Direct git hook execution eliminates complex intermediate mechanisms and dependency chains
5. **Simplicity**: Clear, linear flow from commit → context collection → journal generation

### Core Architecture

```
┌─────────────────┐    ┌──────────────────┐    ┌─────────────────┐
│   Git Commit    │    │ Standalone       │    │   Journal       │
│   (Trigger)     │───▶│ Generator        │───▶│   Entry         │
│                 │    │ (Background)     │    │   Created       │
└─────────────────┘    └──────────────────┘    └─────────────────┘
                              │
                              ▼
                       ┌──────────────────┐
                       │ Context Sources  │
                       │ • Git diff/log   │
                       │ • Chat history   │
                       │ • Recent journals│
                       │ • Project README │
                       └──────────────────┘
```

## Background Generation Flow

### 1. Git Hook Trigger
- **Post-commit hook** executes after each commit
- **Direct execution** of standalone journal generator
- **No intermediate files** or complex coordination required
- **Never blocks** git operations

### 2. Context Collection
- **Git Context**: Commit metadata, diffs, file changes (always available)
- **Chat History**: Cursor SQLite database extraction with intelligent filtering
- **Recent Journals**: Today's journal + 2 most recent daily entries for continuity
- **Project Context**: README (or configured overview file) ensures AI always understands project goals

### 3. Intelligent Chat Parsing
- **Git-driven filtering**: Extract keywords and concepts from git diffs
- **Relevance scoring**: Match chat content against code changes
- **Boundary detection**: Identify relevant chat sessions and conversations
- **Message limiting**: 200/200 limits act as safety net after 48-hour filtering
- **Context optimization**: Return only chat segments related to the work

### 4. Fresh AI Generation
- **New AI instance** for each journal entry (no persistent context)
- **Comprehensive input**: All collected context provided to AI
- **Structured output**: Human-readable + machine-tagged journal entries
- **Quality focus**: Grounded in real data, captures emotion and decisions

### 5. Automatic Summaries
- **Daily summaries**: Generated automatically when date boundaries are detected
- **Future summaries**: Weekly, monthly, quarterly, yearly (planned)
- **Same pattern**: Background generation using recent journal entries as input

## MCP Server Role

The MCP server provides **interactive tools only**:

### User-Controlled Operations
- **journal/add-reflection**: Add manual thoughts and reflections
- **journal/capture-context**: Capture AI assistant's current knowledge
- **journal/generate-summary**: Manual summary generation (if needed)

### Setup Operations  
- **journal/init**: Initialize journal structure
- **journal/install-hook**: Install git hooks

**Key Point**: The MCP server is NOT required for core journal generation - it runs independently in the background.

## 5-Layer Standalone Journal Generator Architecture

The journal generation process follows a **5-layer architecture** that separates concerns between data collection, AI processing, and content generation:

### Layer 1: Context Collection (Programmatic)
- **Responsibility**: Gather raw data without AI interpretation
- **Key Functions**:
  - `collect_git_context(commit_hash)` - Git metadata, diffs, and commit info
  - `collect_chat_history()` - Raw prompts/responses from cursor_db (separate databases)
  - `collect_journal_context()` - Existing journal entries, reflections, and manual context

These functions are pure data extraction with no AI involved.

### Layer 2: Conversation Reconstruction (AI-Powered)
- **Responsibility**: Intelligently merge separate chat databases using AI
- **Key Functions**:
  - `reconstruct_conversation(raw_chat_data)` - Uses AI to merge user prompts and AI responses
  - Handles mismatched counts, missing responses, multiple prompts
  - Returns unified conversation flow for downstream generators
  - **First AI invocation** in the pipeline

### Layer 3: Orchestration (Coordination)
- **Module**: `standalone_generator.py`
- **Responsibility**: Coordinate the entire journal generation flow
- **Key Functions**:
  - `generate_journal_entry_standalone()` - Main orchestration function
  - Calls all context collectors to gather raw data
  - Invokes conversation reconstruction (Layer 2) to unify chat data
  - Builds the `JournalContext` structure with all collected data
  - Determines which generators need AI vs programmatic execution
  - Assembles complete journal entry from generated sections
  - Handles errors gracefully - if one section fails, others continue

### Layer 4: Section Generators (Mixed AI and Programmatic)
- **Responsibility**: Generate specific journal entry sections
- **Mixed Execution Model**:

**Programmatic Generators** (no AI required):
- `generate_commit_metadata_section()` - Pure git data extraction
- `generate_technical_synopsis_section()` - Code change analysis
- `generate_file_changes_section()` - Git diff analysis

**AI-Powered Generators** (require AI interpretation):
- `generate_summary_section()` - Narrative summary of changes
- `generate_accomplishments_section()` - Achievement interpretation
- `generate_frustrations_section()` - Challenge identification
- `generate_tone_mood_section()` - Emotional indicator detection
- `generate_discussion_notes_section()` - Key conversation excerpts
- `generate_decision_points_section()` - **NEW** - Decision moment identification

Each AI generator has a docstring prompt and returns a placeholder for AI execution.

### Layer 5: AI Invocation (Infrastructure)
- **Responsibility**: Execute AI-powered components with graceful degradation
- **Two AI Invocation Points**:

1. **Conversation Reconstruction** (Layer 2):
   - AI analyzes separate prompt/response databases
   - Intelligently matches and merges them chronologically
   - Handles edge cases and data mismatches

2. **Section Generation** (Layer 4):
   - `execute_ai_function(func, context)` - Executes AI-powered generators
   - Reads docstring prompts and formats context
   - Sends to AI provider and parses responses
   - Provides graceful degradation (returns empty sections if AI unavailable)



## Complete Data Flow

```
1. Git hook triggers → process_git_hook()
2. Orchestrator called → generate_journal_entry_standalone()
3. Context collectors gather → git data, chat history (raw), journal content
4. **AI Call #1**: Conversation reconstruction → unified chat from separate databases
5. Build JournalContext with reconstructed conversation
6. For each generator:
   - Programmatic ones: execute directly
   - **AI Call #2+**: AI generators via executor
7. Assembly → sections combined into complete journal entry
8. Save → journal entry written to daily file
```

### Standalone Journal Generator (Layer 3 Implementation)
```python
def generate_journal_entry_standalone(commit_hash: Optional[str] = None, hook_type: str = 'post-commit') -> bool:
    """Main orchestration function using 5-layer architecture"""
    try:
        # Layer 1: Collect all raw context
        git_context = collect_git_context(commit_hash)
        raw_chat_data = collect_chat_history()
        journal_context = collect_journal_context()
        
        # Layer 2: Reconstruct conversation from raw chat data
        conversation_history = []
        if raw_chat_data:
            try:
                conversation_history = reconstruct_conversation(raw_chat_data)  # AI Call #1
            except Exception as e:
                log_error(f"Failed to reconstruct conversation: {str(e)}")
                # Continue with empty conversation history
        
        # Build the complete journal context
        context = JournalContext(
            git_context=git_context,
            conversation_history=conversation_history,
            journal_context=journal_context,
            hook_type=hook_type
        )
        
        # Layer 4: Generate all sections using mixed execution
        journal_sections = {}
        
        # Execute programmatic generators directly
        for section_name, generator_func in PROGRAMMATIC_GENERATORS.items():
            try:
                journal_sections[section_name] = generator_func(context)
            except Exception as e:
                log_error(f"Failed to generate {section_name} section: {str(e)}")
                journal_sections[section_name] = {}
        
        # Execute AI generators via Layer 5 (AI invocation infrastructure)
        for section_name, generator_func in AI_GENERATORS.items():
            try:
                if is_ai_available():
                    journal_sections[section_name] = execute_ai_function(generator_func, context)  # AI Calls #2+
                else:
                    log_info(f"AI unavailable, skipping {section_name} section")
                    journal_sections[section_name] = {}
            except Exception as e:
                log_error(f"Failed to generate {section_name} section: {str(e)}")
                journal_sections[section_name] = {}
        
        # Save the complete journal entry
        save_journal_entry(journal_sections)
        return True
        
    except Exception as e:
        log_error(f"Journal generation failed: {str(e)}")
        return False
```

### Generator Registry (Layer 4 Implementation)
```python
# Define which generators are AI-powered vs programmatic
PROGRAMMATIC_GENERATORS = {
    'metadata': generate_commit_metadata_section,
    'technical_synopsis': generate_technical_synopsis_section,
    'file_changes': generate_file_changes_section,
}

AI_GENERATORS = {
    'summary': generate_summary_section,
    'accomplishments': generate_accomplishments_section,
    'frustrations': generate_frustrations_section,
    'tone_mood': generate_tone_mood_section,
    'discussion_notes': generate_discussion_notes_section,
    'decision_points': generate_decision_points_section,  # NEW
}
```

## Benefits of This Architecture

### For Developers
- **Zero Friction**: Commit and forget - journal entries appear automatically
- **No Workflow Disruption**: Background generation never interrupts work
- **Full Control**: Add context when needed, ignore when focused
- **Rich Context**: Every entry has comprehensive, relevant information
- **Decision Capture**: New decision points section captures architectural and design choices

### For Quality
- **Fresh Perspective**: Each entry generated by new AI instance with full context
- **Grounded Content**: All entries based on real git changes and conversations  
- **Intelligent Conversation Reconstruction**: AI merges separate chat databases chronologically
- **Mixed Execution**: Programmatic sections provide reliable data, AI sections add interpretation
- **Consistent Format**: Standardized structure across all entries

### For Reliability
- **Layered Concerns**: Clear separation between data collection, AI processing, and content generation
- **Graceful Degradation**: System works even when AI unavailable (programmatic sections continue)
- **Error Isolation**: If one section fails, others continue processing
- **No Dependencies**: Core functionality works without MCP server
- **Fast Execution**: Optimized with programmatic processing where possible

### For Maintainability
- **Separation of Concerns**: Each layer has a single, well-defined responsibility
- **Testable Components**: Layers can be tested independently with appropriate mocks
- **Extensible Design**: New generators can be added to either programmatic or AI categories
- **Clear Data Flow**: Linear progression through layers with explicit boundaries

## User Experience Patterns

### Automatic Operation (Primary)
```bash
# Developer works normally
git add .
git commit -m "Implement user authentication"
# → Journal entry automatically created in background
# → Daily summary generated if date boundary crossed
```

### Manual Context Addition (When Needed)
```bash
# Via MCP tools in editor/AI assistant
journal/add-reflection "Learned about JWT security considerations"
journal/capture-context  # Captures current AI conversation context
```

### Summary Generation
```bash
# Automatic daily summaries when date changes
# Manual generation available via MCP if needed
journal/generate-summary --type=daily --date=2025-06-14
```

## Future Architecture Considerations

### Planned Enhancements
- **Multi-timeframe summaries**: Weekly, monthly, quarterly, yearly
- **Pattern recognition**: Identify recurring themes and challenges
- **Content suggestions**: Recommend blog post topics from journal patterns
- **Integration hooks**: Export to external systems (blogs, documentation)

### Scalability
- **Repository isolation**: Each repo maintains independent journal
- **Context optimization**: Intelligent pruning of large chat histories
- **Performance monitoring**: Telemetry for background generation timing
- **Resource management**: Configurable limits for AI processing

## Related Documentation

- **[MCP API Specification](mcp-api-specification.md)** - Interactive tool reference
- **[Implementation Guide](implementation-guide.md)** - Technical implementation details
- **[Journal Behavior](journal-behavior.md)** - Content generation rules
- **[Context Collection](context-collection.md)** - Data gathering system
- **[Testing Standards](testing_standards.md)** - Quality assurance