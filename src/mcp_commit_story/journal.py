import re
import logging
from typing import List, Optional, Dict, Union, Any
from pathlib import Path
import os
from mcp_commit_story.context_types import ChatHistory, TerminalContext, SummarySection, TechnicalSynopsisSection, JournalContext, AccomplishmentsSection, FrustrationsSection, ToneMoodSection, DiscussionNotesSection, TerminalCommandsSection, CommitMetadataSection
from .telemetry import (
    trace_mcp_operation, 
    get_mcp_metrics, 
    sanitize_for_telemetry
)

logger = logging.getLogger(__name__)

# TechnicalSynopsisSection: Represents the technical synopsis section of a journal entry.
# This section provides a code-focused analysis of what changed, generated by an AI-driven function pattern.
# The function returns a placeholder; the AI agent is expected to execute the docstring prompt and fill in the content.

"""
Journal entry generation for engineering work.

Content Quality Guidelines:
- Focus on signal (unique insights, decisions, challenges) over noise (routine procedures)
- Highlight what makes each entry unique rather than repeating standard practices
- Capture the narrative "story" behind the code changes
- Include emotional context when relevant, but only with clear supporting evidence
- Omit standard workflow details unless they're directly relevant to understanding the work
"""

class JournalParseError(Exception):
    pass

def _add_ai_generation_telemetry(section_type: str, journal_context, start_time: float):
    """
    Utility function to add consistent telemetry for AI generation operations.
    
    Args:
        section_type: The type of section being generated (e.g., 'summary', 'accomplishments')
        journal_context: The journal context being processed
        start_time: The timestamp when generation started
    """
    from opentelemetry import trace
    
    # Add semantic conventions for AI generation telemetry
    current_span = trace.get_current_span()
    if current_span:
        # Calculate context size
        context_size = 0
        if journal_context:
            if hasattr(journal_context, 'chat_history') and journal_context.get('chat_history'):
                context_size += len(journal_context['chat_history'].get('messages', []))
            if hasattr(journal_context, 'terminal_context') and journal_context.get('terminal_context'):
                context_size += len(journal_context['terminal_context'].get('commands', []))
            if hasattr(journal_context, 'file_changes') and journal_context.get('file_changes'):
                context_size += len(journal_context['file_changes'])
        
        current_span.set_attribute("journal.context_size", context_size)
        current_span.set_attribute("journal.entry_id", journal_context.get('commit_hash', 'unknown') if journal_context else 'unknown')
        # AI model info would be added here if available from context

def _record_ai_generation_metrics(section_type: str, duration: float, success: bool, error_category: str = None):
    """
    Utility function to record AI generation metrics consistently.
    
    Args:
        section_type: The type of section being generated
        duration: Time taken for generation
        success: Whether the operation succeeded
        error_category: Category of error if operation failed
    """
    from opentelemetry import trace
    
    metrics = get_mcp_metrics()
    if metrics:
        if success:
            metrics.record_operation_duration(
                "journal.ai_generation_duration_seconds",
                duration,
                section_type=section_type,
                operation_type="ai_generation"
            )
        
        metrics.record_tool_call(
            "journal.generation_operations_total",
            success=success,
            section_type=section_type
        )
    
    if not success and error_category:
        current_span = trace.get_current_span()
        if current_span:
            current_span.set_attribute("error.category", error_category)

def log_ai_agent_interaction(context_sent: Any, response_received: Any, debug_mode: bool = False):
    """
    Simple logging of AI interactions for debugging integration issues.
    
    This utility function provides visibility into AI agent interactions without
    interfering with normal operation. Useful for troubleshooting when AI responses
    don't match expectations or when debugging context size issues.
    
    Args:
        context_sent: The context/prompt data sent to the AI
        response_received: The response received from the AI
        debug_mode: Whether to log debug information (can be set via environment)
    
    Usage:
        # In AI generation functions:
        result = ai_generate_section(context)
        log_ai_agent_interaction(context, result, debug_mode=True)
        return result
    """
    # Check environment variable if debug_mode not explicitly set
    if not debug_mode:
        debug_mode = os.getenv('MCP_DEBUG_AI_INTERACTIONS', 'false').lower() in ('true', '1', 'yes')
    
    if debug_mode:
        context_size = len(str(context_sent)) if context_sent else 0
        response_type = type(response_received).__name__
        response_size = len(str(response_received)) if response_received else 0
        
        logger.debug(f"AI Interaction Debug:")
        logger.debug(f"  Context size: {context_size} characters")
        logger.debug(f"  Response type: {response_type}")
        logger.debug(f"  Response size: {response_size} characters")
        
        # Add telemetry if available
        metrics = get_mcp_metrics()
        if metrics:
            metrics.record_counter(
                "ai_interactions_logged_total",
                1,
                attributes={
                    "context_size_bucket": _get_size_bucket(context_size),
                    "response_type": response_type
                }
            )

def _get_size_bucket(size: int) -> str:
    """Helper function to bucket sizes for telemetry."""
    if size < 1000:
        return "small"
    elif size < 10000:
        return "medium" 
    elif size < 100000:
        return "large"
    else:
        return "xlarge"

class JournalEntry:
    """
    Represents a single engineering journal entry, with Markdown serialization.
    Only non-empty sections are included in output.

    Content should prioritize unique insights and developments over routine
    workflow steps. The goal is to create entries that provide value when
    reviewed in the future, focusing on "why" and "how" rather than just "what".
    """

    def __init__(
        self,
        timestamp: str,
        commit_hash: str,
        summary: Optional[str] = None,
        technical_synopsis: Optional[str] = None,
        accomplishments: Optional[List[str]] = None,
        frustrations: Optional[List[str]] = None,
        terminal_commands: Optional[List[str]] = None,
        discussion_notes: Optional[List[Union[str, Dict[str, str]]]] = None,
        tone_mood: Optional[Dict[str, str]] = None,  # {'mood': str, 'indicators': str}
        commit_metadata: Optional[Dict[str, str]] = None,
    ):
        self.timestamp = timestamp
        self.commit_hash = commit_hash
        self.summary = summary
        self.technical_synopsis = technical_synopsis
        self.accomplishments = accomplishments or []
        self.frustrations = frustrations or []
        self.terminal_commands = terminal_commands or []
        self.discussion_notes = discussion_notes or []
        self.tone_mood = tone_mood
        self.commit_metadata = commit_metadata or {}

    @trace_mcp_operation("journal.serialize_entry", attributes={"operation_type": "serialization", "file_type": "markdown"})
    def to_markdown(self) -> str:
        """
        Serialize the journal entry to Markdown with improved formatting:
        - H3 for entry header
        - H4 for section headers
        - Blank line after section headers
        - Blank line between bullet points
        - Blank line on speaker change in discussion notes
        - Terminal commands as a single bash code block
        - Blockquotes visually distinct
        - (Horizontal rule between entries is handled externally)
        """
        import time
        from opentelemetry import trace
        
        start_time = time.time()
        
        # Add semantic conventions for telemetry
        current_span = trace.get_current_span()
        if current_span:
            current_span.set_attribute("journal.entry_id", self.commit_hash)
            current_span.set_attribute("journal.timestamp", self.timestamp)
        
        try:
            lines = [f"### {self.timestamp} — Commit {self.commit_hash}", ""]

            def section(header, content_lines):
                if not content_lines:
                    return []
                out = [f"#### {header}", ""]
                out.extend(content_lines)
                out.append("")
                return out

            # 1. Summary
            if self.summary:
                lines += section("Summary", [self.summary])

            # 2. Technical Synopsis
            if self.technical_synopsis:
                lines += section("Technical Synopsis", [self.technical_synopsis])

            # 3. Accomplishments
            if self.accomplishments:
                acc_lines = []
                for i, item in enumerate(self.accomplishments):
                    acc_lines.append(f"- {item}")
                    if i < len(self.accomplishments) - 1:
                        acc_lines.append("")  # blank line between bullets
                lines += section("Accomplishments", acc_lines)

            # 4. Frustrations or Roadblocks
            if self.frustrations:
                frus_lines = []
                for i, item in enumerate(self.frustrations):
                    frus_lines.append(f"- {item}")
                    if i < len(self.frustrations) - 1:
                        frus_lines.append("")
                lines += section("Frustrations or Roadblocks", frus_lines)

            # 5. Tone/Mood
            if isinstance(self.tone_mood, dict):
                mood = self.tone_mood.get("mood", "").strip()
                indicators = self.tone_mood.get("indicators", "").strip()
                if mood or indicators:
                    tm_lines = []
                    if mood:
                        tm_lines.append(f"> {mood}")
                    if indicators:
                        tm_lines.append(f"> {indicators}")
                    if tm_lines:
                        lines += section("Tone/Mood", tm_lines)

            # 6. Discussion Notes (from chat)
            if self.discussion_notes:
                dn_lines = []
                prev_speaker = None
                for note in self.discussion_notes:
                    if isinstance(note, dict) and 'speaker' in note and 'text' in note:
                        speaker = note['speaker']
                        text_lines = note['text'].splitlines()
                        if prev_speaker is not None and speaker != prev_speaker:
                            dn_lines.append("")  # blank line on speaker change
                        if text_lines:
                            dn_lines.append(f"> **{speaker}:** {text_lines[0]}")
                            for l in text_lines[1:]:
                                dn_lines.append(f"> {l}")
                        else:
                            dn_lines.append(f"> **{speaker}:**")
                        prev_speaker = speaker
                    else:
                        text_lines = str(note).splitlines()
                        for l in text_lines:
                            dn_lines.append(f"> {l}")
                lines += section("Discussion Notes (from chat)", dn_lines)

            # 7. Terminal Commands (AI Session)
            if self.terminal_commands:
                tc_lines = ["Commands executed by AI during this work session:", "```bash"]
                tc_lines.extend(self.terminal_commands)
                tc_lines.append("```")
                lines += section("Terminal Commands (AI Session)", tc_lines)

            # 8. Commit Metadata
            if self.commit_metadata:
                btc_lines = [f"- **{k}:** {v}" for k, v in self.commit_metadata.items()]
                lines += section("Commit Metadata", btc_lines)

            # Remove trailing blank lines
            while lines and lines[-1] == "":
                lines.pop()
            
            result = "\n".join(lines).strip()
            
            # Record successful serialization metrics
            duration = time.time() - start_time
            metrics = get_mcp_metrics()
            if metrics:
                metrics.record_operation_duration(
                    "journal.serialize_duration_seconds",
                    duration,
                    operation_type="serialize",
                    file_type="markdown"
                )
                metrics.record_tool_call(
                    "journal.serialize_operations_total",
                    success=True,
                    operation_type="serialize"
                )
                
                # Add output size for telemetry
                if current_span:
                    current_span.set_attribute("journal.output_length", len(result))
            
            return result
            
        except Exception as e:
            # Record error metrics
            duration = time.time() - start_time
            metrics = get_mcp_metrics()
            if metrics:
                metrics.record_tool_call(
                    "journal.serialize_operations_total",
                    success=False,
                    operation_type="serialize"
                )
            
            if current_span:
                current_span.set_attribute("error.category", "serialization_failed")
            
            raise

class JournalParser:
    @staticmethod
    @trace_mcp_operation("journal.parse_entry", attributes={"operation_type": "file_read", "file_type": "markdown"})
    def parse(md):
        """
        Parse markdown content into a JournalEntry object.
        
        Args:
            md: Markdown content to parse
            
        Returns:
            JournalEntry: Parsed journal entry
            
        Raises:
            JournalParseError: If the markdown cannot be parsed
        """
        import time
        from opentelemetry import trace
        
        start_time = time.time()
        
        # Add semantic conventions for telemetry
        current_span = trace.get_current_span()
        if current_span:
            current_span.set_attribute("journal.content_length", len(md) if md else 0)
            current_span.set_attribute("journal.operation_type", "parse")
        
        try:
            if not md or not md.strip():
                if current_span:
                    current_span.set_attribute("error.category", "empty_content")
                raise JournalParseError('Empty entry')
            
            # Parse H4 (####) headers for all sections
            def extract_section(header):
                pattern = rf"#### {header}\n(.+?)(?=\n#### |\Z)"
                m = re.search(pattern, md, re.DOTALL)
                return m.group(1).strip() if m else ''
            
            timestamp_commit = re.search(r"###\s+(.*?) — Commit ([a-zA-Z0-9]+)", md)
            if timestamp_commit:
                timestamp = timestamp_commit.group(1)
                commit_hash = timestamp_commit.group(2)
                
                # Add parsed commit info to span
                if current_span:
                    current_span.set_attribute("journal.entry_id", commit_hash)
                    current_span.set_attribute("journal.timestamp", timestamp)
                
                summary = extract_section("Summary")
                technical_synopsis = extract_section("Technical Synopsis")
                
                # Accomplishments
                accomplishments = []
                acc_section = extract_section("Accomplishments")
                if acc_section:
                    accomplishments = [line[2:].strip() for line in acc_section.splitlines() if line.startswith('- ')]
                
                # Frustrations
                frustrations = []
                frus_section = extract_section("Frustrations or Roadblocks")
                if frus_section:
                    frustrations = [line[2:].strip() for line in frus_section.splitlines() if line.startswith('- ')]
                
                # Tone/Mood
                tone_mood = None
                tm_section = extract_section("Tone/Mood")
                if tm_section:
                    tm_lines = [l.strip('> ').strip() for l in tm_section.splitlines() if l.strip().startswith('>')]
                    mood = tm_lines[0] if len(tm_lines) >= 1 else ''
                    indicators = tm_lines[1] if len(tm_lines) >= 2 else ''
                    if mood or indicators:
                        tone_mood = {"mood": mood, "indicators": indicators}
                    else:
                        tone_mood = None
                
                # Discussion Notes
                discussion_notes = []
                dn_section = extract_section("Discussion Notes (from chat)")
                if dn_section:
                    for l in dn_section.splitlines():
                        l = l.strip()
                        if l.startswith('> **'):
                            # Speaker-attributed
                            m = re.match(r'> \*\*(.+?):\*\* (.+)', l)
                            if m:
                                discussion_notes.append({"speaker": m.group(1), "text": m.group(2)})
                        elif l.startswith('> '):
                            discussion_notes.append(l[2:])
                
                # Terminal Commands
                terminal_commands = []
                tc_section = extract_section("Terminal Commands (AI Session)")
                if tc_section:
                    in_block = False
                    for l in tc_section.splitlines():
                        if l.strip() == '```bash':
                            in_block = True
                            continue
                        if l.strip() == '```':
                            in_block = False
                            continue
                        if in_block:
                            terminal_commands.append(l)
                
                # Commit Metadata
                commit_metadata = {}
                cm_section = extract_section("Commit Metadata")
                if cm_section:
                    for l in cm_section.splitlines():
                        l = l.strip()
                        if l.startswith('- **') and ':** ' in l:
                            k, v = l[4:].split(':** ', 1)
                            commit_metadata[k.strip()] = v.strip()
                
                # Count parsed sections for metrics
                section_count = sum([
                    1 if summary else 0,
                    1 if technical_synopsis else 0,
                    1 if accomplishments else 0,
                    1 if frustrations else 0,
                    1 if tone_mood else 0,
                    1 if discussion_notes else 0,
                    1 if terminal_commands else 0,
                    1 if commit_metadata else 0
                ])
                
                if current_span:
                    current_span.set_attribute("journal.sections_parsed", section_count)
                
                # Record successful parsing metrics
                duration = time.time() - start_time
                metrics = get_mcp_metrics()
                if metrics:
                    metrics.record_operation_duration(
                        "journal.parse_duration_seconds",
                        duration,
                        operation_type="parse",
                        file_type="markdown"
                    )
                    metrics.record_tool_call(
                        "journal.parse_operations_total",
                        success=True,
                        operation_type="parse"
                    )
                
                return JournalEntry(
                    timestamp=timestamp,
                    commit_hash=commit_hash,
                    summary=summary,
                    technical_synopsis=technical_synopsis,
                    accomplishments=accomplishments,
                    frustrations=frustrations,
                    tone_mood=tone_mood,
                    discussion_notes=discussion_notes,
                    terminal_commands=terminal_commands,
                    commit_metadata=commit_metadata,
                )
            
            # Failed to parse - invalid format
            if current_span:
                current_span.set_attribute("error.category", "invalid_format")
            
            duration = time.time() - start_time
            metrics = get_mcp_metrics()
            if metrics:
                metrics.record_tool_call(
                    "journal.parse_operations_total",
                    success=False,
                    operation_type="parse"
                )
            
            raise JournalParseError('Unrecognized journal entry format')
            
        except JournalParseError:
            # Re-raise parse errors as-is
            raise
        except Exception as e:
            # Record unexpected error metrics
            duration = time.time() - start_time
            metrics = get_mcp_metrics()
            if metrics:
                metrics.record_tool_call(
                    "journal.parse_operations_total",
                    success=False,
                    operation_type="parse"
                )
            
            if current_span:
                current_span.set_attribute("error.category", "parse_exception")
            
            raise JournalParseError(f'Parse error: {e}')

@trace_mcp_operation("journal.get_file_path", attributes={"operation_type": "file_path_generation"})
def get_journal_file_path(date: str, entry_type: str) -> str:
    """
    Generate the file path for a journal entry with enhanced telemetry and sensitive data filtering.
    
    Args:
        date: Date string in ISO format (e.g., '2025-05-14', '2025-05-01_07', '2025-05', '2025')
        entry_type: Type of journal entry (e.g., 'daily', 'daily_summary', 'weekly_summary', 'monthly_summary', 'yearly_summary')
        
    Returns:
        str: Relative path to the journal file
    """
    import time
    from opentelemetry import trace
    
    start_time = time.time()
    
    # Add semantic conventions for telemetry with sanitization
    current_span = trace.get_current_span()
    if current_span:
        # Use sanitized values for potentially sensitive data
        current_span.set_attribute("journal.entry_type", sanitize_for_telemetry(entry_type))
        current_span.set_attribute("journal.date", sanitize_for_telemetry(date))
        current_span.set_attribute("operation_type", "file_path_generation")
    
    try:
        # Generate file path based on entry type
        if entry_type == "daily":
            filename = f"{date}-journal.md"
            file_path = f"journal/daily/{filename}"
        elif entry_type == "daily_summary":
            filename = f"{date}-daily.md"
            file_path = f"journal/summaries/daily/{filename}"
        elif entry_type == "weekly_summary":
            filename = f"{date}-weekly.md"
            file_path = f"journal/summaries/weekly/{filename}"
        elif entry_type == "monthly_summary":
            filename = f"{date}-monthly.md"
            file_path = f"journal/summaries/monthly/{filename}"
        elif entry_type == "yearly_summary":
            filename = f"{date}-yearly.md"
            file_path = f"journal/summaries/yearly/{filename}"
        else:
            # Default pattern for unknown entry types
            filename = f"{entry_type}_{date}.md"
            file_path = f"journal/{filename}"
        
        # Add sanitized path information (filename only, not full path)
        if current_span:
            current_span.set_attribute("file.name", sanitize_for_telemetry(filename))
            current_span.set_attribute("directory.name", sanitize_for_telemetry(os.path.dirname(file_path)))
        
        # Record successful path generation metrics
        duration = time.time() - start_time
        metrics = get_mcp_metrics()
        if metrics:
            metrics.record_operation_duration(
                "journal.path_generation_duration_seconds",
                duration,
                operation_type="file_path_generation"
            )
            metrics.record_tool_call(
                "journal.path_generation_operations_total",
                success=True,
                operation_type="file_path_generation"
            )
        
        return file_path
        
    except Exception as e:
        # Record error metrics with sanitized error message
        duration = time.time() - start_time
        metrics = get_mcp_metrics()
        if metrics:
            metrics.record_tool_call(
                "journal.path_generation_operations_total",
                success=False,
                operation_type="file_path_generation"
            )
        
        if current_span:
            current_span.set_attribute("error.category", "path_generation_failed")
            current_span.set_attribute("error.message", sanitize_for_telemetry(str(e)))
        
        raise

@trace_mcp_operation("journal.append_file", attributes={"operation_type": "file_write", "file_type": "markdown"})
def append_to_journal_file(text, file_path):
    """
    Append a text entry to the specified journal file, creating parent directories as needed (on-demand pattern).
    This function implements the on-demand pattern for directory creation: parent directories are created only when needed, not upfront.
    Raises ValueError if file cannot be written due to permissions.
    Args:
        text (str): The text to append
        file_path (str or Path): The file to append to
    """
    import time
    from opentelemetry import trace
    
    start_time = time.time()
    file_path = Path(file_path)
    
    # Add semantic conventions for telemetry
    current_span = trace.get_current_span()
    if current_span:
        current_span.set_attribute("file.path", str(file_path.name))  # Only filename for privacy
        current_span.set_attribute("file.extension", file_path.suffix)
        current_span.set_attribute("journal.content_length", len(text))
    
    try:
        ensure_journal_directory(file_path)
    except PermissionError as e:
        if current_span:
            current_span.set_attribute("error.category", "permission_denied_directory")
        raise ValueError(f"Permission denied (directory): {e}")
    
    add_rule = file_path.exists() and file_path.stat().st_size > 0
    
    try:
        with open(file_path, "a") as f:
            if add_rule:
                f.write("\n---\n")
            f.write(text)
        
        # Record file operation metrics
        duration = time.time() - start_time
        metrics = get_mcp_metrics()
        if metrics:
            metrics.record_operation_duration(
                "journal.file_write_duration_seconds",
                duration,
                operation_type="append",
                file_type="markdown"
            )
            
            # Record file size if we can get it
            try:
                file_size = file_path.stat().st_size
                if current_span:
                    current_span.set_attribute("file.size_bytes", file_size)
                # We could add a histogram for file sizes here if needed
            except Exception:
                pass  # Don't fail on metrics collection
                
            # Record success counter
            metrics.record_tool_call(
                "journal.file_write_total",
                success=True,
                operation_type="append"
            )
        
    except PermissionError as e:
        # Record error metrics
        metrics = get_mcp_metrics()
        if metrics:
            metrics.record_tool_call(
                "journal.file_write_total",
                success=False,
                operation_type="append"
            )
        
        if current_span:
            current_span.set_attribute("error.category", "permission_denied_file")
        raise ValueError(f"Permission denied (file): {e}")

# Section Generator: Summary
# Purpose: Generates the Summary section for a journal entry using AI.
# This function creates a narrative summary of what changed and why, using explicit developer statements and technical context from chat and git. It returns a placeholder; the AI agent is expected to execute the docstring prompt and fill in the content.
@trace_mcp_operation("journal.generate_summary", attributes={"operation_type": "ai_generation", "section_type": "summary"})
def generate_summary_section(journal_context) -> SummarySection:
    """
    AI Prompt for Summary Section Generation

    Purpose: Generate a narrative paragraph that captures the essential "story" of what changed and why, using conversational language that preserves the developer's authentic voice and technical context.

    Instructions: Extract explicit purpose statements and technical context from chat history and git changes to create a conversational summary. Focus on WHAT changed and WHY, told as a story.

    Priority for Content Sources:
    1. Explicit developer statements in chat - their own words about motivation, goals, problems
    2. Git commit messages - if they contain reasoning or context
    3. Git changes description - what actually changed in the code

    Purpose Statement Extraction:
    Look for explicit purpose statements in chat, such as:
    - "because..." / "since..." (reasoning)
    - "to fix..." / "to solve..." (problem-solving)
    - "so that..." / "in order to..." (goals)
    - "trying to..." / "attempting to..." (objectives)
    - Direct problem statements: "this is broken", "X isn't working"
    - Goal statements: "want to make Y easier", "need to improve Z"

    Evolution of Thinking:
    If you find multiple purpose statements that evolved over time, show the progression:
    "Started to [initial goal] but [what changed understanding] so [final approach]"

    Adaptive Story Detail:
    Match the narrative detail to what the commit actually accomplished. For example:
    - Large architectural changes might focus on high-level goals and system-wide impact
    - Focused bug fixes might tell the story of problem discovery and solution approach
    - Feature additions might explain user needs and implementation decisions
    - Refactoring might describe why cleanup was needed and what improved

    Language Translation Guidelines:
    When using developer's emotional language from chat and commit messages, translate respectfully:
    - Preserve authentic emotional expression including colorful language
    - Translate unkind language towards others into neutral descriptions, including:
      - Personal attacks or harsh judgments about individuals
      - Racism, sexism, or negative bias based on gender, sexual orientation, religion, etc.
      - Mean statements about people's abilities or worth
    - Preserve positive "who I am" language and translate only negative self-talk
    - Examples:
      - Keep: "this f***ing bug is driving me crazy", "I am awesome!", "I nailed that implementation"
      - Translate: "I'm such an idiot" → "encountered challenging complexity", "Bob writes terrible code" → "encountered challenging legacy implementation"

    ANTI-HALLUCINATION RULES:
    - Do NOT invent, infer, or summarize information not explicitly present in the context
    - Only include purpose statements directly supported by chat transcript or commit messages
    - If developer hasn't stated WHY something was done, describe WHAT was done without guessing motivation
    - If chat context is unavailable, work only with git changes - do not speculate about reasons

    Output Format:
    Single paragraph, conversational tone, any length needed to tell the complete story.

    CHECKLIST:
    - [ ] Searched chat history for explicit purpose statements using keyword patterns
    - [ ] Extracted developer's actual reasoning where explicitly stated
    - [ ] Showed evolution of thinking if multiple purposes emerged over time
    - [ ] Translated harsh personal language while preserving technical meaning and authentic frustration
    - [ ] Used git changes as foundation when chat context unavailable
    - [ ] Did NOT infer or speculate about unstated motivations
    - [ ] Created conversational narrative that explains what changed and why
    - [ ] Preserved authentic technical voice through respectful translation
    - [ ] Included technical context in story-telling format
    - [ ] Verified all content is grounded in actual chat/git evidence
    """
    import time
    
    start_time = time.time()
    _add_ai_generation_telemetry("summary", journal_context, start_time)
    
    try:
        result = SummarySection(summary="")
        
        duration = time.time() - start_time
        _record_ai_generation_metrics("summary", duration, True)
        
        return result
        
    except Exception as e:
        duration = time.time() - start_time
        _record_ai_generation_metrics("summary", duration, False, "ai_generation_failed")
        raise

# Section Generator: Technical Synopsis
# TechnicalSynopsisSection: Represents the technical synopsis section of a journal entry.
# This section provides a code-focused analysis of what changed, generated by an AI-driven function pattern.
# The function returns a placeholder; the AI agent is expected to execute the docstring prompt and fill in the content.
@trace_mcp_operation("journal.generate_technical_synopsis", attributes={"operation_type": "ai_generation", "section_type": "technical_synopsis"})
def generate_technical_synopsis_section(journal_context: JournalContext) -> TechnicalSynopsisSection:
    """
    AI Prompt for Technical Synopsis Section Generation

    Purpose: Generate a code-focused analysis of what changed in this commit, providing technical implementation details that complement the narrative summary.

    Instructions: Extract technical implementation details from chat history and git changes to create a precise synopsis of how the work was accomplished. Focus on architectural patterns, code structure changes, technical approaches, and implementation specifics.

    Priority for Content Sources:
    1. Explicit technical details from chat - developer's own words about implementation approach
    2. Git commit messages - if they contain technical reasoning
    3. Git changes description - what actually changed in the code
    4. Terminal commands - to add technical context when relevant

    Technical Detail Extraction:
    Look for any explicit technical discussion about implementation, including:
    - Architectural patterns and design decisions
    - Specific classes, functions, modules, or files modified
    - Technical approaches and methodologies used
    - Technology stack changes or additions
    - Code structure and organization changes
    - Testing strategies and implementation
    - Performance considerations
    - Integration approaches

    Adaptive Detail Level:
    Match the detail level to what the commit actually changed. For example:
    - Large architectural changes might focus on high-level patterns and system design
    - Focused bug fixes might describe specific technical solutions and root causes
    - Feature additions might detail implementation approach and integration points
    - Refactoring might explain structural improvements and code organization changes

    Language Translation Guidelines:
    When using developer's emotional language from chat and commit messages, translate respectfully:
    - Preserve authentic emotional expression including colorful language
    - Translate unkind language towards others into neutral descriptions, including:
      - Personal attacks or harsh judgments about individuals
      - Racism, sexism, or negative bias based on gender, sexual orientation, religion, etc.
      - Mean statements about people's abilities or worth
    - Preserve positive "who I am" language and translate only negative self-talk
    - Examples:
      - Keep: "this f***ing bug is driving me crazy", "I am awesome!", "I nailed that implementation"
      - Translate: "I'm such an idiot" → "encountered challenging complexity", "Bob writes terrible code" → "encountered challenging legacy implementation"

    ANTI-HALLUCINATION RULES:
    - Do NOT invent, infer, or summarize technical information not explicitly present in the context
    - Only include technical details directly supported by chat transcript, commit messages, or git changes
    - If developer hasn't explained HOW something was implemented, describe only WHAT changed based on available evidence
    - If chat context is unavailable, work only with git changes - do not speculate about implementation approaches
    - Never assume what code changes "probably" accomplish technically

    Output Format:
    Self-contained technical narrative, any length needed to tell the complete technical story. Multiple paragraphs allowed for complex changes. No cross-references to other sections.

    CHECKLIST:
    - [ ] Searched chat history for explicit technical implementation details
    - [ ] Extracted developer's actual technical reasoning where explicitly stated
    - [ ] Adapted detail level to match commit scope and available information
    - [ ] Translated harsh personal language while preserving technical meaning and authentic frustration
    - [ ] Used git changes as foundation when chat context unavailable
    - [ ] Did NOT infer or speculate about technical approaches not explicitly described
    - [ ] Created self-contained technical narrative without cross-references
    - [ ] Preserved authentic technical voice through respectful translation
    - [ ] Included relevant technical context from terminal commands when available
    - [ ] Verified all technical content is grounded in actual chat/git/terminal evidence
    """
    import time
    
    start_time = time.time()
    _add_ai_generation_telemetry("technical_synopsis", journal_context, start_time)
    
    try:
        # Returns a placeholder. The AI agent is expected to execute the docstring prompt and fill in the content.
        result = TechnicalSynopsisSection(technical_synopsis="")
        
        duration = time.time() - start_time
        _record_ai_generation_metrics("technical_synopsis", duration, True)
        
        return result
        
    except Exception as e:
        duration = time.time() - start_time
        _record_ai_generation_metrics("technical_synopsis", duration, False, "ai_generation_failed")
        raise

# Section Generator: Accomplishments
# Purpose: Extracts and summarizes what was successfully completed or achieved in the commit, focusing on developer satisfaction and explicit evidence from chat and git context.
# Assumptions: Only includes accomplishments with clear evidence; does not infer or speculate. Returns an empty list if nothing is found.
# Limitations: Requires AI agent to fulfill the docstring prompt; placeholder implementation returns empty list.
@trace_mcp_operation("journal.generate_accomplishments", attributes={"operation_type": "ai_generation", "section_type": "accomplishments"})
def generate_accomplishments_section(journal_context: JournalContext) -> AccomplishmentsSection:
    """
    AI Prompt for Accomplishments Section Generation

    Purpose: Generate a list of what was successfully completed or achieved in this commit, capturing the developer's authentic sense of accomplishment and satisfaction with their work.

    Instructions: Extract evidence of successful work completion and achievement from chat history and git changes to create a list of meaningful accomplishments. Focus on what the developer explicitly expressed satisfaction with or what can be clearly identified as successful completion.

    Priority for Content Sources:
    1. Explicit statements of success, satisfaction, or achievement from chat - developer's own words
    2. Problem resolution evidence - where a stated problem was clearly solved
    3. Goal completion evidence - where a stated goal was explicitly achieved
    4. Git commit messages - if they indicate successful completion
    5. Git changes description - successful implementation evidence

    Accomplishment Evidence Extraction:
    Look for explicit evidence of accomplishment, such as:
    - Direct statements of success: "got it working", "nailed it", "fixed the issue"
    - Problem resolution: "that solved the bug", "no more errors", "finally working"
    - Goal achievement: "feature is now complete", "successfully implemented"
    - Satisfaction indicators: "happy with this", "pleased with the result", "good solution"
    - Technical validation: "tests are passing", "working as expected", "deployment successful"
    - Learning moments: "learned how to...", "figured out the pattern", "now I understand"

    Quality Standards:
    Focus on accomplishments that:
    - Have clear evidence in the context (not inferred or assumed)
    - Represent meaningful progress or completion
    - Include both technical achievements and personal growth/learning
    - Are specific rather than generic ("fixed authentication bug" vs "made progress")
    - Capture the developer's authentic voice and satisfaction

    Language Translation Guidelines:
    When using developer's language from chat, translate respectfully:
    - Preserve authentic emotional expression including colorful language
    - Translate unkind language towards others into neutral descriptions, including:
      - Personal attacks or harsh judgments about individuals
      - Racism, sexism, or negative bias based on gender, sexual orientation, religion, etc.
      - Mean statements about people's abilities or worth
    - Preserve positive "who I am" language and translate only negative self-talk
    - Examples:
      - Keep: "this f***ing bug is driving me crazy", "I am awesome!", "I nailed that implementation"
      - Translate: "I'm such an idiot" → "encountered challenging complexity", "Bob writes terrible code" → "encountered challenging legacy implementation"

    ANTI-HALLUCINATION RULES:
    - Do NOT invent, infer, or assume accomplishments not explicitly supported by the context
    - Only include accomplishments with clear evidence in chat transcript, commit messages, or git changes
    - If no clear accomplishments are found, return an empty list and omit the section
    - Do not reframe neutral work as accomplishments unless there's explicit positive evidence
    - Never speculate about what the developer "probably" felt accomplished about

    Output Format:
    - List of strings, each representing a specific accomplishment
    - Return empty list if no clear accomplishments are found and omit the section
    - Use present tense or past tense as appropriate ("Fixed the authentication bug", "Successfully implemented user validation")
    - Preserve authentic language while ensuring clarity

    CHECKLIST:
    - [ ] Searched chat history for explicit statements of success, satisfaction, or achievement
    - [ ] Identified clear problem resolution and goal completion evidence
    - [ ] Focused on meaningful progress with specific evidence
    - [ ] Included both technical achievements and learning moments
    - [ ] Applied language translation guidelines while preserving authentic satisfaction
    - [ ] Used only accomplishments directly supported by context evidence
    - [ ] Returned empty list if no clear accomplishments found
    - [ ] Did NOT invent, infer, or assume accomplishments without explicit evidence
    - [ ] Captured developer's authentic voice and satisfaction
    - [ ] Verified all accomplishments are grounded in actual chat/git evidence
    """
    import time
    
    start_time = time.time()
    _add_ai_generation_telemetry("accomplishments", journal_context, start_time)
    
    try:
        result = AccomplishmentsSection(accomplishments=[])
        
        duration = time.time() - start_time
        _record_ai_generation_metrics("accomplishments", duration, True)
        
        return result
        
    except Exception as e:
        duration = time.time() - start_time
        _record_ai_generation_metrics("accomplishments", duration, False, "ai_generation_failed")
        raise

# Section Generator: Frustrations
# Purpose: Extracts and summarizes challenges, setbacks, and frustrations encountered in the commit, using only explicit evidence from chat, terminal, and git context.
# Assumptions: Only includes frustrations with clear evidence; does not infer or speculate. Returns an empty list if nothing is found.
# Limitations: Requires AI agent to fulfill the docstring prompt; placeholder implementation returns empty list.
@trace_mcp_operation("journal.generate_frustrations", attributes={"operation_type": "ai_generation", "section_type": "frustrations"})
def generate_frustrations_section(journal_context: JournalContext) -> FrustrationsSection:
    """
    AI Prompt for Frustrations Section Generation

    Purpose: Generate a list of challenges, blocks, or difficulties encountered during this commit, capturing the developer's authentic experience with obstacles and setbacks.

    Instructions: Extract evidence of challenges, frustrations, or difficulties from chat history and git context to create a list of meaningful obstacles encountered. Focus on what the developer explicitly expressed frustration with or what can be clearly identified as blocking or difficult.

    Priority for Content Sources:
    1. Explicit statements of frustration, difficulty, or challenges from chat - developer's own words
    2. Problem discovery evidence - where new problems were identified
    3. Blocking issues - where progress was hindered or stopped
    4. Git commit messages - if they indicate challenges or fixes for problems
    5. Terminal commands - repetitive or failed commands that suggest struggle

    Frustration Evidence Extraction:
    Look for explicit evidence of frustration or difficulty, such as:
    - Direct statements of frustration: "this is driving me crazy", "can't figure out", "struggling with"
    - Problem discovery: "found a new bug", "this is more complex than expected", "hitting a wall"
    - Blocking issues: "can't proceed until", "stuck on this", "need to resolve this first"
    - Technical difficulties: "tests are failing", "getting errors", "configuration issues"
    - Time/effort statements: "spent hours on this", "took longer than expected", "had to research"
    - Confusion indicators: "don't understand why", "unclear documentation", "confusing behavior"

    Quality Standards:
    Focus on frustrations that:
    - Have clear evidence in the context (not inferred or assumed)
    - Represent meaningful obstacles or learning opportunities
    - Include both technical challenges and process difficulties
    - Are specific rather than generic ("authentication middleware failing" vs "had problems")
    - Capture the developer's authentic voice and experience

    Language Translation Guidelines:
    When using developer's language from chat, translate respectfully:
    - Preserve authentic emotional expression including colorful language
    - Translate unkind language towards others into neutral descriptions, including:
      - Personal attacks or harsh judgments about individuals
      - Racism, sexism, or negative bias based on gender, sexual orientation, religion, etc.
      - Mean statements about people's abilities or worth
    - Preserve positive "who I am" language and translate only negative self-talk
    - Examples:
      - Keep: "this f***ing bug is driving me crazy", "I am awesome!", "I nailed that implementation"
      - Translate: "I'm such an idiot" → "encountered challenging complexity", "Bob writes terrible code" → "encountered challenging legacy implementation"

    ANTI-HALLUCINATION RULES:
    - Do NOT invent, infer, or assume frustrations not explicitly supported by the context
    - Only include frustrations with clear evidence in chat transcript, commit messages, git changes, or terminal commands
    - If no clear frustrations are found, return an empty list and omit the section
    - Do not reframe neutral work as frustrations unless there's explicit negative evidence
    - Never speculate about what the developer "probably" found frustrating

    Output Format:
    - List of strings, each representing a specific frustration or challenge
    - Return empty list if no clear frustrations are found and omit the section
    - Use present tense or past tense as appropriate ("Authentication middleware kept failing", "Spent hours debugging configuration issues")
    - Preserve authentic language while ensuring clarity

    CHECKLIST:
    - [ ] Searched chat history for explicit statements of frustration, difficulty, or challenges
    - [ ] Identified clear problem discovery and blocking issues
    - [ ] Focused on meaningful obstacles with specific evidence
    - [ ] Included both technical challenges and process difficulties
    - [ ] Applied language translation guidelines while preserving authentic frustration
    - [ ] Used only frustrations directly supported by context evidence
    - [ ] Returned empty list if no clear frustrations found
    - [ ] Did NOT invent, infer, or assume frustrations without explicit evidence
    - [ ] Captured developer's authentic voice and experience
    - [ ] Verified all frustrations are grounded in actual chat/git/terminal evidence
    """
    import time
    
    start_time = time.time()
    _add_ai_generation_telemetry("frustrations", journal_context, start_time)
    
    try:
        result = FrustrationsSection(frustrations=[])
        
        duration = time.time() - start_time
        _record_ai_generation_metrics("frustrations", duration, True)
        
        return result
        
    except Exception as e:
        duration = time.time() - start_time
        _record_ai_generation_metrics("frustrations", duration, False, "ai_generation_failed")
        raise

# Section Generator: Tone/Mood
# Purpose: Generates the Tone/Mood section for a journal entry using AI.
# This function analyzes chat, commit, and terminal context to infer the developer's emotional state and supporting evidence. Only includes moods with explicit or strongly inferred evidence; does not speculate. Returns a placeholder until the AI agent executes the docstring prompt.
@trace_mcp_operation("journal.generate_tone_mood", attributes={"operation_type": "ai_generation", "section_type": "tone_mood"})
def generate_tone_mood_section(journal_context: JournalContext) -> ToneMoodSection:
    """
    AI Prompt for Tone/Mood Section Generation

    Purpose: Generate a tone and mood assessment based on the developer's language and emotional indicators in their chat messages, capturing the authentic emotional context of the work session.

    Instructions: Extract emotional indicators and tone from chat history to assess the developer's mood and emotional state during this work session. Focus on explicit emotional language, patterns of expression, and authentic indicators of the developer's experience.

    Priority for Content Sources:
    1. Explicit emotional language from chat - developer's own emotional expressions
    2. Tone patterns in conversation - how the developer communicated
    3. Language intensity and word choice - emotional indicators
    4. Reaction patterns - responses to success, failure, or challenges
    5. Context clues - time of day, session length, complexity of work

    Emotional Indicator Extraction:
    Look for explicit emotional expression and tone indicators, such as:
    - Mood descriptors: "excited about", "frustrated with", "confident in", "anxious about"
    - Energy levels: "pumped up", "exhausted", "motivated", "burned out"
    - Satisfaction indicators: "thrilled with", "pleased about", "disappointed by", "satisfied with"
    - Stress/pressure indicators: "under pressure", "tight deadline", "stressed about", "relaxed"
    - Confidence levels: "confident this will work", "uncertain about", "sure that", "confused by"
    - Enthusiasm markers: exclamation points, caps, enthusiastic language, excitement

    Mood Categories (select the most accurate):
    - **Productive/Focused**: Steady progress, clear thinking, engaged with the work
    - **Excited/Enthusiastic**: High energy, eager to implement, positive about outcomes
    - **Frustrated/Challenged**: Facing obstacles, working through difficulties, problem-solving under pressure
    - **Determined/Persistent**: Pushing through challenges, committed to solutions, resilient
    - **Experimental/Curious**: Trying new approaches, learning-oriented, exploratory
    - **Satisfied/Accomplished**: Content with progress, feeling successful, positive closure
    - **Stressed/Pressured**: Time constraints, high stakes, feeling overwhelmed
    - **Reflective/Analytical**: Thoughtful analysis, careful consideration, methodical approach

    Tone Assessment:
    Evaluate the overall communication tone:
    - **Professional**: Clear, structured, business-focused communication
    - **Casual/Conversational**: Relaxed, informal, natural expression
    - **Technical/Analytical**: Detail-oriented, precise, methodical language
    - **Emotional/Expressive**: Open about feelings, authentic emotional language
    - **Collaborative**: Team-oriented, seeking input, inclusive language
    - **Independent/Self-directed**: Solo problem-solving, autonomous approach

    Language Translation Guidelines:
    When analyzing developer's emotional language from chat, translate respectfully:
    - Preserve authentic emotional expression including colorful language
    - Translate unkind language towards others into neutral descriptions, including:
      - Personal attacks or harsh judgments about individuals  
      - Racism, sexism, or negative bias based on gender, sexual orientation, religion, etc.
      - Mean statements about people's abilities or worth
    - Preserve positive "who I am" language and translate only negative self-talk
    - Examples:
      - Keep: "this f***ing bug is driving me crazy", "I am awesome!", "I nailed that implementation"
      - Translate: "I'm such an idiot" → "encountered challenging complexity", "Bob writes terrible code" → "encountered challenging legacy implementation"

    ANTI-HALLUCINATION RULES:
    - Do NOT invent, infer, or assume emotional states not explicitly supported by the chat language
    - Only assess mood/tone based on clear evidence in the chat transcript
    - If no clear emotional indicators are found, return empty dict and omit the section
    - Do not psychoanalyze or read between the lines - use only explicit emotional language
    - Never speculate about mood based on technical content alone

    Output Format:
    - Dict with 'mood' and 'indicators' keys
    - **mood**: Single primary mood category that best describes the session
    - **indicators**: Specific evidence from chat that supports the mood assessment
    - Return empty dict if no clear emotional indicators are found and omit the section

    CHECKLIST:
    - [ ] Searched chat history for explicit emotional language and tone indicators
    - [ ] Identified clear mood patterns and emotional expression
    - [ ] Selected most accurate mood category based on evidence
    - [ ] Provided specific indicators that support the mood assessment
    - [ ] Applied language translation guidelines while preserving authentic emotional expression
    - [ ] Used only emotional evidence directly supported by chat transcript
    - [ ] Returned empty dict if no clear emotional indicators found
    - [ ] Did NOT invent, infer, or psychoanalyze emotional states without explicit evidence
    - [ ] Captured authentic emotional context of the work session
    - [ ] Verified all mood assessment is grounded in actual chat evidence
    """
    import time
    
    start_time = time.time()
    _add_ai_generation_telemetry("tone_mood", journal_context, start_time)
    
    try:
        result = ToneMoodSection(mood="", indicators="")
        
        duration = time.time() - start_time
        _record_ai_generation_metrics("tone_mood", duration, True)
        
        return result
        
    except Exception as e:
        duration = time.time() - start_time
        _record_ai_generation_metrics("tone_mood", duration, False, "ai_generation_failed")
        raise

# Section Generator: Discussion Notes
# Purpose: Generates the Discussion Notes section for a journal entry using AI.
# This function extracts and curates relevant discussion points from chat context, focusing on technical reasoning, decisions, and key exchanges while filtering out routine conversation. Only includes discussion content with explicit evidence; does not paraphrase or invent content. Returns a placeholder until the AI agent executes the docstring prompt.
@trace_mcp_operation("journal.generate_discussion_notes", attributes={"operation_type": "ai_generation", "section_type": "discussion_notes"})
def generate_discussion_notes_section(journal_context: JournalContext) -> DiscussionNotesSection:
    """
    AI Prompt for Discussion Notes Section Generation

    Purpose: Extract and curate relevant discussion points from chat history that provide insight into the thinking process, technical reasoning, decisions, and key exchanges, filtering out routine conversation.

    Instructions: Extract meaningful conversation excerpts from chat history to preserve important technical reasoning, decision-making discussions, and insights. Focus on content that reveals thought processes, technical considerations, and valuable exchanges while excluding routine conversation.

    Priority for Content Sources:
    1. Chat history from journal_context.chat_history - the only source for discussion content
    2. No other sources - discussion notes can only come from actual conversations

    Priority Types (in descending order of importance):
    1. **Technical decision reasoning** - explaining why specific technical choices were made
    2. **Problem-solving discussions** - working through challenges, debugging, solution approaches
    3. **Questions and clarifications** - meaningful Q&A that reveals understanding or requirements
    4. **Important context or background** - setup, constraints, requirements, or context that frames the work
    5. **Learning moments** - insights, discoveries, or "aha!" moments during conversation
    6. **Emotional expressions** - authentic frustration, excitement, satisfaction that adds human context

    Content Filtering Guidelines:
    **Include discussion that:**
    - Reveals technical reasoning or decision-making process
    - Shows problem-solving approaches or debugging strategies
    - Contains meaningful questions that led to insights
    - Provides important context for understanding the work
    - Captures learning moments or discoveries
    - Expresses authentic emotional responses that add human context

    **Exclude routine content:**
    - Standard greetings, pleasantries, and social chat
    - Simple confirmation responses ("ok", "sounds good", "got it")
    - Routine status updates without insight ("working on X now")
    - Basic commands or requests without reasoning
    - Off-topic conversations unrelated to the technical work

    Length and Excerpting Guidelines:
    - **Preserve full quotes** when they're concise and meaningful (under ~50 words)
    - **Use excerpts** for longer discussions: include key parts with `[...]` notation
    - **Group related exchanges** chronologically when they form a coherent discussion
    - **Separate distinct topics** with blank lines between different conversation threads
    - **Prioritize recent discussions** when space is limited, but include earlier context if it's crucial

    Length Target Guidelines:
    - **Rich discussion context**: Include 5-10 meaningful exchanges or excerpts
    - **Moderate discussion**: Include 3-5 key exchanges
    - **Minimal meaningful discussion**: Include 1-3 most important points
    - **No meaningful discussion**: Return empty list and omit section

    Excerpt Guidelines for Long Content:
    - Use `[...]` notation where content is omitted for length
    - Focus on parts that match priority types and explicit reasoning keywords ("because", "since", "the tradeoff is")

    Speaker Attribution and Formatting:
    - Always try to attribute to speaker when possible, fall back to plain string if unclear
    - Present in chronological order, grouping messages by topic with blank lines between different topics
    - Trust AI judgment to recognize natural topic boundaries and appropriate granularity
    - Preserve natural conversation flow and connections between related messages when length permits

    Language Translation Guidelines:
    When using developer's language from chat, translate respectfully:
    - Preserve authentic emotional expression including colorful language
    - Translate unkind language towards others into neutral descriptions, including:
     - Personal attacks or harsh judgments about individuals
     - Racism, sexism, or negative bias based on gender, sexual orientation, religion, etc.
     - Mean statements about people's abilities or worth
    - Preserve positive "who I am" language and translate only negative self-talk
    - Examples:
     - Keep: "this f***ing bug is driving me crazy", "I am awesome!", "I nailed that implementation"
     - Translate: "I'm such an idiot" → "encountered challenging complexity", "Bob writes terrible code" → "encountered challenging legacy implementation"

    ANTI-HALLUCINATION RULES:
    - Do NOT invent, infer, or summarize discussion points not explicitly present in the chat history
    - Only include discussion content directly supported by the chat transcript
    - Do not paraphrase or rewrite quotes - extract actual conversation content
    - If no relevant discussion is found, return an empty list and omit the section
    - Never combine separate conversations or create composite quotes

    Output Format:
    - List of strings or dicts with 'speaker' and 'text' fields for attributed quotes
    - Return empty list if no relevant discussion notes are found and omit the section
    - Maintain chronological order grouped by topic
    - Include `[...]` notation when content has been shortened

    CHECKLIST:
    - [ ] Searched chat history for content matching priority criteria (emotions, decisions, problem-solving, questions, technical discussion)
    - [ ] Extracted actual conversation content without paraphrasing or invention
    - [ ] Applied appropriate length limits with excerpt notation when needed
    - [ ] Included sufficient context to make quotes meaningful
    - [ ] Preserved conversational flow and speaker attribution when possible
    - [ ] Grouped by topic with appropriate granularity and blank lines between topics
    - [ ] Applied language translation guidelines while preserving authentic expression
    - [ ] Used only content directly supported by chat transcript
    - [ ] Returned empty list and omitted section if no relevant discussion was found
    - [ ] Did NOT invent, combine, or composite any discussion content
    """
    import time
    
    start_time = time.time()
    _add_ai_generation_telemetry("discussion_notes", journal_context, start_time)
    
    try:
        result = DiscussionNotesSection(discussion_notes=[])
        
        duration = time.time() - start_time
        _record_ai_generation_metrics("discussion_notes", duration, True)
        
        return result
        
    except Exception as e:
        duration = time.time() - start_time
        _record_ai_generation_metrics("discussion_notes", duration, False, "ai_generation_failed")
        raise

# Section Generator: Terminal Commands
# Purpose: Generates the Terminal Commands section for a journal entry using AI.
# This function extracts and lists all relevant terminal commands executed during the commit, focusing on commands that demonstrate problem-solving, technical approach, or challenges. Only includes commands with explicit evidence in terminal context; does not paraphrase or invent content. Returns a placeholder until the AI agent executes the docstring prompt.
@trace_mcp_operation("journal.generate_terminal_commands", attributes={"operation_type": "ai_generation", "section_type": "terminal_commands"})
def generate_terminal_commands_section(journal_context: JournalContext) -> TerminalCommandsSection:
    """
    AI Prompt for Terminal Commands Section Generation

    Purpose
    Extract and list all relevant terminal commands executed during this commit, focusing on commands that demonstrate problem-solving, technical approach, or challenges.

    Instructions
    Extract terminal commands from terminal context to create a chronological list of meaningful commands executed during the work session. Focus on commands that provide insight into the technical process and decision-making approach.

    Priority for Content Sources
    1. Terminal context from journal_context.terminal - the authoritative source for executed commands
    2. No other sources - work exclusively with the provided terminal context

    Terminal Command Extraction
    Look for commands that provide technical insight, such as:
    - Commands that demonstrate problem-solving steps
    - Commands that show the technical approach taken
    - Failed commands that highlight challenges or errors
    - Repetitive commands that might indicate frustrations or iteration
    - Build, test, or deployment commands that show progress
    - Debugging or investigation commands

    Filtering Guidelines
    Exclude commands that don't add narrative value:
    - Routine git commands (add, status, commit) unless they are significant to the narrative
    - Commands run specifically as part of journal entry creation or extraction
    - Commands containing passwords, API keys, tokens, or other sensitive information
    - Commands that reveal personal identifiable information (PII)
    - Basic navigation commands (cd, ls, pwd) unless part of a meaningful sequence
    - Obviously invalid commands: typos, malformed syntax, incomplete commands that add no technical insight

    Include meaningful failures that demonstrate problem-solving:
    - Commands that failed due to missing dependencies (shows environment setup process)
    - Commands that failed due to configuration issues (shows troubleshooting approach)
    - Commands that revealed errors leading to solutions (shows debugging process)
    - Failed attempts that led to successful alternatives (shows iteration and learning)

    Deduplication Rules
    - Apply adjacent identical command deduplication: compress consecutive identical commands with count notation (e.g., "npm test x3")
    - Preserve chronological order of distinct commands
    - Do not deduplicate non-adjacent commands even if identical

    ANTI-HALLUCINATION RULES
    - Do NOT invent, infer, or summarize commands that are not explicitly present in the terminal context
    - Only include commands that are directly supported by the terminal transcript
    - If no terminal context is available, return an empty list and omit the section
    - Never speculate about commands that might have been run
    - If a command is not present in the context, do NOT fill in gaps or assume it happened

    Output Format
    - List of strings, each representing a single terminal command
    - Return empty list if no relevant commands are found and omit the section
    - Maintain chronological order as they appeared in the terminal context
    - Apply deduplication formatting where appropriate (e.g., "command x3")

    CHECKLIST
    - [ ] Extracted commands exclusively from journal_context.terminal
    - [ ] Focused on commands that demonstrate problem-solving, technical approach, or challenges
    - [ ] Excluded routine git commands unless significant to the narrative
    - [ ] Excluded journal entry creation commands
    - [ ] Excluded commands containing sensitive information or PII
    - [ ] Excluded obviously invalid commands (typos, malformed syntax) while preserving meaningful failures
    - [ ] Included meaningful failures that demonstrate debugging and problem-solving process
    - [ ] Applied adjacent identical command deduplication with count notation
    - [ ] Preserved chronological order of distinct commands
    - [ ] Did NOT invent, infer, or speculate about commands not present in context
    - [ ] Returned empty list if no relevant commands found
    - [ ] Verified all commands are grounded in actual terminal context evidence
    """
    import time
    
    start_time = time.time()
    _add_ai_generation_telemetry("terminal_commands", journal_context, start_time)
    
    try:
        result = TerminalCommandsSection(terminal_commands=[])
        
        duration = time.time() - start_time
        _record_ai_generation_metrics("terminal_commands", duration, True)
        
        return result
        
    except Exception as e:
        duration = time.time() - start_time
        _record_ai_generation_metrics("terminal_commands", duration, False, "ai_generation_failed")
        raise

# Section Generator: Commit Metadata
# Purpose: Generates the Commit Metadata section for a journal entry using AI.
# This function extracts and formats relevant commit metadata from git context, providing key statistics and classifications that support the journal entry narrative. Only includes metadata fields directly supported by the git context data; does not invent or speculate. Returns a placeholder until the AI agent executes the docstring prompt.
@trace_mcp_operation("journal.generate_commit_metadata", attributes={"operation_type": "ai_generation", "section_type": "commit_metadata"})
def generate_commit_metadata_section(journal_context: JournalContext) -> CommitMetadataSection:
    """
    AI Prompt for Commit Metadata Section Generation

    Purpose: Extract and format relevant commit metadata from git context, providing key statistics and classifications that support the journal entry narrative.

    Instructions: Extract commit metadata from git context to create a clean, scannable summary of commit statistics and characteristics. Focus on metadata that provides useful context for understanding the scope and nature of the changes.

    Priority for Content Sources:
    1. Git context from journal_context.git - the authoritative source for all commit metadata
    2. No other sources - work exclusively with the provided git context

    Metadata Extraction:
    Extract and format the following metadata fields when available and meaningful:
    - **files_changed**: Total number of files modified in the commit
    - **insertions**: Total lines added
    - **deletions**: Total lines removed  
    - **size_classification**: Pre-classified commit size (small/medium/large)
    - **is_merge**: Whether this was a merge commit
    - **source_files**: Count of source code files changed
    - **config_files**: Count of configuration files changed
    - **docs_files**: Count of documentation files changed
    - **tests_files**: Count of test files changed

    Filtering Guidelines:
    Include only metadata that adds narrative value:
    - Include basic statistics (files_changed, insertions, deletions) when non-zero
    - Include size_classification and merge status when meaningful
    - Include file type counts only when non-zero and show meaningful distribution
    - Exclude metadata fields that are empty, zero, or not meaningful for this commit

    ANTI-HALLUCINATION RULES:
    - Do NOT invent, infer, or calculate metadata not explicitly present in the git context
    - Only include metadata fields directly supported by the git context data
    - If no git context is available, return an empty dict and omit the section
    - Never speculate about commit characteristics not present in the data

    Output Format:
    - Dict[str, str] with specified field names as keys and formatted values as strings
    - Return empty dict if no relevant metadata is found and omit the section
    - All values should be formatted as strings suitable for display

    CHECKLIST:
    - [ ] Extracted metadata exclusively from journal_context.git
    - [ ] Used specified field names for consistency
    - [ ] Included only meaningful/non-zero metadata fields
    - [ ] Formatted all values as display-ready strings
    - [ ] Did NOT invent, calculate, or speculate about metadata not present in git context
    - [ ] Returned empty dict if no relevant metadata found
    - [ ] Verified all metadata is grounded in actual git context data
    """
    import time
    
    start_time = time.time()
    _add_ai_generation_telemetry("commit_metadata", journal_context, start_time)
    
    try:
        result = CommitMetadataSection(commit_metadata={})
        
        duration = time.time() - start_time
        _record_ai_generation_metrics("commit_metadata", duration, True)
        
        return result
        
    except Exception as e:
        duration = time.time() - start_time
        _record_ai_generation_metrics("commit_metadata", duration, False, "ai_generation_failed")
        raise

@trace_mcp_operation("journal.ensure_directory", attributes={"operation_type": "directory_creation"})
def ensure_journal_directory(file_path):
    """
    Ensure the parent directory for the given file_path exists.
    Creates all missing parent directories as needed (on-demand pattern).
    This utility should be used by all file operations that need to ensure parent directories exist, replacing any upfront directory creation pattern.
    Raises PermissionError if directory creation fails due to permissions.
    Does nothing if the directory already exists.
    Args:
        file_path (Path or str): The file path whose parent directory should be ensured.
    Example:
        file_path = Path("journal/daily/2025-05-28-journal.md")
        ensure_journal_directory(file_path)
    """
    import time
    from opentelemetry import trace
    
    start_time = time.time()
    parent_dir = Path(file_path).parent
    
    # Add semantic conventions for telemetry
    current_span = trace.get_current_span()
    if current_span:
        current_span.set_attribute("directory.path", str(parent_dir.name))  # Only directory name for privacy
        current_span.set_attribute("directory.exists", parent_dir.exists())
    
    try:
        parent_dir.mkdir(parents=True, exist_ok=True)
        
        # Record successful directory operation metrics
        duration = time.time() - start_time
        metrics = get_mcp_metrics()
        if metrics:
            metrics.record_operation_duration(
                "journal.directory_operation_duration_seconds",
                duration,
                operation_type="ensure_directory"
            )
            metrics.record_tool_call(
                "journal.directory_operations_total",
                success=True,
                operation_type="ensure_directory"
            )
        
    except PermissionError:
        # Record error metrics
        metrics = get_mcp_metrics()
        if metrics:
            metrics.record_tool_call(
                "journal.directory_operations_total",
                success=False,
                operation_type="ensure_directory"
            )
        
        if current_span:
            current_span.set_attribute("error.category", "permission_denied_directory")
        raise  # Re-raise PermissionError as documented
    except Exception as e:
        # Record error metrics
        metrics = get_mcp_metrics()
        if metrics:
            metrics.record_tool_call(
                "journal.directory_operations_total",
                success=False,
                operation_type="ensure_directory"
            )
        
        if current_span:
            current_span.set_attribute("error.category", "directory_creation_failed")
        raise OSError(f"Failed to create directory {parent_dir}: {e}")

@trace_mcp_operation("journal.load_context", attributes={"operation_type": "config_read", "file_type": "toml"})
def load_journal_context(config_path: str) -> dict:
    """
    Load configuration from a TOML file to establish journal context.
    
    Args:
        config_path: Path to the TOML configuration file
        
    Returns:
        dict: Parsed configuration data
        
    Raises:
        JournalError: If configuration cannot be loaded
    """
    import time
    from opentelemetry import trace
    
    start_time = time.time()
    
    # Add semantic conventions for telemetry
    current_span = trace.get_current_span()
    if current_span:
        current_span.set_attribute("config.file_path", os.path.basename(config_path))
        current_span.set_attribute("operation_type", "config_load")
    
    try:
        import tomllib
        import os
        
        if not os.path.exists(config_path):
            if current_span:
                current_span.set_attribute("error.category", "file_not_found")
            raise JournalError(f"Configuration file not found: {config_path}")
        
        file_size = os.path.getsize(config_path)
        if current_span:
            current_span.set_attribute("config.file_size_bytes", file_size)
        
        with open(config_path, 'rb') as f:
            config_data = tomllib.load(f)
        
        # Record successful config load metrics
        duration = time.time() - start_time
        metrics = get_mcp_metrics()
        if metrics:
            metrics.record_operation_duration(
                "journal.config_load_duration_seconds",
                duration,
                operation_type="config_load",
                file_type="toml"
            )
            metrics.record_tool_call(
                "journal.config_load_operations_total",
                success=True,
                operation_type="config_load"
            )
        
        if current_span:
            current_span.set_attribute("config.sections_loaded", len(config_data))
        
        return config_data
        
    except Exception as e:
        # Record error metrics
        duration = time.time() - start_time
        metrics = get_mcp_metrics()
        if metrics:
            metrics.record_tool_call(
                "journal.config_load_operations_total",
                success=False,
                operation_type="config_load"
            )
        
        if current_span:
            current_span.set_attribute("error.category", "config_load_failed")
        
        raise JournalError(f"Failed to load configuration: {e}")
