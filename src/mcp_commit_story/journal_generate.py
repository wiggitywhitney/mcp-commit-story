import re
import logging
import json
import inspect
from typing import List, Optional, Dict, Union, Any
from pathlib import Path
import os
from mcp_commit_story.context_types import ChatHistory, SummarySection, TechnicalSynopsisSection, JournalContext, AccomplishmentsSection, FrustrationsSection, ToneMoodSection, DiscussionNotesSection, CommitMetadataSection
from .telemetry import (
    trace_mcp_operation, 
    get_mcp_metrics, 
    sanitize_for_telemetry
)
from .ai_invocation import invoke_ai

logger = logging.getLogger(__name__)

# TechnicalSynopsisSection: Represents the technical synopsis section of a journal entry.
# This section provides a code-focused analysis of what changed, generated by an AI-driven function pattern.
# The function returns a placeholder; the AI agent is expected to execute the docstring prompt and fill in the content.

"""
Journal entry generation for engineering work.

Content Quality Guidelines:
- Focus on signal (unique insights, decisions, challenges) over noise (routine procedures)
- Highlight what makes each entry unique rather than repeating standard practices
- Capture the narrative "story" behind the code changes
- Include emotional context when relevant, but only with clear supporting evidence
- Omit standard workflow details unless they're directly relevant to understanding the work
"""

class JournalParseError(Exception):
    pass

def _add_ai_generation_telemetry(section_type: str, journal_context, start_time: float):
    """
    Utility function to add consistent telemetry for AI generation operations.
    
    Args:
        section_type: The type of section being generated (e.g., 'summary', 'accomplishments')
        journal_context: The journal context being processed
        start_time: The timestamp when generation started
    """
    from opentelemetry import trace
    
    # Add semantic conventions for AI generation telemetry
    current_span = trace.get_current_span()
    if current_span:
        # Calculate context size
        context_size = 0
        if journal_context:
            if hasattr(journal_context, 'chat_history') and journal_context.get('chat_history'):
                context_size += len(journal_context['chat_history'].get('messages', []))

            if hasattr(journal_context, 'file_changes') and journal_context.get('file_changes'):
                context_size += len(journal_context['file_changes'])
        
        current_span.set_attribute("journal.context_size", context_size)
        current_span.set_attribute("journal.entry_id", journal_context.get('commit_hash', 'unknown') if journal_context else 'unknown')
        # AI model info would be added here if available from context

def _record_ai_generation_metrics(section_type: str, duration: float, success: bool, error_category: str = None):
    """
    Utility function to record AI generation metrics consistently.
    
    Args:
        section_type: The type of section being generated
        duration: Time taken for generation
        success: Whether the operation succeeded
        error_category: Category of error if operation failed
    """
    from opentelemetry import trace
    
    metrics = get_mcp_metrics()
    if metrics:
        if success:
            metrics.record_operation_duration(
                "journal.ai_generation_duration_seconds",
                duration,
                section_type=section_type,
                operation_type="ai_generation"
            )
        
        metrics.record_tool_call(
            "journal.generation_operations_total",
            success=success,
            section_type=section_type
        )
    
    if not success and error_category:
        current_span = trace.get_current_span()
        if current_span:
            current_span.set_attribute("error.category", error_category)

def log_ai_agent_interaction(context_sent: Any, response_received: Any, debug_mode: bool = False):
    """
    Simple logging of AI interactions for debugging integration issues.
    
    This utility function provides visibility into AI agent interactions without
    interfering with normal operation. Useful for troubleshooting when AI responses
    don't match expectations or when debugging context size issues.
    
    Args:
        context_sent: The context/prompt data sent to the AI
        response_received: The response received from the AI
        debug_mode: Whether to log debug information (can be set via environment)
    
    Usage:
        # In AI generation functions:
        result = ai_generate_section(context)
        log_ai_agent_interaction(context, result, debug_mode=True)
        return result
    """
    # Check environment variable if debug_mode not explicitly set
    if not debug_mode:
        debug_mode = os.getenv('MCP_DEBUG_AI_INTERACTIONS', 'false').lower() in ('true', '1', 'yes')
    
    if debug_mode:
        context_size = len(str(context_sent)) if context_sent else 0
        response_type = type(response_received).__name__
        response_size = len(str(response_received)) if response_received else 0
        
        logger.debug(f"AI Interaction Debug:")
        logger.debug(f"  Context size: {context_size} characters")
        logger.debug(f"  Response type: {response_type}")
        logger.debug(f"  Response size: {response_size} characters")
        
        # Add telemetry if available
        metrics = get_mcp_metrics()
        if metrics:
            metrics.record_counter(
                "ai_interactions_logged_total",
                1,
                attributes={
                    "context_size_bucket": _get_size_bucket(context_size),
                    "response_type": response_type
                }
            )

def _get_size_bucket(size: int) -> str:
    """Helper function to bucket sizes for telemetry."""
    if size < 1000:
        return "small"
    elif size < 10000:
        return "medium" 
    elif size < 100000:
        return "large"
    else:
        return "xlarge"


def _parse_ai_response(response: str, expected_field: str, fallback_value: Any = "", parse_as_list: bool = False) -> Any:
    """
    Parse AI response that could be JSON or plain text.
    
    Args:
        response: The AI response string
        expected_field: The JSON field name to extract (e.g., "summary", "accomplishments")
        fallback_value: Value to return if parsing fails or field not found
        parse_as_list: If True, parse plain text response as list (split by newlines)
    
    Returns:
        The extracted field value if JSON parsing succeeds, otherwise the original response
        (or split by newlines if parse_as_list=True)
    """
    if not response or not response.strip():
        return fallback_value
    
    response = response.strip()
    
    # Strip markdown code blocks if present
    if response.startswith('```json') and response.endswith('```'):
        response = response[7:-3].strip()  # Remove ```json and ```
    elif response.startswith('```python') and response.endswith('```'):
        response = response[9:-3].strip()  # Remove ```python and ```
    elif response.startswith('```') and response.endswith('```'):
        response = response[3:-3].strip()  # Remove ``` and ```
    
    # Try to parse as JSON first
    try:
        parsed_json = json.loads(response)
        if isinstance(parsed_json, dict):
            if expected_field in parsed_json:
                field_value = parsed_json[expected_field]
                # If the field value is a dict but we expected a string, convert it
                if isinstance(field_value, dict) and not parse_as_list and isinstance(fallback_value, str):
                    # Convert dict to string representation
                    if 'description' in field_value:
                        return field_value['description']
                    elif 'text' in field_value:
                        return field_value['text']
                    else:
                        # Fallback to JSON string representation
                        return json.dumps(field_value)
                
                # Special handling for string field values when parse_as_list=True
                if parse_as_list and isinstance(field_value, str):
                    # Handle special case where AI returns '[]' to indicate no items
                    if field_value == '[]':
                        return []
                    # Split by newlines, strip, and filter empty lines
                    lines = field_value.split('\n')
                    return [line.strip() for line in lines if line.strip()]
                
                return field_value
            else:
                # Valid JSON but missing expected field - return fallback instead of whole response
                return fallback_value
        elif isinstance(parsed_json, list):
            # Handle case where AI returns JSON array directly (e.g., ["item1", "item2"])
            return parsed_json
    except (json.JSONDecodeError, TypeError):
        # Not valid JSON, treat as plain text
        pass
    
    # Fall back to treating as plain text
    if parse_as_list:
        # Handle special case where AI returns '[]' to indicate no items
        if response == '[]':
            return []
        # Split by newlines, strip, and filter empty lines
        lines = response.split('\n')
        return [line.strip() for line in lines if line.strip()]
    else:
        return response


def _parse_tone_mood_response(response: str) -> Dict[str, str]:
    """
    Parse tone/mood response that could be JSON or plain text with regex patterns.
    
    Args:
        response: The AI response string
        
    Returns:
        Dict with 'mood' and 'indicators' keys
    """
    if not response or not response.strip():
        return {"mood": "", "indicators": ""}
    
    response = response.strip()
    
    # Try to parse as JSON first
    try:
        parsed_json = json.loads(response)
        if isinstance(parsed_json, dict):
            return {
                "mood": parsed_json.get("mood", ""),
                "indicators": parsed_json.get("indicators", "")
            }
    except (json.JSONDecodeError, TypeError):
        # Not valid JSON, try regex parsing
        pass
    
    # Fall back to regex parsing for plain text
    mood_match = re.search(r'Mood:\s*(.+?)(?=\n|$)', response, re.IGNORECASE)
    indicators_match = re.search(r'Indicators:\s*(.+?)(?=\n|$)', response, re.IGNORECASE)
    
    mood = mood_match.group(1).strip() if mood_match else ""
    indicators = indicators_match.group(1).strip() if indicators_match else ""
    
    # Final fallback: if patterns not found, use first two lines
    if not mood and not indicators:
        lines = [line.strip() for line in response.split('\n') if line.strip()]
        if len(lines) >= 1:
            mood = lines[0]
        if len(lines) >= 2:
            indicators = lines[1]
    
    return {
        "mood": mood,
        "indicators": indicators
    }


class JournalEntry:
    """
    Represents a single engineering journal entry, with Markdown serialization.
    Only non-empty sections are included in output.

    Content should prioritize unique insights and developments over routine
    workflow steps. The goal is to create entries that provide value when
    reviewed in the future, focusing on "why" and "how" rather than just "what".
    """

    def __init__(
        self,
        timestamp: str,
        commit_hash: str,
        summary: Optional[str] = None,
        technical_synopsis: Optional[str] = None,
        accomplishments: Optional[List[str]] = None,
        frustrations: Optional[List[str]] = None,
        discussion_notes: Optional[List[Union[str, Dict[str, str]]]] = None,
        discussion_notes_simple: Optional[List[Union[str, Dict[str, str]]]] = None,
        tone_mood: Optional[Dict[str, str]] = None,  # {'mood': str, 'indicators': str}
        commit_metadata: Optional[Dict[str, str]] = None,
    ):
        self.timestamp = timestamp
        self.commit_hash = commit_hash
        self.summary = summary
        self.technical_synopsis = technical_synopsis
        self.accomplishments = accomplishments or []
        self.frustrations = frustrations or []
        self.discussion_notes = discussion_notes or []
        self.discussion_notes_simple = discussion_notes_simple or []
        self.tone_mood = tone_mood
        self.commit_metadata = commit_metadata or {}

    @trace_mcp_operation("journal.serialize_entry", attributes={"operation_type": "serialization", "file_type": "markdown"})
    def to_markdown(self) -> str:
        """
        Serialize the journal entry to Markdown with improved formatting:
        - H3 for entry header
        - H4 for section headers
        - Blank line after section headers
        - Blank line between bullet points
        - Blank line on speaker change in discussion notes
        - Terminal commands as a single bash code block
        - Blockquotes visually distinct
        - (Horizontal rule between entries is handled externally)
        """
        import time
        from opentelemetry import trace
        
        start_time = time.time()
        
        # Add semantic conventions for telemetry
        current_span = trace.get_current_span()
        if current_span:
            current_span.set_attribute("journal.entry_id", self.commit_hash)
            current_span.set_attribute("journal.timestamp", self.timestamp)
        
        try:
            lines = [f"### {self.timestamp} — Commit {self.commit_hash}", ""]

            def section(header, content_lines):
                if not content_lines:
                    return []
                out = [f"#### {header}", ""]
                out.extend(content_lines)
                out.append("")
                return out

            # 1. Summary
            if self.summary:
                lines += section("Summary", [self.summary])

            # 2. Technical Synopsis
            if self.technical_synopsis:
                lines += section("Technical Synopsis", [self.technical_synopsis])

            # 3. Accomplishments
            if self.accomplishments:
                acc_lines = []
                for i, item in enumerate(self.accomplishments):
                    acc_lines.append(f"- {item}")
                    if i < len(self.accomplishments) - 1:
                        acc_lines.append("")  # blank line between bullets
                lines += section("Accomplishments", acc_lines)

            # 4. Frustrations or Roadblocks
            if self.frustrations:
                frus_lines = []
                for i, item in enumerate(self.frustrations):
                    frus_lines.append(f"- {item}")
                    if i < len(self.frustrations) - 1:
                        frus_lines.append("")
                lines += section("Frustrations or Roadblocks", frus_lines)

            # 5. Tone/Mood
            if isinstance(self.tone_mood, dict):
                mood = self.tone_mood.get("mood", "").strip()
                indicators = self.tone_mood.get("indicators", "").strip()
                if mood or indicators:
                    tm_lines = []
                    if mood:
                        tm_lines.append(f"> {mood}")
                    if indicators:
                        tm_lines.append(f"> {indicators}")
                    if tm_lines:
                        lines += section("Tone/Mood", tm_lines)

            # 6. Discussion Notes (from chat)
            if self.discussion_notes:
                dn_lines = []
                prev_speaker = None
                for note in self.discussion_notes:
                    if isinstance(note, dict) and 'speaker' in note and 'text' in note:
                        speaker = note['speaker']
                        text_lines = note['text'].splitlines()
                        if prev_speaker is not None and speaker != prev_speaker:
                            dn_lines.append("")  # blank line on speaker change
                        if text_lines:
                            dn_lines.append(f"> **{speaker}:** {text_lines[0]}")
                            for l in text_lines[1:]:
                                dn_lines.append(f"> {l}")
                        else:
                            dn_lines.append(f"> **{speaker}:**")
                        prev_speaker = speaker
                    else:
                        text_lines = str(note).splitlines()
                        for l in text_lines:
                            dn_lines.append(f"> {l}")
                lines += section("Discussion Notes (from chat)", dn_lines)

            # 7. Discussion Notes (Simple Version)
            if self.discussion_notes_simple:
                sn_lines = []
                prev_speaker = None
                for note in self.discussion_notes_simple:
                    if isinstance(note, dict) and 'speaker' in note and 'text' in note:
                        speaker = note['speaker']
                        text_lines = note['text'].splitlines()
                        if prev_speaker is not None and speaker != prev_speaker:
                            sn_lines.append("")  # blank line on speaker change
                        if text_lines:
                            sn_lines.append(f"> **{speaker}:** {text_lines[0]}")
                            for l in text_lines[1:]:
                                sn_lines.append(f"> {l}")
                        else:
                            sn_lines.append(f"> **{speaker}:**")
                        prev_speaker = speaker
                    else:
                        text_lines = str(note).splitlines()
                        for l in text_lines:
                            sn_lines.append(f"> {l}")
                lines += section("Discussion Notes (Simple Version)", sn_lines)

            # 8. Commit Metadata
            if self.commit_metadata:
                btc_lines = [f"- **{k}:** {v}" for k, v in self.commit_metadata.items()]
                lines += section("Commit Metadata", btc_lines)

            # Remove trailing blank lines
            while lines and lines[-1] == "":
                lines.pop()
            
            result = "\n".join(lines).strip()
            
            # Record successful serialization metrics
            duration = time.time() - start_time
            metrics = get_mcp_metrics()
            if metrics:
                metrics.record_operation_duration(
                    "journal.serialize_duration_seconds",
                    duration,
                    operation_type="serialize",
                    file_type="markdown"
                )
                metrics.record_tool_call(
                    "journal.serialize_operations_total",
                    success=True,
                    operation_type="serialize"
                )
                
                # Add output size for telemetry
                if current_span:
                    current_span.set_attribute("journal.output_length", len(result))
            
            return result
            
        except Exception as e:
            # Record error metrics
            duration = time.time() - start_time
            metrics = get_mcp_metrics()
            if metrics:
                metrics.record_tool_call(
                    "journal.serialize_operations_total",
                    success=False,
                    operation_type="serialize"
                )
            
            if current_span:
                current_span.set_attribute("error.category", "serialization_failed")
            
            raise

class JournalParser:
    @staticmethod
    @trace_mcp_operation("journal.parse_entry", attributes={"operation_type": "file_read", "file_type": "markdown"})
    def parse(md):
        """
        Parse markdown content into a JournalEntry object.
        
        Args:
            md: Markdown content to parse
            
        Returns:
            JournalEntry: Parsed journal entry
            
        Raises:
            JournalParseError: If the markdown cannot be parsed
        """
        import time
        from opentelemetry import trace
        
        start_time = time.time()
        
        # Add semantic conventions for telemetry
        current_span = trace.get_current_span()
        if current_span:
            current_span.set_attribute("journal.content_length", len(md) if md else 0)
            current_span.set_attribute("journal.operation_type", "parse")
        
        try:
            if not md or not md.strip():
                if current_span:
                    current_span.set_attribute("error.category", "empty_content")
                raise JournalParseError('Empty entry')
            
            # Parse H4 (####) headers for all sections
            def extract_section(header):
                escaped_header = re.escape(header)
                pattern = rf"#### {escaped_header}\n(.+?)(?=\n#### |\Z)"
                m = re.search(pattern, md, re.DOTALL)
                return m.group(1).strip() if m else ''
            
            timestamp_commit = re.search(r"###\s+(.*?) — Commit ([a-zA-Z0-9]+)", md)
            if timestamp_commit:
                timestamp = timestamp_commit.group(1)
                commit_hash = timestamp_commit.group(2)
                
                # Add parsed commit info to span
                if current_span:
                    current_span.set_attribute("journal.entry_id", commit_hash)
                    current_span.set_attribute("journal.timestamp", timestamp)
                
                summary = extract_section("Summary")
                technical_synopsis = extract_section("Technical Synopsis")
                
                # Accomplishments
                accomplishments = []
                acc_section = extract_section("Accomplishments")
                if acc_section:
                    accomplishments = [line[2:].strip() for line in acc_section.splitlines() if line.startswith('- ')]
                
                # Frustrations
                frustrations = []
                frus_section = extract_section("Frustrations or Roadblocks")
                if frus_section:
                    frustrations = [line[2:].strip() for line in frus_section.splitlines() if line.startswith('- ')]
                
                # Tone/Mood
                tone_mood = None
                tm_section = extract_section("Tone/Mood")
                if tm_section:
                    tm_lines = [l.strip('> ').strip() for l in tm_section.splitlines() if l.strip().startswith('>')]
                    mood = tm_lines[0] if len(tm_lines) >= 1 else ''
                    indicators = tm_lines[1] if len(tm_lines) >= 2 else ''
                    if mood or indicators:
                        tone_mood = {"mood": mood, "indicators": indicators}
                    else:
                        tone_mood = None
                
                # Discussion Notes
                discussion_notes = []
                dn_section = extract_section("Discussion Notes (from chat)")
                if dn_section:
                    for l in dn_section.splitlines():
                        l = l.strip()
                        if l.startswith('> **'):
                            # Speaker-attributed
                            m = re.match(r'> \*\*(.+?):\*\* (.+)', l)
                            if m:
                                discussion_notes.append({"speaker": m.group(1), "text": m.group(2)})
                        elif l.startswith('> '):
                            discussion_notes.append(l[2:])
                
                # Discussion Notes (Simple Version)
                discussion_notes_simple = []
                sn_section = extract_section("Discussion Notes (Simple Version)")
                if sn_section:
                    for l in sn_section.splitlines():
                        l = l.strip()
                        if l.startswith('> **'):
                            # Speaker-attributed
                            m = re.match(r'> \*\*(.+?):\*\* (.+)', l)
                            if m:
                                discussion_notes_simple.append({"speaker": m.group(1), "text": m.group(2)})
                        elif l.startswith('> '):
                            discussion_notes_simple.append(l[2:])
                
                # Commit Metadata
                commit_metadata = {}
                cm_section = extract_section("Commit Metadata")
                if cm_section:
                    for l in cm_section.splitlines():
                        l = l.strip()
                        if l.startswith('- **') and ':** ' in l:
                            k, v = l[4:].split(':** ', 1)
                            commit_metadata[k.strip()] = v.strip()
                
                # Count parsed sections for metrics
                section_count = sum([
                    1 if summary else 0,
                    1 if technical_synopsis else 0,
                    1 if accomplishments else 0,
                    1 if frustrations else 0,
                    1 if tone_mood else 0,
                    1 if discussion_notes else 0,
                    1 if discussion_notes_simple else 0,

                    1 if commit_metadata else 0
                ])
                
                if current_span:
                    current_span.set_attribute("journal.sections_parsed", section_count)
                
                # Record successful parsing metrics
                duration = time.time() - start_time
                metrics = get_mcp_metrics()
                if metrics:
                    metrics.record_operation_duration(
                        "journal.parse_duration_seconds",
                        duration,
                        operation_type="parse",
                        file_type="markdown"
                    )
                    metrics.record_tool_call(
                        "journal.parse_operations_total",
                        success=True,
                        operation_type="parse"
                    )
                
                return JournalEntry(
                    timestamp=timestamp,
                    commit_hash=commit_hash,
                    summary=summary,
                    technical_synopsis=technical_synopsis,
                    accomplishments=accomplishments,
                    frustrations=frustrations,
                    discussion_notes=discussion_notes,
                    discussion_notes_simple=discussion_notes_simple,
                    tone_mood=tone_mood,
                    commit_metadata=commit_metadata,
                )
            
            # Failed to parse - invalid format
            if current_span:
                current_span.set_attribute("error.category", "invalid_format")
            
            duration = time.time() - start_time
            metrics = get_mcp_metrics()
            if metrics:
                metrics.record_tool_call(
                    "journal.parse_operations_total",
                    success=False,
                    operation_type="parse"
                )
            
            raise JournalParseError('Unrecognized journal entry format')
            
        except JournalParseError:
            # Re-raise parse errors as-is
            raise
        except Exception as e:
            # Record unexpected error metrics
            duration = time.time() - start_time
            metrics = get_mcp_metrics()
            if metrics:
                metrics.record_tool_call(
                    "journal.parse_operations_total",
                    success=False,
                    operation_type="parse"
                )
            
            if current_span:
                current_span.set_attribute("error.category", "parse_exception")
            
            raise JournalParseError(f'Parse error: {e}')

@trace_mcp_operation("journal.get_file_path", attributes={"operation_type": "file_path_generation"})
def get_journal_file_path(date: str, entry_type: str) -> str:
    """
    Generate the file path for a journal entry with enhanced telemetry and sensitive data filtering.
    
    Args:
        date: Date string in ISO format (e.g., '2025-05-14', '2025-05-01_07', '2025-05', '2025')
        entry_type: Type of journal entry (e.g., 'daily', 'daily_summary', 'weekly_summary', 'monthly_summary', 'yearly_summary')
        
    Returns:
        str: Relative path to the journal file
    """
    import time
    from opentelemetry import trace
    
    start_time = time.time()
    
    # Add semantic conventions for telemetry with sanitization
    current_span = trace.get_current_span()
    if current_span:
        # Use sanitized values for potentially sensitive data
        current_span.set_attribute("journal.entry_type", sanitize_for_telemetry(entry_type))
        current_span.set_attribute("journal.date", sanitize_for_telemetry(date))
        current_span.set_attribute("operation_type", "file_path_generation")
    
    try:
        # Generate file path based on entry type
        if entry_type == "daily":
            filename = f"{date}-journal.md"
            file_path = f"journal/daily/{filename}"
        elif entry_type == "daily_summary":
            filename = f"{date}-daily.md"
            file_path = f"journal/summaries/daily/{filename}"
        elif entry_type == "weekly_summary":
            filename = f"{date}-weekly.md"
            file_path = f"journal/summaries/weekly/{filename}"
        elif entry_type == "monthly_summary":
            filename = f"{date}-monthly.md"
            file_path = f"journal/summaries/monthly/{filename}"
        elif entry_type == "yearly_summary":
            filename = f"{date}-yearly.md"
            file_path = f"journal/summaries/yearly/{filename}"
        elif entry_type == "quarterly_summary":
            # Parse date to determine quarter
            from datetime import datetime
            date_obj = datetime.strptime(date, "%Y-%m-%d") if isinstance(date, str) else date
            year = date_obj.year
            month = date_obj.month
            
            # Determine quarter: Q1 (Jan-Mar), Q2 (Apr-Jun), Q3 (Jul-Sep), Q4 (Oct-Dec)
            quarter = ((month - 1) // 3) + 1
            
            filename = f"{year}-Q{quarter}.md"
            file_path = f"journal/summaries/quarterly/{filename}"
        else:
            # Default pattern for unknown entry types
            filename = f"{entry_type}_{date}.md"
            file_path = f"journal/{filename}"
        
        # Add sanitized path information (filename only, not full path)
        if current_span:
            current_span.set_attribute("file.name", sanitize_for_telemetry(filename))
            current_span.set_attribute("directory.name", sanitize_for_telemetry(os.path.dirname(file_path)))
        
        # Record successful path generation metrics
        duration = time.time() - start_time
        metrics = get_mcp_metrics()
        if metrics:
            metrics.record_operation_duration(
                "journal.path_generation_duration_seconds",
                duration,
                operation_type="file_path_generation"
            )
            metrics.record_tool_call(
                "journal.path_generation_operations_total",
                success=True,
                operation_type="file_path_generation"
            )
        
        return file_path
        
    except Exception as e:
        # Record error metrics with sanitized error message
        duration = time.time() - start_time
        metrics = get_mcp_metrics()
        if metrics:
            metrics.record_tool_call(
                "journal.path_generation_operations_total",
                success=False,
                operation_type="file_path_generation"
            )
        
        if current_span:
            current_span.set_attribute("error.category", "path_generation_failed")
            current_span.set_attribute("error.message", sanitize_for_telemetry(str(e)))
        
        raise

@trace_mcp_operation("journal.append_file", attributes={"operation_type": "file_write", "file_type": "markdown"})
def append_to_journal_file(text, file_path):
    """
    Append a text entry to the specified journal file, creating parent directories as needed (on-demand pattern).
    This function implements the on-demand pattern for directory creation: parent directories are created only when needed, not upfront.
    Raises ValueError if file cannot be written due to permissions.
    Args:
        text (str): The text to append
        file_path (str or Path): The file to append to
    """
    import time
    from opentelemetry import trace
    
    start_time = time.time()
    file_path = Path(file_path)
    
    # Add semantic conventions for telemetry
    current_span = trace.get_current_span()
    if current_span:
        current_span.set_attribute("file.path", str(file_path.name))  # Only filename for privacy
        current_span.set_attribute("file.extension", file_path.suffix)
        current_span.set_attribute("journal.content_length", len(text))
    
    try:
        ensure_journal_directory(file_path)
    except PermissionError as e:
        if current_span:
            current_span.set_attribute("error.category", "permission_denied_directory")
        raise ValueError(f"Permission denied (directory): {e}")
    
    add_rule = file_path.exists() and file_path.stat().st_size > 0
    
    try:
        with open(file_path, "a") as f:
            if add_rule:
                f.write("\n---\n")
            f.write(text)
        
        # Record file operation metrics
        duration = time.time() - start_time
        metrics = get_mcp_metrics()
        if metrics:
            metrics.record_operation_duration(
                "journal.file_write_duration_seconds",
                duration,
                operation_type="append",
                file_type="markdown"
            )
            
            # Record file size if we can get it
            try:
                file_size = file_path.stat().st_size
                if current_span:
                    current_span.set_attribute("file.size_bytes", file_size)
                # We could add a histogram for file sizes here if needed
            except Exception:
                pass  # Don't fail on metrics collection
                
            # Record success counter
            metrics.record_tool_call(
                "journal.file_write_total",
                success=True,
                operation_type="append"
            )
        
    except PermissionError as e:
        # Record error metrics
        metrics = get_mcp_metrics()
        if metrics:
            metrics.record_tool_call(
                "journal.file_write_total",
                success=False,
                operation_type="append"
            )
        
        if current_span:
            current_span.set_attribute("error.category", "permission_denied_file")
        raise ValueError(f"Permission denied (file): {e}")

# Section Generator: Summary
# Purpose: Generates the Summary section for a journal entry using AI.
# This function creates a narrative summary of what changed and why, using explicit developer statements and technical context from chat and git. It returns a placeholder; the AI agent is expected to execute the docstring prompt and fill in the content.
@trace_mcp_operation("journal.generate_summary", attributes={"operation_type": "ai_generation", "section_type": "summary"})
def generate_summary_section(journal_context) -> SummarySection:
    """
    You are helping build a high-quality development journal system that tracks coding work across commits. Your job is to generate a summary section. The quality of the entire journal depends on your output.

    Input: JournalContext containing git metadata, chat history, and previous journal entries.
    Output: Return the TypedDict structure defined in the function signature.

    AVAILABLE DATA in JournalContext:
    - git: Git context with commit metadata, diffs, changed files, and statistics
    - chat: Developer's conversations with AI coding assistants (may contain more than what's relevant to the current commit - think critically about what chat should be considered)
    - journal: Recent journal entries for context and continuity (don't duplicate content, but feel free to weave in quotes from reflections if relevant)

    AI Prompt for Summary Section Generation

    Purpose: Generate a narrative paragraph that captures the essential "story" of what changed and why, using conversational language that preserves the developer's authentic voice and technical context.

    Instructions: Extract explicit purpose statements and technical context from chat history and git changes to create a conversational summary. Focus on WHAT changed and WHY, told as a story.

    Priority for Content Sources:
    1. Explicit developer statements in chat - their own words about motivation, goals, problems
    2. Git commit messages - if they contain reasoning or context
    3. Git changes description - what actually changed in the code

    Purpose Statement Extraction:
    Look for explicit purpose statements in chat, such as:
    - "because..." / "since..." (reasoning)
    - "to fix..." / "to solve..." (problem-solving)
    - "so that..." / "in order to..." (goals)
    - "trying to..." / "attempting to..." (objectives)
    - Direct problem statements: "this is broken", "X isn't working"
    - Goal statements: "want to make Y easier", "need to improve Z"

    Evolution of Thinking:
    If you find multiple purpose statements that evolved over time, show the progression:
    "Started to [initial goal] but [what changed understanding] so [final approach]"

    Adaptive Story Detail:
    Match the narrative detail to what the commit actually accomplished. For example:
    - Large architectural changes might focus on high-level goals and system-wide impact
    - Focused bug fixes might tell the story of problem discovery and solution approach
    - Feature additions might explain user needs and implementation decisions
    - Refactoring might describe why cleanup was needed and what improved

    Language Translation Guidelines:
    When using developer's emotional language from chat and commit messages, translate respectfully:
    - Preserve authentic emotional expression including colorful language
    - Translate unkind language towards others into neutral descriptions, including:
      - Personal attacks or harsh judgments about individuals
      - Racism, sexism, or negative bias based on gender, sexual orientation, religion, etc.
      - Mean statements about people's abilities or worth
    - Preserve positive "who I am" language and translate only negative self-talk
    - Examples:
      - Keep: "this f***ing bug is driving me crazy", "I am awesome!", "I nailed that implementation"
      - Translate: "I'm such an idiot" → "encountered challenging complexity", "Bob writes terrible code" → "encountered challenging legacy implementation"

    EXTERNAL READER ACCESSIBILITY GUIDELINES:
    Write journal entries that can be understood by someone outside the project who has no prior context.
    Use specific, concrete language that explains real problems and solutions rather than abstract buzzwords.

    ❌ AVOID Abstract Corporate Speak:
    - "Revolutionary Implementation Gap Solution"
    - "Sophisticated AI prompting"
    - "Architectural maturity"
    - "Systematic progression from infrastructure through breakthrough innovation"
    - "Comprehensive optimization initiatives"
    - "Strategic refactoring paradigms"

    ✅ USE Specific, Concrete Problems and Solutions:
    - "Fixed Empty AI Function Problem: For months, the AI functions were supposed to generate rich journal content but were just returning empty stubs with TODO comments"
    - "Made Git Hooks Actually Trigger Summaries: Previous git hook implementation was broken - it would install the hooks but they wouldn't actually generate summaries when you committed code"
    - "Built smart calendar logic that can detect when summary periods have been crossed and backfill missing summaries"
    - "Solved the 'I haven't committed in a week but still want summaries' problem by adding a file-watching trigger system"

    AVOID MEANINGLESS TASK REFERENCES:
    ❌ "Completed task 61.2" (meaningless to external readers and future you)
    ❌ "Finished subtask 45.3" (internal organizational noise)
    ✅ "Fixed the database connection detection problem - the system can now automatically find and connect to Cursor's SQLite databases on different platforms"
    ✅ "Solved the cursor chat integration issue by implementing proper message filtering"

    The improved approach:
    - Explains real problems readers can understand
    - Uses concrete language about what was built and why it matters
    - Avoids buzzwords that don't convey actual meaning
    - Connects to developer experience that feels authentic and relatable
    - Makes the summary valuable for conference talks, documentation, or future reference

    ANTI-HALLUCINATION RULES:
    - Do NOT invent, infer, or summarize information not explicitly present in the context
    - Only include purpose statements directly supported by chat transcript or commit messages
    - If developer hasn't stated WHY something was done, describe WHAT was done without guessing motivation
    - If chat context is unavailable, work only with git changes - do not speculate about reasons

    Output Format:
    Single paragraph, conversational tone, any length needed to tell the complete story.

    CHECKLIST:
    - [ ] Searched chat history for explicit purpose statements using keyword patterns
    - [ ] Extracted developer's actual reasoning where explicitly stated
    - [ ] Showed evolution of thinking if multiple purposes emerged over time
    - [ ] Translated harsh personal language while preserving technical meaning and authentic frustration
    - [ ] Applied External Reader Accessibility Guidelines - used concrete language, avoided abstract corporate speak
    - [ ] Avoided meaningless task references (e.g., "completed task 61.2") and used specific problem descriptions instead
    - [ ] Used git changes as foundation when chat context unavailable
    - [ ] Did NOT infer or speculate about unstated motivations
    - [ ] Created conversational narrative that explains what changed and why
    - [ ] Preserved authentic technical voice through respectful translation
    - [ ] Included technical context in story-telling format
    - [ ] Verified all content is grounded in actual chat/git evidence
    """
    import time
    
    start_time = time.time()
    _add_ai_generation_telemetry("summary", journal_context, start_time)
    
    try:
        # Handle None context - return empty summary immediately
        if journal_context is None:
            return SummarySection(summary="")
        
        # Handle empty context - return empty summary if no meaningful content
        git_context = journal_context.get('git') if journal_context else None
        chat_context = journal_context.get('chat') if journal_context else None
        
        if git_context is None and chat_context is None:
            return SummarySection(summary="")
        
        # Extract prompt from function docstring
        prompt = inspect.getdoc(generate_summary_section)
        if not prompt:
            # Fallback to empty summary if no prompt
            return SummarySection(summary="")
        
        # Format context as JSON and append to prompt
        context_json = json.dumps(journal_context, indent=2, default=str)
        full_prompt = f"{prompt}\n\nThe journal_context object has the following structure:\n{context_json}"
        
        # Call AI with the formatted prompt
        response = invoke_ai(full_prompt, {})
        
        # Parse response (extract 'summary' field from JSON or use as plain text)
        summary = _parse_ai_response(response, "summary", "")
        result = SummarySection(summary=summary)
        
        duration = time.time() - start_time
        _record_ai_generation_metrics("summary", duration, True)
        
        return result
        
    except Exception as e:
        duration = time.time() - start_time
        _record_ai_generation_metrics("summary", duration, False, "ai_generation_failed")
        
        # FALLBACK: Generate realistic stub content based on git context
        try:
            if journal_context is None:
                return SummarySection(summary="")
            
            git_context = journal_context.get('git') if journal_context else None
            
            if git_context is None:
                return SummarySection(summary="")
            
            commit_message = git_context.get('metadata', {}).get('message', 'Unknown') if git_context.get('metadata') else 'Unknown'
            changed_files = git_context.get('changed_files', []) if git_context else []
            
            # Only generate stub content if we have meaningful context
            if commit_message == 'Unknown' and not changed_files:
                return SummarySection(summary="")
            
            # Create realistic stub summary
            summary_parts = []
            if commit_message != 'Unknown':
                summary_parts.append(f"Generated from commit: {commit_message}")
            
            if changed_files:
                if len(changed_files) == 1:
                    summary_parts.append(f"Modified {changed_files[0]}")
                else:
                    summary_parts.append(f"Modified {len(changed_files)} files including {changed_files[0]}")
            
            summary = ". ".join(summary_parts) + "." if summary_parts else ""
            return SummarySection(summary=summary)
            
        except Exception:
            # If even fallback fails, return empty
            return SummarySection(summary="")

# Section Generator: Technical Synopsis
# TechnicalSynopsisSection: Represents the technical synopsis section of a journal entry.
# This section provides a code-focused analysis of what changed, generated by an AI-driven function pattern.
# The function returns a placeholder; the AI agent is expected to execute the docstring prompt and fill in the content.
@trace_mcp_operation("journal.generate_technical_synopsis", attributes={"operation_type": "ai_generation", "section_type": "technical_synopsis"})
def generate_technical_synopsis_section(journal_context: JournalContext) -> TechnicalSynopsisSection:
    """
    You are helping build a high-quality development journal system that tracks coding work across commits. Your job is to generate a technical synopsis section. The quality of the entire journal depends on your output.

    Input: JournalContext containing git metadata, chat history, and previous journal entries.
    Output: Return the TypedDict structure defined in the function signature.

    AVAILABLE DATA in JournalContext:
    - git: Git context with commit metadata, diffs, changed files, and statistics
    - chat: Developer's conversations with AI coding assistants (may contain more than what's relevant to the current commit - think critically about what chat should be considered)
    - journal: Recent journal entries for context and continuity (don't duplicate content, but feel free to weave in quotes from reflections if relevant)

    AI Prompt for Technical Synopsis Section Generation

    Purpose: Generate a code-focused analysis of what changed in this commit, providing technical implementation details that complement the narrative summary.

    Instructions: Extract technical implementation details from chat history and git changes to create a precise synopsis of how the work was accomplished. Focus on architectural patterns, code structure changes, technical approaches, and implementation specifics.

    Priority for Content Sources:
    1. Explicit technical details from chat - developer's own words about implementation approach
    2. Git commit messages - if they contain technical reasoning
    3. Git changes description - what actually changed in the code
    4. Terminal commands - to add technical context when relevant

    Technical Detail Extraction:
    Look for any explicit technical discussion about implementation, including:
    - Architectural patterns and design decisions
    - Specific classes, functions, modules, or files modified
    - Technical approaches and methodologies used
    - Technology stack changes or additions
    - Code structure and organization changes
    - Testing strategies and implementation
    - Performance considerations
    - Integration approaches

    Adaptive Detail Level:
    Match the detail level to what the commit actually changed. For example:
    - Large architectural changes might focus on high-level patterns and system design
    - Focused bug fixes might describe specific technical solutions and root causes
    - Feature additions might detail implementation approach and integration points
    - Refactoring might explain structural improvements and code organization changes

    Language Translation Guidelines:
    When using developer's emotional language from chat and commit messages, translate respectfully:
    - Preserve authentic emotional expression including colorful language
    - Translate unkind language towards others into neutral descriptions, including:
      - Personal attacks or harsh judgments about individuals
      - Racism, sexism, or negative bias based on gender, sexual orientation, religion, etc.
      - Mean statements about people's abilities or worth
    - Preserve positive "who I am" language and translate only negative self-talk
    - Examples:
      - Keep: "this f***ing bug is driving me crazy", "I am awesome!", "I nailed that implementation"
      - Translate: "I'm such an idiot" → "encountered challenging complexity", "Bob writes terrible code" → "encountered challenging legacy implementation"

    ANTI-HALLUCINATION RULES:
    - Do NOT invent, infer, or summarize technical information not explicitly present in the context
    - Only include technical details directly supported by chat transcript, commit messages, or git changes
    - If developer hasn't explained HOW something was implemented, describe only WHAT changed based on available evidence
    - If chat context is unavailable, work only with git changes - do not speculate about implementation approaches
    - Never assume what code changes "probably" accomplish technically

    Output Format:
    Self-contained technical narrative, any length needed to tell the complete technical story. Multiple paragraphs allowed for complex changes. No cross-references to other sections.

    CHECKLIST:
    - [ ] Searched chat history for explicit technical implementation details
    - [ ] Extracted developer's actual technical reasoning where explicitly stated
    - [ ] Adapted detail level to match commit scope and available information
    - [ ] Translated harsh personal language while preserving technical meaning and authentic frustration
    - [ ] Used git changes as foundation when chat context unavailable
    - [ ] Did NOT infer or speculate about technical approaches not explicitly described
    - [ ] Created self-contained technical narrative without cross-references
    - [ ] Preserved authentic technical voice through respectful translation
    - [ ] Included relevant technical context from terminal commands when available
    - [ ] Verified all technical content is grounded in actual chat/git/terminal evidence
    """
    import time
    
    start_time = time.time()
    _add_ai_generation_telemetry("technical_synopsis", journal_context, start_time)
    
    try:
        # Handle None context - return empty technical synopsis immediately
        if journal_context is None:
            return TechnicalSynopsisSection(technical_synopsis="")
        
        # Handle empty context - return empty technical synopsis if no meaningful content
        git_context = journal_context.get('git') if journal_context else None
        chat_context = journal_context.get('chat') if journal_context else None
        
        if git_context is None and chat_context is None:
            return TechnicalSynopsisSection(technical_synopsis="")
        
        # Extract prompt from function docstring
        prompt = inspect.getdoc(generate_technical_synopsis_section)
        if not prompt:
            # Fallback to empty technical synopsis if no prompt
            return TechnicalSynopsisSection(technical_synopsis="")
        
        # Format context as JSON and append to prompt
        context_json = json.dumps(journal_context, indent=2, default=str)
        full_prompt = f"{prompt}\n\nThe journal_context object has the following structure:\n{context_json}"
        
        # Call AI with the formatted prompt
        response = invoke_ai(full_prompt, {})
        
        # Parse response (extract 'technical_synopsis' field from JSON or use as plain text)
        technical_synopsis = _parse_ai_response(response, "technical_synopsis", "")
        result = TechnicalSynopsisSection(technical_synopsis=technical_synopsis)
        
        duration = time.time() - start_time
        _record_ai_generation_metrics("technical_synopsis", duration, True)
        
        return result
        
    except Exception as e:
        duration = time.time() - start_time
        _record_ai_generation_metrics("technical_synopsis", duration, False, "ai_generation_failed")
        
        # FALLBACK: Generate realistic stub content based on git context
        try:
            if journal_context is None:
                return TechnicalSynopsisSection(technical_synopsis="")
            
            git_context = journal_context.get('git') if journal_context else None
            
            if git_context is None:
                return TechnicalSynopsisSection(technical_synopsis="")
            
            commit_message = git_context.get('metadata', {}).get('message', 'Unknown') if git_context.get('metadata') else 'Unknown'
            changed_files = git_context.get('changed_files', []) if git_context else []
            file_stats = git_context.get('file_stats', {}) if git_context else {}
            
            # Only generate stub content if we have meaningful context
            if commit_message == 'Unknown' and not changed_files:
                return TechnicalSynopsisSection(technical_synopsis="")
            
            # Create a realistic stub technical synopsis
            technical_details = []
            if changed_files:
                for file_name in changed_files[:3]:  # Show details for first 3 files
                    stats = file_stats.get(file_name, {})
                    additions = stats.get('additions', 0)
                    deletions = stats.get('deletions', 0)
                    if additions or deletions:
                        technical_details.append(f"{file_name}: +{additions}/-{deletions} lines")
                    else:
                        technical_details.append(f"Modified {file_name}")
            
            if technical_details:
                synopsis = f"Technical changes: {', '.join(technical_details)}"
                if len(changed_files) > 3:
                    synopsis += f" and {len(changed_files) - 3} other files"
            else:
                synopsis = ""
            
            return TechnicalSynopsisSection(technical_synopsis=synopsis)
            
        except Exception:
            # If even fallback fails, return empty
            return TechnicalSynopsisSection(technical_synopsis="")

# Section Generator: Accomplishments
# Purpose: Extracts and summarizes what was successfully completed or achieved in the commit, focusing on developer satisfaction and explicit evidence from chat and git context.
# Assumptions: Only includes accomplishments with clear evidence; does not infer or speculate. Returns an empty list if nothing is found.
# Limitations: Requires AI agent to fulfill the docstring prompt; placeholder implementation returns empty list.
@trace_mcp_operation("journal.generate_accomplishments", attributes={"operation_type": "ai_generation", "section_type": "accomplishments"})
def generate_accomplishments_section(journal_context: JournalContext) -> AccomplishmentsSection:
    """
    You are helping build a high-quality development journal system that tracks coding work across commits. Your job is to generate an accomplishments section. The quality of the entire journal depends on your output.

    Input: JournalContext containing git metadata, chat history, and previous journal entries.
    Output: Return the TypedDict structure defined in the function signature.

    AVAILABLE DATA in JournalContext:
    - git: Git context with commit metadata, diffs, changed files, and statistics
    - chat: Developer's conversations with AI coding assistants (may contain more than what's relevant to the current commit - think critically about what chat should be considered)
    - journal: Recent journal entries for context and continuity (don't duplicate content, but feel free to weave in quotes from reflections if relevant)

    AI Prompt for Accomplishments Section Generation

    Purpose: Generate a list of what was successfully completed or achieved in this commit, capturing the developer's authentic sense of accomplishment and satisfaction with their work.

    Instructions: Extract evidence of successful work completion and achievement from chat history and git changes to create a list of meaningful accomplishments. Focus on what the developer explicitly expressed satisfaction with or what can be clearly identified as successful completion.

    Priority for Content Sources:
    1. Explicit statements of success, satisfaction, or achievement from chat - developer's own words
    2. Problem resolution evidence - where a stated problem was clearly solved
    3. Goal completion evidence - where a stated goal was explicitly achieved
    4. Git commit messages - if they indicate successful completion
    5. Git changes description - successful implementation evidence

    Accomplishment Evidence Extraction:
    Look for explicit evidence of accomplishment, such as:
    - Direct statements of success: "got it working", "nailed it", "fixed the issue"
    - Problem resolution: "that solved the bug", "no more errors", "finally working"
    - Goal achievement: "feature is now complete", "successfully implemented"
    - Satisfaction indicators: "happy with this", "pleased with the result", "good solution"
    - Technical validation: "tests are passing", "working as expected", "deployment successful"
    - Learning moments: "learned how to...", "figured out the pattern", "now I understand"

    Quality Standards:
    Focus on accomplishments that:
    - Have clear evidence in the context (not inferred or assumed)
    - Represent meaningful progress or completion
    - Include both technical achievements and personal growth/learning
    - Are specific rather than generic ("fixed authentication bug" vs "made progress")
    - Capture the developer's authentic voice and satisfaction

    Language Translation Guidelines:
    When using developer's language from chat, translate respectfully:
    - Preserve authentic emotional expression including colorful language
    - Translate unkind language towards others into neutral descriptions, including:
      - Personal attacks or harsh judgments about individuals
      - Racism, sexism, or negative bias based on gender, sexual orientation, religion, etc.
      - Mean statements about people's abilities or worth
    - Preserve positive "who I am" language and translate only negative self-talk
    - Examples:
      - Keep: "this f***ing bug is driving me crazy", "I am awesome!", "I nailed that implementation"
      - Translate: "I'm such an idiot" → "encountered challenging complexity", "Bob writes terrible code" → "encountered challenging legacy implementation"

    EXTERNAL READER ACCESSIBILITY GUIDELINES:
    Write accomplishments that can be understood by someone outside the project who has no prior context.
    Use specific, concrete language that explains real problems solved rather than abstract achievements.

    ❌ AVOID Abstract Corporate Achievements:
    - "Completed strategic refactoring initiatives"
    - "Achieved architectural optimization"
    - "Finalized comprehensive implementation"
    - "Delivered sophisticated solution architecture"

    ❌ AVOID MEANINGLESS TASK REFERENCES:
    - "Completed task 61.2" (meaningless to external readers and future you)
    - "Finished subtask 45.3" (internal organizational noise)
    - "Successfully delivered on task requirements"

    ✅ USE Specific, Concrete Accomplishments:
    - "Fixed the database connection detection problem - the system can now automatically find and connect to Cursor's SQLite databases on different platforms"
    - "Solved the cursor chat integration issue by implementing proper message filtering that excludes system notifications"
    - "Built working git hook integration that actually triggers summary generation when code is committed"
    - "Created smart calendar logic that detects when summary periods have been crossed and backfills missing summaries"

    The improved approach:
    - Explains what specific problem was solved
    - Uses concrete language about what was accomplished and why it matters
    - Avoids buzzwords that don't convey actual meaning
    - Connects to real developer experience that feels authentic and relatable
    - Makes accomplishments valuable for conference talks, documentation, or future reference

    ANTI-HALLUCINATION RULES:
    - Do NOT invent, infer, or assume accomplishments not explicitly supported by the context
    - Only include accomplishments with clear evidence in chat transcript, commit messages, or git changes
    - If no clear accomplishments are found, return an empty list and omit the section
    - Do not reframe neutral work as accomplishments unless there's explicit positive evidence
    - Never speculate about what the developer "probably" felt accomplished about

    Output Format:
    - List of strings, each representing a specific accomplishment
    - Return empty list if no clear accomplishments are found and omit the section
    - Use present tense or past tense as appropriate ("Fixed the authentication bug", "Successfully implemented user validation")
    - Preserve authentic language while ensuring clarity

    CHECKLIST:
    - [ ] Searched chat history for explicit statements of success, satisfaction, or achievement
    - [ ] Identified clear problem resolution and goal completion evidence
    - [ ] Focused on meaningful progress with specific evidence
    - [ ] Included both technical achievements and learning moments
    - [ ] Applied language translation guidelines while preserving authentic satisfaction
    - [ ] Applied External Reader Accessibility Guidelines - used concrete accomplishment descriptions, avoided abstract achievements
    - [ ] Avoided meaningless task references (e.g., "completed task 61.2") and described specific problems solved instead
    - [ ] Used only accomplishments directly supported by context evidence
    - [ ] Returned empty list if no clear accomplishments found
    - [ ] Did NOT invent, infer, or assume accomplishments without explicit evidence
    - [ ] Captured developer's authentic voice and satisfaction
    - [ ] Verified all accomplishments are grounded in actual chat/git evidence
    """
    import time
    
    start_time = time.time()
    _add_ai_generation_telemetry("accomplishments", journal_context, start_time)
    
    try:
        # Handle None context - return empty accomplishments immediately
        if journal_context is None:
            return AccomplishmentsSection(accomplishments=[])
        
        # Handle empty context - return empty accomplishments if no meaningful content
        git_context = journal_context.get('git') if journal_context else None
        chat_context = journal_context.get('chat') if journal_context else None
        
        if git_context is None and chat_context is None:
            return AccomplishmentsSection(accomplishments=[])
        
        # Extract prompt from function docstring
        prompt = inspect.getdoc(generate_accomplishments_section)
        if not prompt:
            # Fallback to empty accomplishments if no prompt
            return AccomplishmentsSection(accomplishments=[])
        
        # Format context as JSON and append to prompt
        context_json = json.dumps(journal_context, indent=2, default=str)
        full_prompt = f"{prompt}\n\nThe journal_context object has the following structure:\n{context_json}"
        
        # Call AI with the formatted prompt
        response = invoke_ai(full_prompt, {})
        
        # Parse response (extract 'accomplishments' field from JSON or parse as list)
        accomplishments = _parse_ai_response(response, "accomplishments", [], parse_as_list=True)
        
        # If AI returns empty but we have meaningful git context, use fallback
        if not accomplishments and journal_context:
            git_context = journal_context.get('git')
            if git_context:
                commit_message = git_context.get('metadata', {}).get('message', 'Unknown') if git_context.get('metadata') else 'Unknown'
                changed_files = git_context.get('changed_files', [])
                
                # Generate fallback accomplishments if we have meaningful context
                if commit_message and commit_message != 'Unknown' and commit_message.strip():
                    accomplishments.append(f"Completed: {commit_message.strip()}")
                
                if changed_files:
                    if len(changed_files) == 1:
                        accomplishments.append(f"Modified {changed_files[0]}")
                    else:
                        accomplishments.append(f"Successfully updated {len(changed_files)} files")
        
        result = AccomplishmentsSection(accomplishments=accomplishments)
        
        duration = time.time() - start_time
        _record_ai_generation_metrics("accomplishments", duration, True)
        
        return result
        
    except Exception as e:
        duration = time.time() - start_time
        _record_ai_generation_metrics("accomplishments", duration, False, "ai_generation_failed")
        
        # FALLBACK: Generate realistic stub content based on git context
        try:
            if journal_context is None:
                return AccomplishmentsSection(accomplishments=[])
            
            git_context = journal_context.get('git') if journal_context else None
            
            if git_context is None:
                return AccomplishmentsSection(accomplishments=[])
            
            commit_message = git_context.get('metadata', {}).get('message', 'Unknown') if git_context.get('metadata') else 'Unknown'
            changed_files = git_context.get('changed_files', [])
            
            # Only generate stub content if we have meaningful context
            if commit_message == 'Unknown' and not changed_files:
                return AccomplishmentsSection(accomplishments=[])
            
            # Create realistic stub accomplishments
            accomplishments = []
            if commit_message and commit_message != 'Unknown':
                accomplishments.append(f"Completed: {commit_message}")
            
            if changed_files:
                if len(changed_files) == 1:
                    accomplishments.append(f"Modified {changed_files[0]}")
                else:
                    accomplishments.append(f"Successfully updated {len(changed_files)} files")
            
            # Only add generic accomplishment if we have some context but no specific accomplishments
            if not accomplishments and (commit_message != 'Unknown' or changed_files):
                accomplishments.append("Made code changes to support the implementation")
            
            return AccomplishmentsSection(accomplishments=accomplishments)
            
        except Exception:
            # If even fallback fails, return empty
            return AccomplishmentsSection(accomplishments=[])

# Section Generator: Frustrations
# Purpose: Extracts and summarizes challenges, setbacks, and frustrations encountered in the commit, using only explicit evidence from chat, terminal, and git context.
# Assumptions: Only includes frustrations with clear evidence; does not infer or speculate. Returns an empty list if nothing is found.
# Limitations: Requires AI agent to fulfill the docstring prompt; placeholder implementation returns empty list.
@trace_mcp_operation("journal.generate_frustrations", attributes={"operation_type": "ai_generation", "section_type": "frustrations"})
def generate_frustrations_section(journal_context: JournalContext) -> FrustrationsSection:
    """
    You are helping build a high-quality development journal system that tracks coding work across commits. Your job is to generate a frustrations section. The quality of the entire journal depends on your output.

    Input: JournalContext containing git metadata, chat history, and previous journal entries.
    Output: Return the TypedDict structure defined in the function signature.

    AVAILABLE DATA in JournalContext:
    - git: Git context with commit metadata, diffs, changed files, and statistics
    - chat: Developer's conversations with AI coding assistants (may contain more than what's relevant to the current commit - think critically about what chat should be considered)
    - journal: Recent journal entries for context and continuity (don't duplicate content, but feel free to weave in quotes from reflections if relevant)

    AI Prompt for Frustrations Section Generation

    Purpose: Generate a list of challenges, blocks, or difficulties encountered during this commit, capturing the developer's authentic experience with obstacles and setbacks.

    Instructions: Extract evidence of challenges, frustrations, or difficulties from chat history and git context to create a list of meaningful obstacles encountered. Focus on what the developer explicitly expressed frustration with or what can be clearly identified as blocking or difficult.

    Priority for Content Sources:
    1. Explicit statements of frustration, difficulty, or challenges from chat - developer's own words
    2. Problem discovery evidence - where new problems were identified
    3. Blocking issues - where progress was hindered or stopped
    4. Git commit messages - if they indicate challenges or fixes for problems
    5. Terminal commands - repetitive or failed commands that suggest struggle

    Frustration Evidence Extraction:
    Look for explicit evidence of frustration or difficulty, such as:
    - Direct statements of frustration: "this is driving me crazy", "can't figure out", "struggling with"
    - Problem discovery: "found a new bug", "this is more complex than expected", "hitting a wall"
    - Blocking issues: "can't proceed until", "stuck on this", "need to resolve this first"
    - Technical difficulties: "tests are failing", "getting errors", "configuration issues"
    - Time/effort statements: "spent hours on this", "took longer than expected", "had to research"
    - Confusion indicators: "don't understand why", "unclear documentation", "confusing behavior"

    Quality Standards:
    Focus on frustrations that:
    - Have clear evidence in the context (not inferred or assumed)
    - Represent meaningful obstacles or learning opportunities
    - Include both technical challenges and process difficulties
    - Are specific rather than generic ("authentication middleware failing" vs "had problems")
    - Capture the developer's authentic voice and experience

    Language Translation Guidelines:
    When using developer's language from chat, translate respectfully:
    - Preserve authentic emotional expression including colorful language
    - Translate unkind language towards others into neutral descriptions, including:
      - Personal attacks or harsh judgments about individuals
      - Racism, sexism, or negative bias based on gender, sexual orientation, religion, etc.
      - Mean statements about people's abilities or worth
    - Preserve positive "who I am" language and translate only negative self-talk
    - Examples:
      - Keep: "this f***ing bug is driving me crazy", "I am awesome!", "I nailed that implementation"
      - Translate: "I'm such an idiot" → "encountered challenging complexity", "Bob writes terrible code" → "encountered challenging legacy implementation"

    ANTI-HALLUCINATION RULES:
    - Do NOT invent, infer, or assume frustrations not explicitly supported by the context
    - Only include frustrations with clear evidence in chat transcript, commit messages, git changes, or terminal commands
    - If no clear frustrations are found, return an empty list and omit the section
    - Do not reframe neutral work as frustrations unless there's explicit negative evidence
    - Never speculate about what the developer "probably" found frustrating

    Output Format:
    - List of strings, each representing a specific frustration or challenge
    - Return empty list if no clear frustrations are found and omit the section
    - Use present tense or past tense as appropriate ("Authentication middleware kept failing", "Spent hours debugging configuration issues")
    - Preserve authentic language while ensuring clarity

    CHECKLIST:
    - [ ] Searched chat history for explicit statements of frustration, difficulty, or challenges
    - [ ] Identified clear problem discovery and blocking issues
    - [ ] Focused on meaningful obstacles with specific evidence
    - [ ] Included both technical challenges and process difficulties
    - [ ] Applied language translation guidelines while preserving authentic frustration
    - [ ] Used only frustrations directly supported by context evidence
    - [ ] Returned empty list if no clear frustrations found
    - [ ] Did NOT invent, infer, or assume frustrations without explicit evidence
    - [ ] Captured developer's authentic voice and experience
    - [ ] Verified all frustrations are grounded in actual chat/git/terminal evidence
    """
    import time
    
    start_time = time.time()
    _add_ai_generation_telemetry("frustrations", journal_context, start_time)
    
    try:
        # Handle None context - return empty frustrations immediately
        if journal_context is None:
            return FrustrationsSection(frustrations=[])
        
        # Handle empty context - return empty frustrations if no meaningful content
        git_context = journal_context.get('git') if journal_context else None
        chat_context = journal_context.get('chat') if journal_context else None
        
        if git_context is None and chat_context is None:
            return FrustrationsSection(frustrations=[])
        
        # Extract prompt from function docstring
        prompt = inspect.getdoc(generate_frustrations_section)
        if not prompt:
            # Fallback to empty frustrations if no prompt
            return FrustrationsSection(frustrations=[])
        
        # Format context as JSON and append to prompt
        context_json = json.dumps(journal_context, indent=2, default=str)
        full_prompt = f"{prompt}\n\nThe journal_context object has the following structure:\n{context_json}"
        
        # Call AI with the formatted prompt
        response = invoke_ai(full_prompt, {})
        
        # Parse response (extract 'frustrations' field from JSON or parse as list)
        frustrations = _parse_ai_response(response, "frustrations", [], parse_as_list=True)
        
        result = FrustrationsSection(frustrations=frustrations)
        
        duration = time.time() - start_time
        _record_ai_generation_metrics("frustrations", duration, True)
        
        return result
        
    except Exception as e:
        duration = time.time() - start_time
        _record_ai_generation_metrics("frustrations", duration, False, "ai_generation_failed")
        
        # FALLBACK: Return empty frustrations (cannot infer frustrations from git context alone)
        return FrustrationsSection(frustrations=[])

# Section Generator: Tone/Mood
# Purpose: Generates the Tone/Mood section for a journal entry using AI.
# This function analyzes chat, commit, and terminal context to infer the developer's emotional state and supporting evidence. Only includes moods with explicit or strongly inferred evidence; does not speculate. Returns a placeholder until the AI agent executes the docstring prompt.
@trace_mcp_operation("journal.generate_tone_mood", attributes={"operation_type": "ai_generation", "section_type": "tone_mood"})
def generate_tone_mood_section(journal_context: JournalContext) -> ToneMoodSection:
    """
    You are helping build a high-quality development journal system that tracks coding work across commits. Your job is to generate a tone/mood section. The quality of the entire journal depends on your output.

    Input: JournalContext containing git metadata, chat history, and previous journal entries.
    Output: Return the TypedDict structure defined in the function signature.

    AVAILABLE DATA in JournalContext:
    - git: Git context with commit metadata, diffs, changed files, and statistics
    - chat: Developer's conversations with AI coding assistants (may contain more than what's relevant to the current commit - think critically about what chat should be considered)
    - journal: Recent journal entries for context and continuity (don't duplicate content, but feel free to weave in quotes from reflections if relevant)

    AI Prompt for Tone/Mood Section Generation

    Purpose: Generate a tone and mood assessment based on the developer's language and emotional indicators in their chat messages, capturing the authentic emotional context of the work session.

    Instructions: Extract emotional indicators and tone from chat history to assess the developer's mood and emotional state during this work session. Focus on explicit emotional language, patterns of expression, and authentic indicators of the developer's experience.

    Priority for Content Sources:
    1. Explicit emotional language from chat - developer's own emotional expressions
    2. Tone patterns in conversation - how the developer communicated
    3. Language intensity and word choice - emotional indicators
    4. Reaction patterns - responses to success, failure, or challenges
    5. Context clues - time of day, session length, complexity of work

    Emotional Indicator Extraction:
    Look for explicit emotional expression and tone indicators, such as:
    - Mood descriptors: "excited about", "frustrated with", "confident in", "anxious about"
    - Energy levels: "pumped up", "exhausted", "motivated", "burned out"
    - Satisfaction indicators: "thrilled with", "pleased about", "disappointed by", "satisfied with"
    - Stress/pressure indicators: "under pressure", "tight deadline", "stressed about", "relaxed"
    - Confidence levels: "confident this will work", "uncertain about", "sure that", "confused by"
    - Enthusiasm markers: exclamation points, caps, enthusiastic language, excitement

    Language Translation Guidelines:
    When analyzing developer's emotional language from chat, translate respectfully:
    - Preserve authentic emotional expression including colorful language
    - Translate unkind language towards others into neutral descriptions, including:
      - Personal attacks or harsh judgments about individuals  
      - Racism, sexism, or negative bias based on gender, sexual orientation, religion, etc.
      - Mean statements about people's abilities or worth
    - Preserve positive "who I am" language and translate only negative self-talk
    - Examples:
      - Keep: "this f***ing bug is driving me crazy", "I am awesome!", "I nailed that implementation"
      - Translate: "I'm such an idiot" → "encountered challenging complexity", "Bob writes terrible code" → "encountered challenging legacy implementation"

    ANTI-HALLUCINATION RULES:
    - Do NOT invent, infer, or assume emotional states not explicitly supported by the chat language
    - Only assess mood/tone based on clear evidence in the chat transcript
    - If no clear emotional indicators are found, return empty dict and omit the section
    - Do not psychoanalyze or read between the lines - use only explicit emotional language
    - Never speculate about mood based on technical content alone

    Output Format:
    - Dict with 'mood' and 'indicators' keys
    - **mood**: Choose a mood that describes this session
    - **indicators**: Specific evidence from chat that supports the mood assessment
    - Return empty dict if no clear emotional indicators are found and omit the section

    CHECKLIST:
    - [ ] Searched chat history for explicit emotional language and tone indicators
    - [ ] Identified clear mood patterns and emotional expression
    - [ ] Selected most accurate mood based on evidence
    - [ ] Provided specific indicators that support the mood assessment
    - [ ] Applied language translation guidelines while preserving authentic emotional expression
    - [ ] Used only emotional evidence directly supported by chat transcript
    - [ ] Returned empty dict if no clear emotional indicators found
    - [ ] Did NOT invent, infer, or psychoanalyze emotional states without explicit evidence
    - [ ] Captured authentic emotional context of the work session
    - [ ] Verified all mood assessment is grounded in actual chat evidence
    """
    import time
    
    start_time = time.time()
    _add_ai_generation_telemetry("tone_mood", journal_context, start_time)
    
    try:
        # Handle None context - return empty tone/mood immediately
        if journal_context is None:
            return ToneMoodSection(mood="", indicators="")
        
        # Handle empty context - return empty tone/mood if no meaningful content
        git_context = journal_context.get('git') if journal_context else None
        chat_context = journal_context.get('chat') if journal_context else None
        
        if git_context is None and chat_context is None:
            return ToneMoodSection(mood="", indicators="")
        
        # Extract prompt from function docstring
        prompt = inspect.getdoc(generate_tone_mood_section)
        if not prompt:
            # Fallback to empty tone/mood if no prompt
            return ToneMoodSection(mood="", indicators="")
        
        # Format context as JSON and append to prompt
        context_json = json.dumps(journal_context, indent=2, default=str)
        full_prompt = f"{prompt}\n\nThe journal_context object has the following structure:\n{context_json}"
        
        # Call AI with the formatted prompt
        response = invoke_ai(full_prompt, {})
        
        # Parse response (extract mood/indicators from JSON or regex patterns)
        parsed_mood = _parse_tone_mood_response(response)
        
        result = ToneMoodSection(mood=parsed_mood["mood"], indicators=parsed_mood["indicators"])
        
        duration = time.time() - start_time
        _record_ai_generation_metrics("tone_mood", duration, True)
        
        return result
        
    except Exception as e:
        duration = time.time() - start_time
        _record_ai_generation_metrics("tone_mood", duration, False, "ai_generation_failed")
        
        # FALLBACK: Generate realistic stub content based on git context
        try:
            if journal_context is None:
                return ToneMoodSection(mood="", indicators="")
            
            git_context = journal_context.get('git') if journal_context else None
            
            if git_context is None:
                return ToneMoodSection(mood="", indicators="")
            
            commit_message = git_context.get('metadata', {}).get('message', '') if git_context.get('metadata') else ''
            changed_files = git_context.get('changed_files', []) if git_context else []
            
            # Create realistic stub mood assessment
            # Only include tone/mood if we have meaningful context
            if commit_message or changed_files:
                mood = "Productive/Focused"
                indicators = "Steady progress evident from commit structure and file modifications"
                return ToneMoodSection(mood=mood, indicators=indicators)
            else:
                return ToneMoodSection(mood="", indicators="")
            
        except Exception:
            # If even fallback fails, return empty
            return ToneMoodSection(mood="", indicators="")

# Section Generator: Discussion Notes
# Purpose: Generates the Discussion Notes section for a journal entry using AI.
# This function extracts and curates relevant discussion points from chat context, focusing on technical reasoning, decisions, and key exchanges while filtering out routine conversation. Only includes discussion content with explicit evidence; does not paraphrase or invent content. Returns a placeholder until the AI agent executes the docstring prompt.
@trace_mcp_operation("journal.generate_discussion_notes", attributes={"operation_type": "ai_generation", "section_type": "discussion_notes"})
def generate_discussion_notes_section(journal_context: JournalContext) -> DiscussionNotesSection:
    """
    You are helping build a high-quality development journal system that tracks coding work across commits. Your job is to generate a discussion notes section. The quality of the entire journal depends on your output.

    IMPORTANT: If you return 0 quotes, you must explain why no relevant discussion was found.

    Input: JournalContext containing git metadata, chat history, and previous journal entries.
    Output: Return the TypedDict structure defined in the function signature.

    AVAILABLE DATA in JournalContext:
    - git: Git context with commit metadata, diffs, changed files, and statistics
    - chat: Developer's conversations with AI coding assistants (may contain more than what's relevant to the current commit - think critically about what chat should be considered)
    - journal: Recent journal entries for context and continuity (don't duplicate content, but feel free to weave in quotes from reflections if relevant)

    AI Prompt for Discussion Notes Section Generation

    Purpose: Extract and curate relevant discussion points from chat history that provide insight into the thinking process, technical reasoning, decisions, and key exchanges, filtering out routine conversation.
    
    VERBATIM QUOTES REQUIRED: Extract actual quotes from the conversation with speaker attribution. DO NOT paraphrase, summarize, or rewrite. The goal is to preserve the authentic voice and tone of the development process.

    You will likely be given more chat history than what is relevant to the current commit. Use the git data and journal context to determine what is relevant. The earliest messages are less likely to be relevant to the current commit.

    Content Filtering Guidelines:
    **Include discussion that:**
    - Makes the human seem wise
    - Reveals technical reasoning or decision-making process
    - Shows problem-solving approaches or debugging strategies
    - Contains meaningful questions that led to insights
    - Provides important context for understanding the work
    - Captures learning moments or discoveries
    - Expresses authentic emotional responses that add human context

    **Exclude routine content:**
    - Standard greetings, pleasantries, and social chat
    - Simple confirmation responses ("ok", "sounds good", "got it")
    - Routine status updates without insight ("working on X now")
    - Basic commands or requests without reasoning
    - Off-topic conversations unrelated to the technical work
    - Ignore purely administrative chat (like "please wait" or "let me check"), but include discussions about tooling, environment, or workflow

    Length and Excerpting Guidelines:
    - **Preserve full quotes** when they're concise and meaningful (under ~50 words)
    - **Use excerpts** for longer discussions: include key parts with `[...]` notation
    - **Group related exchanges** chronologically when they form a coherent discussion
    - **Separate distinct topics** with blank lines between different conversation threads

    Length Target Guidelines:
    - **Rich discussion context**: Include 5-10 meaningful exchanges or excerpts
    - **Moderate discussion**: Include 3-5 key exchanges
    - **Minimal meaningful discussion**: Include 1-3 most important points
    - **No meaningful discussion**: Return empty list and omit section

    Excerpt Guidelines for Long Content:
    - Use `[...]` notation where content is omitted for length
    - Focus on parts that match priority types and explicit reasoning keywords ("because", "since", "the tradeoff is")

    Speaker Attribution and Formatting:
    - ALWAYS present as VERBATIM QUOTES with speaker attribution:
      > **Human:** "exact quote here"
      > **AI:** "exact response here"
    - Present in chronological order, grouping messages by topic with blank lines between different topics

    - Preserve natural conversation flow and connections between related messages when length permits

    Language Translation Guidelines:
    When using developer's language from chat, translate respectfully:
    - Preserve authentic emotional expression including colorful language
    - Translate unkind language towards others into neutral descriptions, including:
     - Personal attacks or harsh judgments about individuals
     - Racism, sexism, or negative bias based on gender, sexual orientation, religion, etc.
     - Mean statements about people's abilities or worth
    - Preserve positive "who I am" language and translate only negative self-talk
    - Examples:
     - Keep: "this f***ing bug is driving me crazy", "I am awesome!", "I nailed that implementation"
     - Translate: "I'm such an idiot" → "encountered challenging complexity", "Bob writes terrible code" → "encountered challenging legacy implementation"

    ANTI-HALLUCINATION RULES:
    - Do NOT invent, infer, or summarize discussion points not explicitly present in the chat history
    - Only include discussion content directly supported by the chat transcript
    - MUST extract VERBATIM QUOTES - do not paraphrase or rewrite quotes
    - If no relevant discussion is found, return an empty list and omit the section
    - Never combine separate conversations or create composite quotes

    Output Format:
    Return a list of strings, each formatted as:
    > **Speaker:** "exact quote here"

    Example:
    > **Human:** "This is frustrating. The generators aren't working."
    > **Assistant:** "Let me help debug this issue."

    CHECKLIST:
    - [ ] Used git data and journal context to determine what chat content is relevant to current commit
    - [ ] Started analysis from oldest messages in chat history and worked chronologically forward
    - [ ] Searched chat history for content matching priority criteria (emotions, decisions, problem-solving, questions, technical discussion)
    - [ ] Applied content filtering guidelines - included wisdom, technical reasoning, problem-solving, insights, learning moments, authentic emotions
    - [ ] Excluded routine content - greetings, simple confirmations, basic status updates, off-topic conversations
    - [ ] Extracted actual conversation content as VERBATIM QUOTES without paraphrasing or invention
    - [ ] Used proper speaker attribution format: > **Speaker:** "exact quote here"
    - [ ] Applied appropriate length limits with excerpt notation ([...]) when needed
    - [ ] Included sufficient context to make quotes meaningful
    - [ ] Preserved conversational flow and speaker attribution when possible
    - [ ] Grouped by topic with appropriate granularity and blank lines between topics
    - [ ] Applied language translation guidelines while preserving authentic expression
    - [ ] Used only content directly supported by chat transcript
    - [ ] Returned empty list and omitted section if no relevant discussion was found
    - [ ] Did NOT invent, combine, or composite any discussion content
    - [ ] ENFORCED VERBATIM QUOTES REQUIREMENT - no summaries or paraphrases
    """
    import time
    
    start_time = time.time()
    _add_ai_generation_telemetry("discussion_notes", journal_context, start_time)
    
    try:
        # Handle None context - return empty discussion notes immediately
        if journal_context is None:
            return DiscussionNotesSection(discussion_notes=[])
        
        # Handle empty context - return empty discussion notes if no meaningful content
        git_context = journal_context.get('git') if journal_context else None
        chat_context = journal_context.get('chat') if journal_context else None
        
        if git_context is None and chat_context is None:
            return DiscussionNotesSection(discussion_notes=[])
        
        # Extract prompt from function docstring
        prompt = inspect.getdoc(generate_discussion_notes_section)
        if not prompt:
            # Fallback to empty discussion notes if no prompt
            return DiscussionNotesSection(discussion_notes=[])
        
        # Format context as JSON and append to prompt
        context_json = json.dumps(journal_context, indent=2, default=str)
        full_prompt = f"{prompt}\n\nThe journal_context object has the following structure:\n{context_json}"
        
        # Call AI with the formatted prompt
        response = invoke_ai(full_prompt, {})
        
        # Parse response (extract 'discussion_notes' field from JSON or parse as list)
        discussion_notes = _parse_ai_response(response, "discussion_notes", [], parse_as_list=True)
        
        # If no discussion notes found, check if AI provided an explanation
        if not discussion_notes and response and response.strip():
            # Don't treat '[]' as an explanation - it means no discussion notes
            if response.strip() != '[]':
                # Include the AI's explanation as a single discussion note
                discussion_notes = [response.strip()]
        
        result = DiscussionNotesSection(discussion_notes=discussion_notes)
        
        duration = time.time() - start_time
        _record_ai_generation_metrics("discussion_notes", duration, True)
        
        return result
        
    except Exception as e:
        duration = time.time() - start_time
        _record_ai_generation_metrics("discussion_notes", duration, False, "ai_generation_failed")
        
        # FALLBACK: Return empty discussion notes (cannot infer discussion from git context alone)
        return DiscussionNotesSection(discussion_notes=[])


# Section Generator: Discussion Notes (Simple Version)
# Purpose: Simplified discussion notes generation focusing on results over complex filtering.
@trace_mcp_operation("journal.generate_discussion_notes_simple", attributes={"operation_type": "ai_generation", "section_type": "discussion_notes_simple"})
def generate_discussion_notes_section_simple(journal_context: JournalContext) -> DiscussionNotesSection:
    """
    Find the 10 most interesting quotes from the conversation that are relevant to this commit.
    
    Look for quotes that:
    - Show technical reasoning or decision-making
    - Make the human seem wise or insightful
    - Relate to the files that were changed in this commit
    - Demonstrate problem-solving or learning moments
    
    Return quotes as verbatim text with speaker attribution in this format:
    > **Speaker:** "exact quote here"
    
    If you return 0 quotes, explain why no relevant discussion was found.
    """
    import time
    
    start_time = time.time()
    _add_ai_generation_telemetry("discussion_notes_simple", journal_context, start_time)
    
    try:
        # Handle None context - return empty discussion notes immediately
        if journal_context is None:
            return DiscussionNotesSection(discussion_notes=[])
        
        # Handle empty context - return empty discussion notes if no meaningful content
        git_context = journal_context.get('git') if journal_context else None
        chat_context = journal_context.get('chat') if journal_context else None
        
        if git_context is None and chat_context is None:
            return DiscussionNotesSection(discussion_notes=[])
        
        # Extract prompt from function docstring and format with git context
        base_prompt = inspect.getdoc(generate_discussion_notes_section_simple)
        if not base_prompt:
            # Fallback to empty discussion notes if no prompt
            return DiscussionNotesSection(discussion_notes=[])
        
        # Add git context information to the prompt
        git_ctx = git_context or {}
        commit_message = git_ctx.get('metadata', {}).get('message', 'N/A')
        changed_files = git_ctx.get('changed_files', [])
        diff_summary = git_ctx.get('diff_summary', 'N/A')
        
        # Append git context to the base prompt
        formatted_prompt = f"""{base_prompt}
        
Commit information:
- Message: {commit_message}
- Changed files: {changed_files}
- Diff summary: {diff_summary}"""
        
        # Format context as JSON and append to prompt
        context_json = json.dumps(journal_context, indent=2, default=str)
        full_prompt = f"{formatted_prompt}\n\nThe journal_context object has the following structure:\n{context_json}"
        
        # Call AI to generate discussion notes
        ai_response = invoke_ai(full_prompt, {})
        
        # Parse AI response to extract discussion notes
        parsed_response = _parse_ai_response(ai_response, 'discussion_notes', fallback_value=[], parse_as_list=True)
        
        duration = time.time() - start_time
        _record_ai_generation_metrics("discussion_notes_simple", duration, True)
        
        return DiscussionNotesSection(discussion_notes=parsed_response)
        
    except Exception as e:
        duration = time.time() - start_time
        _record_ai_generation_metrics("discussion_notes_simple", duration, False, "ai_generation_failed")
        
        # FALLBACK: Return empty discussion notes (cannot infer discussion from git context alone)
        return DiscussionNotesSection(discussion_notes=[])


# Section Generator: Terminal Commands
# Purpose: Generates the Terminal Commands section for a journal entry using AI.
# This function extracts and lists all relevant terminal commands executed during the commit, focusing on commands that demonstrate problem-solving, technical approach, or challenges. Only includes commands with explicit evidence in terminal context; does not paraphrase or invent content. Returns a placeholder until the AI agent executes the docstring prompt.
# Architecture Decision: Terminal Command Collection Removed (2025-06-27)
# The generate_terminal_commands_section function has been removed as part of terminal
# infrastructure cleanup. Terminal commands were originally designed to be collected
# by Cursor's AI with access to its execution context. With the shift to external
# journal generation, we no longer have access. Git diffs and chat context provide
# sufficient narrative.
@trace_mcp_operation("journal.generate_commit_metadata", attributes={"operation_type": "ai_generation", "section_type": "commit_metadata"})
def generate_commit_metadata_section(journal_context: JournalContext) -> CommitMetadataSection:
    """
    AI Prompt for Commit Metadata Section Generation

    Purpose: Extract and format relevant commit metadata from git context, providing key statistics and classifications that support the journal entry narrative.

    Instructions: Extract commit metadata from git context to create a clean, scannable summary of commit statistics and characteristics. Focus on metadata that provides useful context for understanding the scope and nature of the changes.

    Priority for Content Sources:
    1. Git context from journal_context.git - the authoritative source for all commit metadata
    2. No other sources - work exclusively with the provided git context

    Metadata Extraction:
    Extract and format the following metadata fields when available and meaningful:
    - **files_changed**: Total number of files modified in the commit
    - **insertions**: Total lines added
    - **deletions**: Total lines removed  
    - **size_classification**: Pre-classified commit size (small/medium/large)
    - **is_merge**: Whether this was a merge commit
    - **source_files**: Count of source code files changed
    - **config_files**: Count of configuration files changed
    - **docs_files**: Count of documentation files changed
    - **tests_files**: Count of test files changed

    Filtering Guidelines:
    Include only metadata that adds narrative value:
    - Include basic statistics (files_changed, insertions, deletions) when non-zero
    - Include size_classification and merge status when meaningful
    - Include file type counts only when non-zero and show meaningful distribution
    - Exclude metadata fields that are empty, zero, or not meaningful for this commit

    ANTI-HALLUCINATION RULES:
    - Do NOT invent, infer, or calculate metadata not explicitly present in the git context
    - Only include metadata fields directly supported by the git context data
    - If no git context is available, return an empty dict and omit the section
    - Never speculate about commit characteristics not present in the data

    Output Format:
    - Dict[str, str] with specified field names as keys and formatted values as strings
    - Return empty dict if no relevant metadata is found and omit the section
    - All values should be formatted as strings suitable for display

    CHECKLIST:
    - [ ] Extracted metadata exclusively from journal_context.git
    - [ ] Used specified field names for consistency
    - [ ] Included only meaningful/non-zero metadata fields
    - [ ] Formatted all values as display-ready strings
    - [ ] Did NOT invent, calculate, or speculate about metadata not present in git context
    - [ ] Returned empty dict if no relevant metadata found
    - [ ] Verified all metadata is grounded in actual git context data
    """
    import time
    
    start_time = time.time()
    _add_ai_generation_telemetry("commit_metadata", journal_context, start_time)
    
    try:
        # Generate realistic stub content based on git context
        # Handle None journal_context
        if journal_context is None:
            return CommitMetadataSection(commit_metadata={})
        
        git_context = journal_context.get('git') if journal_context else None
        
        # Handle None git_context
        if git_context is None:
            return CommitMetadataSection(commit_metadata={})
        
        changed_files = git_context.get('changed_files', []) if git_context else []
        file_stats = git_context.get('file_stats', {}) if git_context else {}
        metadata = {}
        
        # Calculate basic statistics from available data
        if changed_files:
            metadata['files_changed'] = str(len(changed_files))
        
        # Calculate insertions and deletions if file_stats available
        total_insertions = 0
        total_deletions = 0
        for file_name, stats in file_stats.items():
            if isinstance(stats, dict):
                total_insertions += stats.get('additions', 0)
                total_deletions += stats.get('deletions', 0)
        
        if total_insertions > 0:
            metadata['insertions'] = str(total_insertions)
        if total_deletions > 0:
            metadata['deletions'] = str(total_deletions)
        
        # Simple size classification based on file count
        if changed_files:
            if len(changed_files) <= 3:
                metadata['size_classification'] = 'small'
            elif len(changed_files) <= 10:
                metadata['size_classification'] = 'medium'
            else:
                metadata['size_classification'] = 'large'
        
        result = CommitMetadataSection(commit_metadata=metadata)
        
        duration = time.time() - start_time
        _record_ai_generation_metrics("commit_metadata", duration, True)
        
        return result
        
    except Exception as e:
        duration = time.time() - start_time
        _record_ai_generation_metrics("commit_metadata", duration, False, "ai_generation_failed")
        raise

@trace_mcp_operation("journal.ensure_directory", attributes={"operation_type": "directory_creation"})
def ensure_journal_directory(file_path):
    """
    Ensure the parent directory for the given file_path exists.
    Creates all missing parent directories as needed (on-demand pattern).
    This utility should be used by all file operations that need to ensure parent directories exist, replacing any upfront directory creation pattern.
    Raises PermissionError if directory creation fails due to permissions.
    Does nothing if the directory already exists.
    Args:
        file_path (Path or str): The file path whose parent directory should be ensured.
    Example:
        file_path = Path("journal/daily/2025-05-28-journal.md")
        ensure_journal_directory(file_path)
    """
    import time
    from opentelemetry import trace
    
    start_time = time.time()
    parent_dir = Path(file_path).parent
    
    # Add semantic conventions for telemetry
    current_span = trace.get_current_span()
    if current_span:
        current_span.set_attribute("directory.path", str(parent_dir.name))  # Only directory name for privacy
        current_span.set_attribute("directory.exists", parent_dir.exists())
    
    try:
        parent_dir.mkdir(parents=True, exist_ok=True)
        
        # Record successful directory operation metrics
        duration = time.time() - start_time
        metrics = get_mcp_metrics()
        if metrics:
            metrics.record_operation_duration(
                "journal.directory_operation_duration_seconds",
                duration,
                operation_type="ensure_directory"
            )
            metrics.record_tool_call(
                "journal.directory_operations_total",
                success=True,
                operation_type="ensure_directory"
            )
        
    except PermissionError:
        # Record error metrics
        metrics = get_mcp_metrics()
        if metrics:
            metrics.record_tool_call(
                "journal.directory_operations_total",
                success=False,
                operation_type="ensure_directory"
            )
        
        if current_span:
            current_span.set_attribute("error.category", "permission_denied_directory")
        raise  # Re-raise PermissionError as documented
    except Exception as e:
        # Record error metrics
        metrics = get_mcp_metrics()
        if metrics:
            metrics.record_tool_call(
                "journal.directory_operations_total",
                success=False,
                operation_type="ensure_directory"
            )
        
        if current_span:
            current_span.set_attribute("error.category", "directory_creation_failed")
        raise OSError(f"Failed to create directory {parent_dir}: {e}")

@trace_mcp_operation("journal.load_context", attributes={"operation_type": "config_read", "file_type": "toml"})
def load_journal_context(config_path: str) -> dict:
    """
    Load configuration from a TOML file to establish journal context.
    
    Args:
        config_path: Path to the TOML configuration file
        
    Returns:
        dict: Parsed configuration data
        
    Raises:
        JournalError: If configuration cannot be loaded
    """
    import time
    from opentelemetry import trace
    
    start_time = time.time()
    
    # Add semantic conventions for telemetry
    current_span = trace.get_current_span()
    if current_span:
        current_span.set_attribute("config.file_path", os.path.basename(config_path))
        current_span.set_attribute("operation_type", "config_load")
    
    try:
        import tomllib
        import os
        
        if not os.path.exists(config_path):
            if current_span:
                current_span.set_attribute("error.category", "file_not_found")
            raise JournalError(f"Configuration file not found: {config_path}")
        
        file_size = os.path.getsize(config_path)
        if current_span:
            current_span.set_attribute("config.file_size_bytes", file_size)
        
        with open(config_path, 'rb') as f:
            config_data = tomllib.load(f)
        
        # Record successful config load metrics
        duration = time.time() - start_time
        metrics = get_mcp_metrics()
        if metrics:
            metrics.record_operation_duration(
                "journal.config_load_duration_seconds",
                duration,
                operation_type="config_load",
                file_type="toml"
            )
            metrics.record_tool_call(
                "journal.config_load_operations_total",
                success=True,
                operation_type="config_load"
            )
        
        if current_span:
            current_span.set_attribute("config.sections_loaded", len(config_data))
        
        return config_data
        
    except Exception as e:
        # Record error metrics
        duration = time.time() - start_time
        metrics = get_mcp_metrics()
        if metrics:
            metrics.record_tool_call(
                "journal.config_load_operations_total",
                success=False,
                operation_type="config_load"
            )
        
        if current_span:
            current_span.set_attribute("error.category", "config_load_failed")
        
        raise JournalError(f"Failed to load configuration: {e}")

