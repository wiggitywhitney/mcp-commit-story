# Task ID: 4
# Title: Implement Telemetry System
# Status: in-progress
# Dependencies: None
# Priority: high
# Description: Set up OpenTelemetry integration for tracing, metrics, and logging to provide observability for the MCP server.
# Details:
Implement telemetry system in `src/mcp_journal/telemetry.py` with the following features:

1. OpenTelemetry setup:
```python
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.resources import SERVICE_NAME, Resource

def setup_telemetry(config):
    """Initialize OpenTelemetry based on configuration"""
    if not config.get("telemetry.enabled", True):
        return
        
    service_name = config.get("telemetry.service_name", "mcp-journal")
    resource = Resource(attributes={SERVICE_NAME: service_name})
    
    tracer_provider = TracerProvider(resource=resource)
    trace.set_tracer_provider(tracer_provider)
    
    # Configure exporters based on config
    # ...
```

2. Tracing utilities:
```python
def get_tracer(name="mcp_journal"):
    """Get a tracer for the specified name"""
    return trace.get_tracer(name)

def trace_operation(name):
    """Decorator for tracing operations"""
    def decorator(func):
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            tracer = get_tracer()
            with tracer.start_as_current_span(name):
                return func(*args, **kwargs)
        return wrapper
    return decorator
```

3. Metrics collection:
```python
# Setup metrics collection for key operations
# Track operation duration, success/failure, etc.
```

4. Logging integration:
```python
import logging

def setup_logging(debug=False):
    """Configure logging with appropriate levels"""
    level = logging.DEBUG if debug else logging.INFO
    logging.basicConfig(level=level)
    # Additional logging configuration
```

5. Ensure telemetry system is compatible with the new MCP/AI agent architecture.

6. Focus on telemetry for the setup-only CLI scope, ensuring proper instrumentation of configuration and initialization processes.

# Test Strategy:
1. Unit tests for telemetry initialization
2. Tests for tracing decorator functionality
3. Tests for metrics collection
4. Tests for logging configuration
5. Mock telemetry exporters for testing
6. Verify telemetry can be disabled via configuration
7. Test telemetry integration with the setup-only CLI functionality

# Subtasks:
## 1. OpenTelemetry Foundation Setup [done]
### Dependencies: None
### Description: Create basic OpenTelemetry initialization and configuration system
### Details:
TDD Steps:

1. WRITE TESTS FIRST:
   - Test setup_telemetry(config_dict) with enabled/disabled settings
   - Test get_tracer(name) returns correct tracer instance
   - Test get_meter(name) returns correct meter instance
   - Test resource configuration with service name and attributes
   - Test TracerProvider and MeterProvider initialization
   - Test telemetry disabling via configuration
   - RUN TESTS - VERIFY THEY FAIL

2. GET APPROVAL FOR DESIGN CHOICES:
   - Module structure: src/mcp_journal/telemetry.py with sub-modules?
   - Configuration schema for telemetry settings
   - Default state (enabled/disabled)
   - Resource attributes beyond service name
   - PAUSE FOR MANUAL APPROVAL

3. IMPLEMENT FUNCTIONALITY:
   - Add OpenTelemetry dependencies to requirements.txt
   - Create telemetry.py module with initialization functions
   - Implement resource configuration with service name and version
   - Set up TracerProvider with appropriate processors
   - Set up MeterProvider with appropriate readers
   - Implement get_tracer and get_meter utility functions
   - Add configuration-based disabling
   - RUN TESTS - VERIFY THEY PASS

4. DOCUMENT AND COMPLETE:
   - Add docstrings to all public functions
   - Add module-level documentation explaining usage
   - Add comments explaining configuration options

## 2. MCP Operation Instrumentation Decorators [done]
### Dependencies: None
### Description: Create tracing decorators specifically for MCP operations
### Details:
TDD Steps:

1. WRITE TESTS FIRST:
   - Test @trace_mcp_operation(name) decorator on synchronous function
   - Test decorator on function that raises exception
   - Test decorator on async function
   - Test span attributes are correctly set
   - Test span context propagation to child operations
   - Test error recording in spans
   - Test custom attribute addition via decorator
   - RUN TESTS - VERIFY THEY FAIL

2. GET APPROVAL FOR DESIGN CHOICES:
   - Semantic attribute naming convention for MCP operations
   - Error handling strategy (record exception vs fail silently)
   - Decorator API design (parameters, defaults)
   - Async support approach
   - PAUSE FOR MANUAL APPROVAL

3. IMPLEMENT FUNCTIONALITY:
   - Create MCPTracer class with trace_mcp_operation decorator
   - Implement semantic attributes for MCP operations
   - Add support for async function decoration
   - Implement span status and exception recording
   - Add context propagation utilities
   - Create helper for adding custom attributes
   - RUN TESTS - VERIFY THEY PASS

4. DOCUMENT AND COMPLETE:
   - Add docstrings with examples for all decorators
   - Document semantic conventions for MCP spans
   - Add usage examples in module documentation

## 3. Auto-Instrumentation Integration [done]
### Dependencies: None
### Description: Configure OpenTelemetry auto-instrumentation for common libraries
### Details:
TDD Steps:

1. WRITE TESTS FIRST:
   - Test enable_auto_instrumentation(config) with all instrumentors
   - Test selective enabling of instrumentors
   - Test disabled instrumentation
   - Test HTTP request tracing with requests/aiohttp
   - Test asyncio operation tracing
   - Test SQLAlchemy instrumentation if applicable
   - RUN TESTS - VERIFY THEY FAIL

2. GET APPROVAL FOR DESIGN CHOICES:
   - Which auto-instrumentors to include by default
   - Configuration format for enabling/disabling instrumentors
   - Performance vs observability trade-offs
   - Integration with existing MCP components
   - PAUSE FOR MANUAL APPROVAL

3. IMPLEMENT FUNCTIONALITY:
   - Add instrumentor dependencies to requirements.txt
   - Implement enable_auto_instrumentation function
   - Create configuration schema for instrumentors
   - Add selective instrumentor enabling
   - Integrate with configuration system
   - Implement graceful fallback for missing instrumentors
   - RUN TESTS - VERIFY THEY PASS

4. DOCUMENT AND COMPLETE:
   - Document each supported instrumentor
   - Add configuration examples for common scenarios
   - Document performance implications

## 4. MCP-Specific Metrics Collection [done]
### Dependencies: None
### Description: Define and implement metrics collection for MCP operations
### Details:
TDD Steps:

1. WRITE TESTS FIRST:
   - Test MCPMetrics class initialization
   - Test record_tool_call(tool_name, success) method
   - Test record_operation_duration(operation, duration) method
   - Test metric labels and values are correctly set
   - Test metric export format
   - Test counter increments
   - Test histogram recordings
   - Test gauge updates
   - RUN TESTS - VERIFY THEY FAIL

2. GET APPROVAL FOR DESIGN CHOICES:
   - Metric naming convention
   - Business metrics to track
   - Technical metrics to track
   - Metric aggregation strategy
   - Histogram bucket configuration
   - PAUSE FOR MANUAL APPROVAL

3. IMPLEMENT FUNCTIONALITY:
   - Create MCPMetrics class
   - Implement counters for operations and events
   - Implement histograms for durations
   - Implement gauges for state tracking
   - Add semantic metric attributes
   - Create metrics recording utilities
   - Implement metric view configuration
   - RUN TESTS - VERIFY THEY PASS

4. DOCUMENT AND COMPLETE:
   - Document each metric with purpose and interpretation
   - Add examples of querying metrics in common systems
   - Document metric data model and attributes

## 5. Multi-Exporter Configuration System [done]
### Dependencies: None
### Description: Support multiple telemetry exporters (console, OTLP, Prometheus) for vendor-neutral observability
### Details:
TDD Steps:

1. WRITE TESTS FIRST:
   - Test configure_exporters(config) with console exporter
   - Test OTLP exporter configuration
   - Test Prometheus exporter configuration
   - Test multiple exporters simultaneously
   - Test invalid configuration handling
   - Test exporter initialization and failure handling
   - Test environment variable overrides
   - RUN TESTS - VERIFY THEY FAIL

2. GET APPROVAL FOR DESIGN CHOICES:
   - Configuration schema structure for exporters
   - Fallback strategy when exporters fail
   - Environment variable naming and precedence
   - Prometheus metrics port and endpoint configuration
   - OTLP endpoint configuration
   - PAUSE FOR MANUAL APPROVAL

3. IMPLEMENT FUNCTIONALITY:
   - Add exporter dependencies to requirements.txt
   - Implement console exporter configuration
   - Implement OTLP exporter with gRPC and HTTP options
   - Implement Prometheus exporter with MetricReader
   - Create configuration validation
   - Add graceful fallback handling
   - Implement environment variable overrides
   - Support vendor-neutral deployment options
   - RUN TESTS - VERIFY THEY PASS

4. DOCUMENT AND COMPLETE:
   - Document each exporter configuration option
   - Add examples for common observability backends
   - Document environment variables for configuration

## 6. Structured Logging with Trace Correlation [done]
### Dependencies: None
### Description: Integrate logging with OpenTelemetry trace context
### Details:
✅ TASK 4.6 COMPLETED SUCCESSFULLY!

## Implementation Summary

Successfully implemented comprehensive structured logging with OpenTelemetry trace correlation:

### ✅ Core Features Implemented:
- **OTelFormatter**: JSON formatter with automatic trace/span ID injection
- **LogMetricsHandler**: Optional log-based metrics collection
- **Sensitive Data Protection**: Automatic redaction of passwords, tokens, API keys
- **Performance Optimization**: LazyLogData wrapper and level-aware logging helpers
- **Integration Functions**: setup_structured_logging(), get_correlated_logger()

### ✅ Enhanced Features (User Requested):
- **Sensitive Data Filtering**: Recursive sanitization with configurable patterns
- **Performance Optimization**: Lazy evaluation for expensive computations
- **Clean Integration**: Seamless integration with existing telemetry system

### ✅ Test Coverage: 23/23 PASSING
- OTelFormatter functionality (8 tests)
- Log-based metrics (3 tests) 
- Utility functions (3 tests)
- Sensitive data filtering (3 tests)
- Performance optimization (3 tests)
- Integration patterns (3 tests)

### ✅ Documentation Updated:
1. **docs/telemetry.md**: Comprehensive structured logging documentation
2. **PRD**: Updated observability section with structured logging features
3. **Engineering Spec**: Added detailed structured logging implementation section

### ✅ Integration Complete:
- Integrated into main telemetry.py module
- Automatic initialization during setup_telemetry()
- Works with or without active telemetry (graceful degradation)
- Follows multi-exporter configuration patterns

### ✅ Security & Performance:
- Automatic redaction of sensitive fields (passwords, tokens, keys)
- Lazy evaluation prevents expensive computations when logging disabled
- JSON format enables rich querying in centralized logging systems
- Trace correlation enables drilling down from metrics to specific requests

**Status: COMPLETE** ✅

TDD Steps:

1. ✅ WRITE TESTS FIRST:
   - Test OTelFormatter class initialization
   - Test trace ID injection in log records
   - Test span ID injection in log records
   - Test structured log format (JSON)
   - Test log correlation with active spans
   - Test log-based metrics (optional)
   - Test different log levels
   - RUN TESTS - VERIFIED THEY FAILED

2. ✅ GET APPROVAL FOR DESIGN CHOICES:
   - Log format (JSON vs structured text) - APPROVED: JSON
   - Which log levels to correlate with traces - APPROVED: All levels
   - Log-based metrics implementation - APPROVED: Optional LogMetricsHandler
   - Log enrichment strategy - APPROVED: Automatic trace context injection
   - ADDITIONAL USER ENHANCEMENTS APPROVED

3. ✅ IMPLEMENT FUNCTIONALITY:
   - Create OTelFormatter class
   - Implement trace correlation
   - Add structured logging configuration
   - Implement log record enrichment
   - Add optional log-based metrics
   - Create logging utility functions
   - Integrate with existing logging
   - RUN TESTS - VERIFIED THEY PASS (23/23)

4. ✅ DOCUMENT AND COMPLETE:
   - Document logging configuration options
   - Add examples of querying correlated logs
   - Document log format specification
   - Updated PRD and Engineering Spec

## 7. MCP Server Integration and End-to-End Testing [pending]
### Dependencies: None
### Description: Integrate telemetry with MCP server and validate complete pipeline [Updated: 5/31/2025]
### Details:
TDD Steps:

1. WRITE TESTS FIRST:
   - Test full MCP server startup with telemetry
   - Test tool call tracing end-to-end
   - Test configuration validation
   - Test telemetry disable/enable scenarios
   - Test span propagation across components
   - Test metrics collection during operations
   - Test graceful degradation when telemetry fails
   - RUN TESTS - VERIFY THEY FAIL

2. GET APPROVAL FOR DESIGN CHOICES:
   - Integration points in MCP server lifecycle
   - Configuration schema final structure
   - Performance impact acceptance criteria
   - Telemetry data volume estimates
   - PAUSE FOR MANUAL APPROVAL

3. IMPLEMENT FUNCTIONALITY:
   - Integrate telemetry setup into MCP server initialization
   - Update configuration schema with telemetry section
   - Apply tracing decorators to existing MCP operations
   - Add metrics collection to key operations
   - Implement graceful degradation when disabled
   - Add health checks for telemetry system
   - Create telemetry shutdown hooks
   - RUN TESTS - VERIFY THEY PASS

4. DOCUMENT AND COMPLETE:
   - Add documentation IF NEEDED in three places:
     1. **Docs directory**: Update telemetry.md with integration guide and configuration examples
     2. **PRD**: Update observability section with end-to-end telemetry capabilities
     3. **Engineering Spec**: Update with MCP server integration details and architecture
   - **Do not remove existing information unless it's incorrect**
   - **No approval needed** - make documentation edits directly
   - **Run the entire test suite and make sure all tests are passing**
   - **Make sure pyproject.toml is updated as needed**
   - Double check all subtask requirements are met before marking this subtask as complete
   - **MARK COMPLETE**

## 8. Instrument Journal Management Operations (Task 3) [pending]
### Dependencies: None
### Description: Add telemetry to existing journal creation and file operations for AI context flow observability
### Details:
TDD Steps:

1. WRITE TESTS FIRST:
   - Test journal creation operations are traced
   - Test file operation metrics (create, read, write times)
   - Test journal entry count metrics
   - Test error scenarios in journal operations
   - Test AI context flow tracing (prompt → journal entry)
   - Test sensitive data handling in spans
   - Test journal operation performance impact
   - RUN TESTS - VERIFY THEY FAIL

2. GET APPROVAL FOR DESIGN CHOICES:
   - Which journal operations to instrument
   - Metrics vs traces for file operations
   - Sensitive data handling in spans
   - Performance overhead acceptance criteria
   - PAUSE FOR MANUAL APPROVAL

3. IMPLEMENT FUNCTIONALITY:
   - Add tracing decorators to journal.py functions
   - Instrument file operations with duration metrics
   - Add journal entry creation counters
   - Implement AI context flow tracing
   - Add error tracking for journal operations
   - Create journal-specific semantic conventions
   - Implement sensitive data filtering
   - RUN TESTS - VERIFY THEY PASS

4. DOCUMENT AND COMPLETE:
   - Add documentation IF NEEDED in three places:
     1. **Docs directory**: Update telemetry.md with journal operation instrumentation examples
     2. **PRD**: Update if adding user-facing journal monitoring features
     3. **Engineering Spec**: Update with journal telemetry implementation details
   - **Do not remove existing information unless it's incorrect**
   - **No approval needed** - make documentation edits directly
   - **Run the entire test suite and make sure all tests are passing**
   - **Make sure pyproject.toml is updated as needed**
   - Double check all subtask requirements are met before marking this subtask as complete
   - **MARK COMPLETE**

## 9. Instrument Context Collection Operations (Task 5) [pending]
### Dependencies: None
### Description: Add telemetry to existing Git operations and file scanning for MCP context flow visibility
### Details:
TDD Steps:

1. WRITE TESTS FIRST:
   - Test Git operation tracing (git log, diff, status timing)
   - Test file scanning metrics (files processed, scan duration)
   - Test context collection success/failure rates
   - Test memory usage during large repository scans
   - Test context flow from Git → structured data
   - Test performance impact on large repositories
   - Test error handling in Git operations
   - RUN TESTS - VERIFY THEY FAIL

2. GET APPROVAL FOR DESIGN CHOICES:
   - Git operation granularity for tracing
   - File content handling in traces
   - Performance impact mitigation for large repos
   - Memory usage tracking approach
   - PAUSE FOR MANUAL APPROVAL

3. IMPLEMENT FUNCTIONALITY:
   - Add tracing decorators to context_collection.py functions
   - Instrument Git operations with command-level tracing
   - Add file scanning performance metrics
   - Implement context flow tracing
   - Add memory usage tracking
   - Create error tracking for Git operations
   - Implement performance optimizations
   - RUN TESTS - VERIFY THEY PASS

4. DOCUMENT AND COMPLETE:
   - Add documentation IF NEEDED in three places:
     1. **Docs directory**: Update telemetry.md with Git operation instrumentation and context collection monitoring
     2. **PRD**: Update if adding user-facing context collection monitoring features
     3. **Engineering Spec**: Update with context collection telemetry implementation details
   - **Do not remove existing information unless it's incorrect**
   - **No approval needed** - make documentation edits directly
   - **Run the entire test suite and make sure all tests are passing**
   - **Make sure pyproject.toml is updated as needed**
   - Double check all subtask requirements are met before marking this subtask as complete
   - **MARK COMPLETE**

## 10. Instrument Configuration Management (Task 6) [pending]
### Dependencies: None
### Description: Add telemetry to existing config loading and validation for system initialization observability
### Details:
TDD Steps:

1. WRITE TESTS FIRST:
   - Test configuration loading time tracking
   - Test validation success/failure metrics
   - Test configuration change detection
   - Test environment variable resolution tracing
   - Test sensitive value masking
   - Test configuration reload events
   - Test configuration → MCP server startup flow
   - RUN TESTS - VERIFY THEY FAIL

2. GET APPROVAL FOR DESIGN CHOICES:
   - Configuration value privacy (mask sensitive values)
   - Validation error detail level in spans
   - Configuration reload event tracking
   - Configuration metrics granularity
   - PAUSE FOR MANUAL APPROVAL

3. IMPLEMENT FUNCTIONALITY:
   - Add tracing decorators to config.py functions
   - Instrument config loading with duration metrics
   - Add validation error tracking with context
   - Implement sensitive value masking
   - Create configuration change detection
   - Add configuration reload event tracking
   - Trace configuration → MCP server startup flow
   - RUN TESTS - VERIFY THEY PASS

4. DOCUMENT AND COMPLETE:
   - Add documentation IF NEEDED in three places:
     1. **Docs directory**: Update telemetry.md with configuration loading instrumentation examples
     2. **PRD**: Update if adding user-facing configuration monitoring features
     3. **Engineering Spec**: Update with configuration telemetry implementation details
   - **Do not remove existing information unless it's incorrect**
   - **No approval needed** - make documentation edits directly
   - **Run the entire test suite and make sure all tests are passing**
   - **Make sure pyproject.toml is updated as needed**
   - Double check all subtask requirements are met before marking this subtask as complete
   - **MARK COMPLETE**

## 11. Instrument Integration Tests for Telemetry Validation (Task 8) [pending]
### Dependencies: None
### Description: Add telemetry awareness to existing integration tests for end-to-end observability validation
### Details:
TDD Steps:

1. WRITE TESTS FIRST:
   - Test integration tests generate expected spans
   - Test trace continuity across MCP tool chains
   - Test metrics collection during integration scenarios
   - Test telemetry doesn't break existing integration tests
   - Test span attribute correctness
   - Test metric value correctness
   - Test telemetry in error scenarios
   - RUN TESTS - VERIFY THEY FAIL

2. GET APPROVAL FOR DESIGN CHOICES:
   - Integration test telemetry scope
   - Test environment telemetry configuration
   - Telemetry assertion patterns in tests
   - Mock vs real telemetry backends for testing
   - PAUSE FOR MANUAL APPROVAL

3. IMPLEMENT FUNCTIONALITY:
   - Update existing integration tests to validate telemetry
   - Add telemetry configuration for test environments
   - Create telemetry assertion helpers
   - Implement span collection and verification
   - Add metric collection and verification
   - Ensure AI → MCP → tool chain observability
   - Create test-specific telemetry exporters
   - RUN TESTS - VERIFY THEY PASS

4. DOCUMENT AND COMPLETE:
   - Add documentation IF NEEDED in three places:
     1. **Docs directory**: Update telemetry.md with integration test telemetry validation examples
     2. **PRD**: Update if adding user-facing integration monitoring features
     3. **Engineering Spec**: Update with integration test telemetry implementation details
   - **Do not remove existing information unless it's incorrect**
   - **No approval needed** - make documentation edits directly
   - **Run the entire test suite and make sure all tests are passing**
   - **Make sure pyproject.toml is updated as needed**
   - Double check all subtask requirements are met before marking this subtask as complete
   - **MARK COMPLETE**

