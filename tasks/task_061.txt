# Task ID: 61
# Title: Implement Composer Integration for Full Chat History Access
# Status: in-progress
# Dependencies: None
# Priority: high
# Description: Upgrade cursor_db to use Composer instead of aiService, providing access to complete chronologically-ordered conversation history with timestamps and session names, filtered by git commit time windows.
# Details:
This task involves replacing the current aiService implementation with Cursor's Composer system to improve chat history access:

## Required Reading Before Implementation

**READ FIRST**: `/Users/wiggitywhitney/Repos/mcp-commit-story/docs/cursor-chat-discovery.md` - explains Composer database structure
**REFERENCE**: `/Users/wiggitywhitney/Repos/mcp-commit-story/cursor_chat_sample.json` - shows real data examples

## Key Technical Details from Research

- Composer uses a two-database system:
  - Workspace database (session metadata) in `workspaceStorage/{hash}/state.vscdb`
  - Global database (actual messages) in `globalStorage/state.vscdb`
- Session metadata key: `composer.composerData` in workspace ItemTable
- Message headers key: `composerData:{composerId}` in global cursorDiskKV table
- Individual messages key: `bubbleId:{composerId}:{bubbleId}` in global cursorDiskKV table
- Messages are already chronologically ordered with timestamps

## Implementation Steps

1. Create a new module that interfaces with Composer databases:
   ```typescript
   // Example implementation
   import { Database } from 'better-sqlite3';
   import * as path from 'path';
   import { execSync } from 'child_process';
   
   export class ComposerChatProvider {
     private workspaceDb: Database;
     private globalDb: Database;
     
     constructor(gitRepoPath: string) {
       // Auto-detect workspace based on git repository path
       const workspaceHash = this.detectWorkspaceHash(gitRepoPath);
       this.workspaceDb = new Database(path.join('workspaceStorage', workspaceHash, 'state.vscdb'));
       this.globalDb = new Database(path.join('globalStorage', 'state.vscdb'));
     }
     
     async getChatHistoryForCommit(commitHash: string): Promise<ChatMessage[]> {
       // Get current commit timestamp
       const currentCommitTimestamp = this.getCommitTimestamp(commitHash);
       
       // Get previous commit timestamp
       const previousCommitHash = this.getPreviousCommitHash(commitHash);
       const previousCommitTimestamp = this.getCommitTimestamp(previousCommitHash);
       
       // Define time window: from previous commit to current commit
       const startTime = previousCommitTimestamp;
       const endTime = currentCommitTimestamp;
       
       // Get session metadata from workspace DB
       const sessions = this.getSessionMetadata();
       
       // Get messages from global DB filtered by time window
       const messages = this.getMessagesInTimeWindow(sessions, startTime, endTime);
       
       return messages;
     }
     
     private getCommitTimestamp(commitHash: string): number {
       const timestamp = execSync(
         `git show -s --format=%ct ${commitHash}`,
         { encoding: 'utf-8' }
       ).trim();
       
       // Convert to milliseconds (git returns seconds)
       return parseInt(timestamp) * 1000;
     }
     
     private getPreviousCommitHash(commitHash: string): string {
       return execSync(
         `git rev-parse ${commitHash}~1`,
         { encoding: 'utf-8' }
       ).trim();
     }
     
     private getSessionMetadata() {
       const result = this.workspaceDb.prepare(
         "SELECT value FROM ItemTable WHERE key = 'composer.composerData'"
       ).get();
       
       return JSON.parse(result.value);
     }
     
     private getMessagesInTimeWindow(sessions, startTime, endTime) {
       const messages = [];
       
       for (const session of sessions) {
         // Get message headers
         const headerKey = `composerData:${session.composerId}`;
         const headerRow = this.globalDb.prepare(
           "SELECT value FROM cursorDiskKV WHERE key = ?"
         ).get(headerKey);
         
         if (!headerRow) continue;
         
         const header = JSON.parse(headerRow.value);
         
         // Get individual messages
         for (const bubbleId of header.bubbleIds) {
           const messageKey = `bubbleId:${session.composerId}:${bubbleId}`;
           const messageRow = this.globalDb.prepare(
             "SELECT value FROM cursorDiskKV WHERE key = ?"
           ).get(messageKey);
           
           if (!messageRow) continue;
           
           const message = JSON.parse(messageRow.value);
           
           // Filter by timestamp
           if (message.timestamp >= startTime && message.timestamp <= endTime) {
             messages.push({
               id: message.id,
               role: message.role,
               content: message.content,
               timestamp: message.timestamp,
               sessionName: session.name || 'Unnamed Session',
               // Add other required fields
             });
           }
         }
       }
       
       // Sort by timestamp to ensure chronological order
       return messages.sort((a, b) => a.timestamp - b.timestamp);
     }
     
     private detectWorkspaceHash(gitRepoPath: string): string {
       // Implementation to detect workspace hash based on git repository path
       // This would involve examining the workspaceStorage directory
       // and matching with the current repository
     }
   }
   ```

2. Update the cursor_db module to use the new Composer integration:
   - Replace existing aiService calls with Composer calls
   - Update data models to include timestamps and session names
   - Remove any code related to AI-based conversation reconstruction
   - Implement clear error handling if Composer databases not found

3. Update the API to maintain backward compatibility while providing new features:
   - Keep the same function signatures but enhance return data
   - Add timestamp filtering based on git commit time
   - Include session names in the returned data

4. Update tests:
   - Create new mock data based on the Composer database structure
   - Update existing tests to expect the new data format
   - Add tests for time window filtering

## Implementation Decisions (APPROVED)

- **Time Window Strategy**: Use git commit timestamps to define chat window (previous commit to current commit)
- **Message Filtering**: Filter conversations that happened during the development of the current commit
- **Data Richness**: Include session names from Composer (e.g., "Implement authentication")
- **Workspace Detection**: Auto-detect based on current git repository path
- **Error Handling**: Fail clearly if Composer not found (no fallback to aiService)

# Test Strategy:
1. Unit Tests:
   - Create unit tests for the new Composer integration module
   - Test edge cases like empty chats, very large chats, and malformed responses
   - Mock Composer database responses for predictable testing
   - Test commit-based time window filtering logic with various commit pairs
   - Verify workspace detection logic with different repository paths

2. Integration Tests:
   - Test the integration between cursor_db and the Composer system
   - Verify that chat history is correctly retrieved and formatted
   - Test with real Composer databases in a staging environment
   - Verify session names are correctly included in the output

3. Regression Tests:
   - Ensure all features that previously used aiService continue to work
   - Verify that chat history access works for both recent and older conversations
   - Check that the full chronological history is correctly maintained
   - Verify commit-based time window filtering works correctly with real git commits

4. Performance Tests:
   - Measure and compare load times for chat history between old and new implementations
   - Test with large chat histories to ensure performance remains acceptable
   - Verify memory usage doesn't increase significantly
   - Test performance of SQLite queries on large Composer databases

5. Manual Testing:
   - Manually verify that the complete chat history is accessible
   - Check that the chronological ordering is correct
   - Verify that the UI correctly displays the expanded chat history
   - Test with real git commits to ensure commit-based time windows capture relevant conversations

6. Validation Criteria:
   - All chat history (not just 48 hours) is accessible
   - Messages are correctly filtered by commit-based time windows
   - Session names are included in the output
   - No regression in existing functionality
   - Performance meets or exceeds previous implementation
   - All automated tests pass
   - Journal entries show richer context from complete conversations

# Subtasks:
## 1. Study Composer Database Structure [done]
### Dependencies: None
### Description: Research phase reading docs/cursor-chat-discovery.md and cursor_chat_sample.json, create documentation
### Details:
**READ FIRST**: docs/cursor-chat-discovery.md - explains Composer database structure
**REFERENCE**: cursor_chat_sample.json - shows real data examples

Research the two-database system:
- Workspace database (session metadata) in workspaceStorage/{hash}/state.vscdb
- Global database (actual messages) in globalStorage/state.vscdb
- Session metadata key: composer.composerData in workspace ItemTable
- Message headers key: composerData:{composerId} in global cursorDiskKV table
- Individual messages key: bubbleId:{composerId}:{bubbleId} in global cursorDiskKV table

Create comprehensive documentation of findings for implementation phase.

## 2. Workspace Detection Function [done]
### Dependencies: None
### Description: TDD approach with approval checkpoints for workspace matching strategy
### Details:
**REFERENCE**: docs/cursor-chat-discovery.md for workspace detection strategies

1. Write failing tests for workspace detection based on git repository path
2. Run tests to confirm failure
3. PAUSE FOR MANUAL APPROVAL: Workspace matching strategy (exact path vs fuzzy matching)
4. Implement workspace hash detection function
5. Run tests to confirm they pass
6. Document the chosen approach

Function should auto-detect correct workspace database based on current git repository path by examining workspaceStorage directory structure.
<info added on 2025-06-29T20:47:45.606Z>
**APPROVED DESIGN DECISIONS** (Manual Approval Step 3 Complete)

Design Choice: **Fuzzy Matching with Fallback Strategy**

**Primary Approach**: Scan all workspace directories in workspaceStorage and check workspace.json files for matches

**Matching Criteria (Priority Order)**:
1. **Git remote URL match** (strongest signal - survives repo moves)
2. **Folder path match** (handles case where repo hasn't moved)  
3. **Project/folder name similarity** (last resort before fallback)

**Fallback**: Most recently modified workspace if no good match found
**No caching**: Keep stateless like rest of project
**Confidence threshold**: 0.8 for matches to avoid false positives

**Implementation Considerations**:
- Auto-detect correct workspace database based on current git repository
- Handle edge cases gracefully (missing workspace.json, corrupted data, etc.)
- Log warnings when using fallback strategies

**Telemetry Requirements**:
- Use @trace_mcp_operation decorator on main function
- Track detection_strategy: "workspace_json_match" | "most_recent" | "not_found"
- Track candidates_found: number of potential workspaces scanned
- Track match_confidence: 0.0-1.0 for workspace matches
- Track match_type: "git_remote" | "folder_path" | "folder_name" when matched
- Track fallback_used: boolean
- Error categorization: error.category: "workspace_detection"
- Add metrics: Counter for strategy usage, Histogram for detection duration
- Include span attributes: repo_path, selected workspace path, etc.

**Goal**: Robust solution that reliably finds right workspace database even when developers move/rename projects, with full observability.
</info added on 2025-06-29T20:47:45.606Z>

## 3. Commit-Based Time Window Filtering [done]
### Dependencies: 61.1
### Description: Implement git timestamp filtering with approval for edge cases
### Details:
1. Write failing tests for time window filtering logic
2. Run tests to confirm failure
3. Implement git commit timestamp retrieval functions:
   - getCommitTimestamp(commitHash): Get timestamp for specific commit
   - getPreviousCommitHash(commitHash): Get previous commit hash
   - Handle git command execution errors
   - Convert git timestamps (seconds) to JavaScript timestamps (milliseconds)
4. PAUSE FOR MANUAL APPROVAL: Edge case handling (first commit, merge commits, detached HEAD)
5. Implement message filtering logic for time windows
6. Run tests to confirm they pass

Time window: previous commit timestamp to current commit timestamp.
<info added on 2025-06-29T21:22:10.044Z>
**APPROVED DESIGN DECISIONS** (Manual Approval Complete)

**Core Implementation Strategy:**
- Use GitPython for git operations when possible (preferred over subprocess for consistency)
- Apply error handling pattern from @handle_errors_gracefully decorator in git_hook_worker.py
- Convert git timestamps (seconds) to milliseconds: `parseInt(timestamp) * 1000` to match Cursor's format

**Time Window Logic:**
- **Start Time**: Previous commit timestamp (no buffer)
- **End Time**: Current commit timestamp (no buffer)
- **Rationale**: Captures exact development conversation that led to the commit with no arbitrary time windows

**Edge Case Handling Decisions:**
- **Merge Commit Detection**: Use `len(commit.parents) > 1` (established pattern from context_collection.py)
- **Merge Commits**: Skip entirely - don't generate journal entries for merges to avoid duplicate content
- **First Commit**: Use 24-hour lookback window to capture initial project setup conversations
- **Multiple Parents**: Use `commit.parents[0]` following first-parent convention used throughout codebase  
- **Detached HEAD**: Handle normally using commit's actual parent
- **Git Command Failures**: Follow git_hook_worker.py pattern with log_hook_activity, fall back to 24-hour window, never crash

**Implementation Patterns to Follow:**
```python
# Use GitPython (preferred over subprocess):
is_merge = len(commit.parents) > 1
parent = commit.parents[0] if commit.parents else None

# Timestamp conversion:
timestamp_ms = parseInt(timestamp) * 1000

# Error handling pattern (from git_hook_worker.py):
@handle_errors_gracefully
def get_commit_timestamps(...):
    try:
        # git operations
    except Exception as e:
        log_hook_activity(f"Error message: {str(e)}", "error", repo_path)
        # Use fallback strategy
```

**Telemetry Requirements (per telemetry.md):**
- Use @trace_mcp_operation("commit_time_window_filtering") decorator
- Track span attributes: time_window.strategy ("commit_based" | "fallback_24h" | "first_commit"), time_window.start_timestamp, time_window.end_timestamp, time_window.duration_hours, error.category ("git_command" | "invalid_commit")
- Add metrics using get_mcp_metrics() pattern: Counter for strategy usage, Histogram for time window durations
- Use established error categorization from _categorize_error function

**Logging Pattern:**
```python
logger.debug(f"Using {strategy} time window: {start} to {end}")
logger.warning(f"Git command failed for {commit_hash}, using fallback")
```

**Key Implementation Points:**
- Prefer GitPython over subprocess for git operations (codebase consistency)
- Reuse existing error handling utilities like handle_errors_gracefully
- Follow telemetry patterns exactly as shown in telemetry.py
- Maintain consistency with error categorization approach
- Use established patterns: len(commit.parents) > 1 for merge detection, commit.parents[0] for single parent
</info added on 2025-06-29T21:22:10.044Z>

## 4. ComposerChatProvider Class [done]
### Dependencies: 61.1, 61.2, 61.3
### Description: Main interface class with approval for connection/caching strategies
### Details:
**REFERENCE**: docs/cursor-chat-discovery.md for database connection patterns

1. Write failing tests for ComposerChatProvider class
2. Run tests to confirm failure  
3. Create basic class structure with constructor and main methods
4. PAUSE FOR MANUAL APPROVAL: Database connection strategy (singleton vs per-request, connection pooling)
5. PAUSE FOR MANUAL APPROVAL: Caching strategy for session metadata and messages
6. Implement database connections to both workspace and global databases
7. Implement getChatHistoryForCommit method
8. Implement session metadata retrieval
9. Implement message retrieval with time filtering
10. Run tests to confirm they pass

Class should handle both workspace database (session metadata) and global database (actual messages).
<info added on 2025-06-29T21:33:29.841Z>
**APPROVED DESIGN DECISIONS** (Manual Approval Complete)

**Database Connection Strategy: Per-request connections**
- Create fresh connections for each `getChatHistoryForCommit()` call
- Use context managers for automatic cleanup
- No connection pooling or singleton patterns
- Matches existing pattern in query_executor.py

**Database Path Validation Strategy:**
- Handle missing databases gracefully at query time, not in `__init__()`
- Follow pattern from connection.py where validation happens during actual queries
- Allows class instantiation even if databases aren't ready yet

**Caching Strategy: No caching**
- Query fresh data every time - no caching of session metadata or messages
- Keeps implementation simple and data always accurate
- Consistent with project philosophy (no caching anywhere else in codebase)

**Time Window Input Format:**
- Accept pre-calculated timestamps in milliseconds from Task 61.3
- Method signature: `getChatHistoryForCommit(start_timestamp_ms: int, end_timestamp_ms: int)`
- Clean separation of concerns - Task 61.3 handles git timestamp conversion

**Implementation Guidelines:**
- **Reuse existing infrastructure**:
```python
from ..cursor_db.query_executor import execute_cursor_query
from ..cursor_db.exceptions import CursorDatabaseAccessError, CursorDatabaseQueryError
```

- **Follow connection patterns**:
  - Use `execute_cursor_query()` for all database operations
  - It already handles timeouts, error wrapping, and telemetry
  - Don't create new connection logic

**Error Handling Strategy:**
- Let database errors bubble up from `execute_cursor_query()`
- Add class-specific error handling only where needed
- Use existing exception types (`CursorDatabaseAccessError`, `CursorDatabaseQueryError`)

**Logging Strategy:**
- Add debug logging for empty results following query_executor.py patterns:
```python
logger.debug(f"No sessions found in workspace database: {self.workspace_db_path}")
logger.debug(f"No messages found in time window {start_timestamp_ms} to {end_timestamp_ms}")
```
- Provides debugging info without treating empty results as errors
- Telemetry attributes still track counts (0 sessions, 0 messages) for monitoring

**Telemetry Requirements:**
- Use `@trace_mcp_operation("composer.get_chat_history")` on main method
- Performance threshold: 500ms (from `PERFORMANCE_THRESHOLDS["query_chat_database"]`)
- Track span attributes:
  - `composer.session_count`: Number of sessions found
  - `composer.message_count`: Number of messages retrieved
  - `composer.time_window_hours`: Duration of the time window
  - `database.workspace_path`: Path to workspace DB
  - `database.global_path`: Path to global DB

**Class Structure Design:**
- Keep simple - just store the database paths
- No connection state management
- Methods should be independent (no shared state between calls)
- Focus on business logic of retrieving right messages for time window

**Implementation Focus:**
- Clean, simple implementation leveraging existing database infrastructure
- Avoid reinventing the wheel - reuse proven patterns
- Focus on core responsibility: querying the right chat data for given time windows
- Maintain consistency with established codebase patterns
</info added on 2025-06-29T21:33:29.841Z>

## 5. Update query_cursor_chat_database Function [done]
### Dependencies: 61.4
### Description: Replace aiService with Composer integration
### Details:
**REFERENCE**: src/mcp_commit_story/cursor_db/ module for current implementation

1. Write failing tests for updated query_cursor_chat_database function
2. Run tests to confirm failure
3. Replace aiService calls with ComposerChatProvider calls
4. Update function signature and return data to include timestamps and session names
5. Maintain backward compatibility for existing callers
6. Update error handling to handle Composer database failures
7. Run tests to confirm they pass
8. Document API changes

Function should maintain same external interface while providing richer data from Composer.
<info added on 2025-06-29T23:18:28.761Z>
# Comprehensive Implementation Plan

## Context
Function query_cursor_chat_database() is the integration point between existing system and new Composer approach. Currently only called by collect_chat_history() in context_collection.py.

## Approved Design Decisions

### 1. Function Signature: Keep exactly the same - no parameters
```python
def query_cursor_chat_database() -> Dict[str, Any]:
```
**Rationale**: Maintains perfect backward compatibility

### 2. Time Window: Use "last commit to current" as the default
- Detect current commit internally (since no parameters)
- Calculate time window: previous commit timestamp → current commit timestamp  
- Much smarter than the arbitrary 48-hour window
**Rationale**: Provides precisely the conversations that led to each commit

### 3. Return Structure: Must maintain compatibility
- Keep chat_history key with array of messages
- Each message needs role and content (required)
- Add timestamp and sessionName fields (new)
- Update ChatMessage type definition for type safety
**Rationale**: Existing code continues to work while getting richer data

### 4. Error Handling: Fail fast with clear errors
- No fallback to old aiService approach
- Clear error when Composer databases unavailable
**Rationale**: Makes problems visible rather than masking them

### 5. Implementation: Create new ComposerChatProvider instance per call
- No connection pooling or state management
- Reuse existing execute_cursor_query() infrastructure
**Rationale**: Matches project's stateless patterns

## CRITICAL ADDITIONAL CHANGE
Remove the 200/200 message limiting from collect_chat_history()

**Current Problem**: 
- collect_chat_history() calls query_cursor_chat_database()
- Then applies 200 human + 200 AI message limits
- This was designed for the old 48-hour window approach

**With Composer's precise commit-based time windows**:
- We're already getting ONLY the relevant messages for each commit
- The 200/200 limits are unnecessary and could cut off important conversations
- Example: A 4-hour coding session might have 250 messages - we want them all!

**The Fix**:
- Remove the limit_chat_messages() call from collect_chat_history()
- Return the Composer data directly
- Keep error handling and logging

## Implementation Order
1. First update query_cursor_chat_database() to use Composer
2. Verify it returns messages with new fields
3. Then update collect_chat_history() to remove limiting
4. Test the complete flow

## TDD Implementation Steps
1. Write failing tests for new Composer integration
2. Run tests to confirm they fail
3. Implement query_cursor_chat_database() changes
4. Implement collect_chat_history() changes (remove limiting)
5. Update ChatMessage type definitions
6. Run tests to confirm they pass
7. Test complete integration flow

## Key Implementation Details
- Use get_commit_time_window() from Task 61.3 for precise time windows
- Use detect_workspace() from Task 61.2 for workspace discovery
- Use ComposerChatProvider from Task 61.4 for message retrieval
- Remove limit_chat_messages() from context_collection.py
- Update context_types.py for enhanced ChatMessage type
</info added on 2025-06-29T23:18:28.761Z>
<info added on 2025-06-29T23:20:35.429Z>
## Additional Documentation Update Required:

When implementing the changes, update the collect_chat_history() docstring in context_collection.py to reflect:

**Current docstring mentions:**
- "cursor_db's 48-hour filtering" 
- "hardcoded 200/200 message limits"
- Message limits as "safety net for edge cases"

**New docstring should mention:**
- "commit-based time window filtering" instead of 48-hour
- "complete conversation context" instead of message limits  
- "precise commit-relevant conversations" instead of safety net approach

This ensures the documentation accurately reflects the new intelligent filtering approach.
</info added on 2025-06-29T23:20:35.429Z>

## 6. Remove Conversation Reconstruction Code [done]
### Dependencies: 61.5
### Description: Eliminate AI-based reconstruction since Composer provides chronological data
### Details:
1. Identify all AI-based conversation reconstruction code in cursor_db module
2. Create tests to ensure removal doesn't break functionality
3. Remove message_reconstruction.py functions that are no longer needed
4. Remove any AI provider calls used for conversation ordering
5. Clean up imports and dependencies
6. Update documentation to reflect removal
7. Run all tests to ensure no regressions

Since Composer provides chronologically ordered messages with timestamps, AI-based reconstruction is redundant.
<info added on 2025-06-29T23:57:32.551Z>
## COMPREHENSIVE IMPLEMENTATION PLAN - Approved Design Decisions

### Key Discovery
The current reconstruct_chat_history() is NOT AI-based as originally assumed - it's just a simple formatter that combines prompts and generations. This makes removal much simpler than expected.

### Design Decisions - APPROVED:

**1. Scope: Complete Removal**
- Delete entire message_reconstruction.py module and ALL references
- No deprecation period needed (internal code)
- Since Composer provides already-formatted messages, reconstruction is redundant

**2. Reverse-TDD Strategy**
- Update integration tests FIRST to use Composer data directly
- Prove system works WITHOUT reconstruction before removing code
- Then remove code and failing unit tests
- This provides confidence and reduces risk

**3. Import Cleanup: Aggressive Approach**
- Remove ALL imports systematically using search commands
- Update __init__.py files
- No partial cleanup - complete removal

### Implementation Process - DETAILED:

**STEP 1: Update Integration Tests First**
- Find tests calling reconstruct_chat_history()
- Update to use ComposerChatProvider directly
- Tests should expect enhanced Composer format (timestamp, sessionName, composerId, bubbleId)
- Follow pattern of testing actual implementation vs mocking
- Ensure tests pass (proves new approach works)

**STEP 2: Remove the Code**
- Delete message_reconstruction.py
- Remove all imports using systematic search commands
- Run tests - integration should pass, unit tests for reconstruction will fail (expected)

**STEP 3: Delete Failing Unit Tests**
- Remove test_message_reconstruction.py
- Remove any other reconstruction-specific tests
- Final test run - everything should be green

### Systematic Reference Finding Commands:
```bash
# From project root:
grep -r "message_reconstruction" . --include="*.py"
grep -r "reconstruct_chat_history" . --include="*.py" 
grep -r "from .message_reconstruction" . --include="*.py"
find . -name "__init__.py" -exec grep -l "message_reconstruction" {} \;

# Documentation search:
grep -r "reconstruction" docs/ --include="*.md"
grep -r "reconstruct" docs/ --include="*.md"
grep -r "merge.*conversation" docs/ --include="*.md"

# Code comments/docstrings:
grep -r "reconstruction" . --include="*.py" | grep "#"
```

### Key Implementation Notes:
- **Telemetry Preservation**: Reuse existing @trace_mcp_operation decorators from reconstruction functions on Composer methods
- **Comprehensive Context**: Maintain all telemetry attributes (message counts, session names, etc.)
- **Graceful Degradation**: If Composer data unavailable, return empty results rather than throwing errors
- **Data Format**: Integration tests expect enhanced Composer format with rich metadata
- **Observability**: Same telemetry coverage while simplifying architecture

### Success Criteria:
✓ Zero grep results for "message_reconstruction"
✓ All tests passing  
✓ No documentation mentioning reconstruction
✓ Clean git diff showing only deletions

### Rationale:
This is a great simplification - removing unnecessary complexity because Composer provides better data natively. Less code, better data, same telemetry coverage. Aligns with our principle of using the best available data source.
</info added on 2025-06-29T23:57:32.551Z>

## 7. Update Data Models [pending]
### Dependencies: 61.5
### Description: Enhance with timestamps and session names
### Details:
**REFERENCE**: src/mcp_commit_story/context_types.py for current data models

1. Write failing tests for enhanced data models
2. Run tests to confirm failure
3. Update ChatMessage and related types to include:
   - timestamp: number (JavaScript timestamp in milliseconds)
   - sessionName: string (e.g., \"Implement authentication\")
   - composerId: string (for debugging/tracing)
   - bubbleId: string (for debugging/tracing)
4. Update any interfaces or type definitions
5. Ensure backward compatibility for existing code
6. Run tests to confirm they pass
7. Update documentation for new fields

Enhanced models should support the richer data available from Composer.
<info added on 2025-06-30T00:59:11.891Z>
# Approved Design Decisions for Chat Context Manager

## Core Architecture
- **Thin orchestration layer**: Manager calls `query_cursor_chat_database()` and transforms data for CollectedContext integration
- **No additional filtering**: Trust existing commit-based filtering in `query_cursor_chat_database()`
- **Use existing cursor_db package**: Leverage proven implementation patterns

## Implementation Details

**File**: `src/mcp_commit_story/chat_context_manager.py`

**Data Structures**:
```python
class TimeWindow(TypedDict):
    start_timestamp_ms: int
    end_timestamp_ms: int
    strategy: str  # "commit_based", "first_commit", "fallback_24h"
    duration_hours: float

class ChatContextData(TypedDict):
    messages: List[ChatMessage]  # Using existing ChatMessage from context_types
    time_window: TimeWindow      # From commit_time_window.py
    session_names: List[str]     # Unique session identifiers
    metadata: Dict[str, Any]     # Additional context (message counts, etc.)
```

**Core Function**:
```python
@trace_mcp_operation("chat_context_manager.extract")
def extract_chat_for_commit() -> ChatContextData:
    """Extract chat context for current commit using Composer integration."""
    # 1. Call query_cursor_chat_database() (handles all filtering)
    # 2. Transform response to ChatContextData format
    # 3. Extract unique session names from messages
    # 4. Build TimeWindow from workspace_info
    # 5. Add telemetry spans and error handling
```

## Integration Points
- **Use existing components**: `query_cursor_chat_database()`, `detect_workspace()`, existing telemetry patterns
- **Data format**: Work with `speaker: "user"/"assistant"` from existing ChatMessage
- **Error handling**: Follow cursor_db patterns with graceful degradation
- **Performance**: 500ms threshold per telemetry.md standards

## Key Clarifications
1. **No additional filtering**: Manager trusts `query_cursor_chat_database()`'s commit-based filtering
2. **Database scope**: Let `query_cursor_chat_database()` decide workspace vs global databases  
3. **Message format**: Use existing `speaker` field format, not raw database internals
4. **Return format**: `ChatContextData` as specified above for easy CollectedContext integration

## Telemetry Attributes
- `chat.messages_found` (count)
- `chat.time_window_hours` (duration) 
- `chat.session_count` (unique sessions)
- `chat.workspace_detected` (boolean)
- `error.category` if failures occur

## TDD Implementation Approach
- Start with comprehensive tests defining expected behavior
- Implement minimal code to make tests pass
- Focus on simplicity and reusability
- Follow existing patterns from `context_collection.py` and `cursor_db/`
</info added on 2025-06-30T00:59:11.891Z>

## 8. Comprehensive Error Handling [pending]
### Dependencies: 61.4, 61.5
### Description: Implement robust error handling for database access, git operations, and edge cases
### Details:
1. Write failing tests for various error scenarios
2. Run tests to confirm failure
3. Implement error handling for:
   - Composer databases not found or inaccessible
   - Corrupted database files
   - Git command failures (invalid commit hash, git not available)
   - Missing workspace detection
   - Network/permission issues
   - Invalid session data or missing bubbleIds
   - Database query failures
4. Add clear error messages with debugging information
5. Implement graceful degradation (fallback behaviors where appropriate)
6. Add logging for debugging purposes
7. Run tests to confirm proper error handling
8. Document error conditions and recovery strategies

Error handling should be comprehensive but never block system operation unnecessarily.

## 9. Mock Data for Testing [pending]
### Dependencies: 61.1, 61.7
### Description: Create comprehensive test fixtures based on real Composer database structure
### Details:
1. Analyze real Composer database structure from cursor_chat_sample.json
2. Create mock workspace database data:
   - Session metadata with composerIds
   - Workspace configuration data
   - Various session states (active, completed, etc.)
3. Create mock global database data:
   - Message headers with bubbleIds arrays
   - Individual bubble messages with timestamps
   - Various message types (user, assistant, system)
   - Multiple sessions with overlapping timeframes
4. Create edge case mock data:
   - Empty sessions
   - Corrupted data scenarios
   - Large message volumes
   - Missing references
5. Implement SQLite test database creation helpers
6. Document mock data structure and usage

Mock data should cover normal operations, edge cases, and error scenarios for comprehensive testing.

## 10. Basic Integration Testing [pending]
### Dependencies: 61.8, 61.9
### Description: Smoke tests and end-to-end validation
### Details:
1. Create smoke tests for basic functionality:
   - ComposerChatProvider can connect to databases
   - Workspace detection works with real repositories
   - Message retrieval returns expected format
   - Time window filtering produces reasonable results
2. Create end-to-end integration tests:
   - Full chat history retrieval workflow
   - Integration with existing MCP tools
   - Performance benchmarks vs previous implementation
   - Memory usage validation
3. Test with real Composer databases (if available)
4. Validate session names are correctly retrieved
5. Verify chronological ordering is maintained
6. Test commit-based time window accuracy
7. Run performance comparison tests
8. Document test results and any issues found

Tests should validate the complete integration works as expected in real-world scenarios.

## 11. Telemetry Integration [pending]
### Dependencies: 61.4, 61.5, 61.8
### Description: Add comprehensive telemetry coverage following project standards
### Details:
**REFERENCE**: docs/telemetry.md for project telemetry standards

1. Add @trace_mcp_operation decorators to all new functions:
   - ComposerChatProvider methods
   - Workspace detection functions
   - Time window filtering functions
   - Database query operations
2. Implement comprehensive metrics:
   - mcp_composer_operations_total{operation_type, status}
   - mcp_composer_database_query_duration_seconds{database_type}
   - mcp_workspace_detection_duration_seconds{detection_method}
   - mcp_time_window_filtering_duration_seconds
   - mcp_chat_message_count{session_name}
3. Add structured logging with trace correlation:
   - Database connection events
   - Workspace detection results
   - Time window calculations
   - Message filtering operations
   - Error conditions
4. Ensure graceful degradation (telemetry failures don't block operation)
5. Add telemetry validation tests
6. Document new metrics and traces
7. Verify integration with existing telemetry exporters

All telemetry should follow project standards: automatic trace correlation, JSON logging, and multi-exporter support.

