# Task ID: 5
# Title: Implement Journal Entry Generation
# Status: pending
# Dependencies: 2, 3
# Priority: high
# Description: Create the core functionality for generating journal entries from Git commits, terminal history, and chat context.
# Details:
Implement journal entry generation in `src/mcp_journal/journal.py` with the following features:

1. Journal entry structure:
```python
class JournalEntry:
    """Represents a journal entry with all sections"""
    def __init__(self, commit, config):
        self.commit = commit
        self.config = config
        self.timestamp = datetime.now()
        self.sections = {}
        # Initialize sections based on config
    
    def to_markdown(self):
        """Convert entry to markdown format"""
        # Implementation
```

2. Section generators:
```python
def generate_summary_section(commit, context):
    """Generate the summary section"""
    # Implementation

def generate_accomplishments_section(commit, context):
    """Generate the accomplishments section"""
    # Implementation

def generate_frustrations_section(commit, context):
    """Generate the frustrations section"""
    # Implementation

def generate_terminal_section(context):
    """Generate the terminal commands section"""
    # Implementation

def generate_discussion_section(context):
    """Generate the discussion notes section"""
    # Implementation

def generate_tone_section(commit, context):
    """Generate the tone/mood section"""
    # Implementation

def generate_commit_details_section(commit):
    """Generate the commit details section"""
    # Implementation
```

3. Context collection:
```python
def collect_terminal_history(since_timestamp=None):
    """Collect terminal history since timestamp"""
    # Implementation

def collect_chat_history(since_commit=None):
    """Collect chat history since commit reference"""
    # Implementation

def collect_ai_terminal_commands():
    """Collect terminal commands executed by AI"""
    # Implementation
```

4. File operations:
```python
def get_journal_file_path(date=None):
    """Get path to journal file for date"""
    # Implementation

def append_to_journal_file(entry, file_path):
    """Append entry to journal file"""
    # Implementation

def create_journal_directories():
    """Create journal directory structure"""
    # Implementation
```

# Test Strategy:
1. Unit tests for each section generator
2. Tests for context collection methods
3. Tests for file operations
4. Tests for markdown formatting
5. Tests for handling missing context gracefully
6. Integration tests for full entry generation
7. Tests for anti-hallucination rules
8. Tests for incorporating user preferences and feedback

# Subtasks:
## 1. Implement JournalEntry class with tests [pending]
### Dependencies: None
### Description: Create the JournalEntry class structure and tests for initialization and markdown conversion, with explicit user collaboration at each step
### Details:
Create tests in `tests/test_journal_entry.py` that verify: 1) JournalEntry initializes with commit and config, 2) sections are created based on config, 3) to_markdown() produces correct format. Then implement the JournalEntry class in `src/mcp_journal/journal.py`.

Collaboration steps:
1. Present proposed JournalEntry class structure to user for review
2. Ask specific questions about user preferences:
   - What should the default order of sections be?
   - What timestamp format do you prefer (24h or 12h)?
   - How should section headers be formatted in markdown?
   - What metadata should be included in each entry?
3. Create test cases based on user feedback and present for approval
4. Document all user decisions in code comments and docstrings
5. Implement the class based on approved design
6. Present implementation for final review before marking complete
<info added on 2025-05-20T20:03:40.330Z>
Planned the JournalEntry class implementation based on explicit user preferences and project requirements.

**User Decisions:**
- Sections: Only the following will be included in standard journal entries: Summary, Accomplishments, Frustrations or Roadblocks, Terminal Commands (AI Session), Discussion Notes (from chat), Tone + Mood (inferred), Behind the Commit. The 'Reflections' section is omitted from standard entries and handled separately.
- Field Names: Use the names from the current documentation/spec. Omit empty sections in output; no need to designate required/optional fields.
- Extensibility: No formal extension mechanism now; keep code modular and easy to extend via TDD in the future.
- Output Format: Markdown only, following the established format (headers, lists, code blocks, blockquotes as in examples).
- Review: User will review and approve the proposed class structure and test plan before any code is written.

**Next Steps:**
1. Present a proposed Python class structure and TDD test plan for user review and approval before implementation.
</info added on 2025-05-20T20:03:40.330Z>
<info added on 2025-05-20T21:16:38.374Z>
Finalized the implementation plan for the JournalEntry class and its tests, incorporating user feedback and formatting consistency improvements.

**Key Decisions and Updates:**
- Terminal commands are rendered in a bash code block with a descriptive line, not as a bulleted list.
- Discussion notes support speaker attribution (Human/Agent) and multiline text, rendered as blockquotes with speaker labels.
- The entry header includes both timestamp and commit hash.
- The Tone/Mood section uses blockquotes for both mood and indicators, matching the narrative style of other sections.
- All sections omit empty content, and the class is modular for future extension.
- The test plan covers initialization, Markdown serialization, edge cases (multiline, long entries), and formatting for all sections, including new tests for blockquote formatting in Tone/Mood.

**Next Steps:**
1. Implement the full test file (`tests/test_journal_entry.py`) covering all discussed cases and formatting rules.
2. Implement the JournalEntry class in `src/mcp_commit_story/journal.py` to pass all tests and match the agreed formatting.
</info added on 2025-05-20T21:16:38.374Z>

## 2. Implement file operations with tests [pending]
### Dependencies: None
### Description: Create tests and implement file operation functions for journal management, with explicit user collaboration at each step
### Details:
Create tests in `tests/test_file_operations.py` for get_journal_file_path(), append_to_journal_file(), and create_journal_directories(). Then implement these functions in `src/mcp_journal/journal.py`. Use mocking for file system operations.

Collaboration steps:
1. Present proposed file structure and naming conventions to user
2. Ask specific questions about user preferences:
   - What directory structure do you prefer for journal files?
   - How should files be named (date format, prefixes, etc.)?
   - Should entries be appended to existing files or create new files?
   - What file permissions should be set?
3. Create test cases based on user feedback and present for approval
4. Document all user decisions in code comments and docstrings
5. Implement functions based on approved design
6. Present implementation for final review before marking complete

## 3. Implement context collection functions with tests [pending]
### Dependencies: None
### Description: Create tests and implement functions to collect terminal history, chat history, and AI commands, with explicit user collaboration at each step
### Details:
Create tests in `tests/test_context_collection.py` for collect_terminal_history(), collect_chat_history(), and collect_ai_terminal_commands(). Then implement these functions in `src/mcp_journal/journal.py`. Use mocking for external dependencies.

Collaboration steps:
1. Present proposed context collection approach to user
2. Ask specific questions about user preferences:
   - How far back should terminal history be collected?
   - What format should chat history be stored in?
   - How should AI commands be distinguished from user commands?
   - What context should be excluded or filtered out?
3. Create test cases based on user feedback and present for approval
4. Document all user decisions in code comments and docstrings
5. Implement functions based on approved design
6. Present implementation for final review before marking complete

## 4. Implement commit-based section generators with tests [pending]
### Dependencies: 5.1
### Description: Create tests and implement section generators that use commit information, with explicit user collaboration at each step
### Details:
Create tests in `tests/test_commit_sections.py` for generate_summary_section(), generate_accomplishments_section(), generate_frustrations_section(), and generate_commit_details_section(). Then implement these functions in `src/mcp_journal/journal.py`.

Collaboration steps:
1. Present proposed section generator designs to user
2. Ask specific questions about user preferences:
   - What information should be included in the summary section?
   - How should accomplishments be formatted and categorized?
   - What tone and style should be used for frustrations?
   - What commit details are most important to include?
3. Create test cases based on user feedback and present for approval
4. Document all user decisions in code comments and docstrings
5. Implement functions based on approved design
6. Present implementation for final review before marking complete

## 5. Implement context-based section generators with tests [pending]
### Dependencies: 5.1, 5.3
### Description: Create tests and implement section generators that use terminal and chat context, with explicit user collaboration at each step
### Details:
Create tests in `tests/test_context_sections.py` for generate_terminal_section(), generate_discussion_section(), and generate_tone_section(). Then implement these functions in `src/mcp_journal/journal.py`.

Collaboration steps:
1. Present proposed context-based section generator designs to user
2. Ask specific questions about user preferences:
   - How should terminal commands be grouped or categorized?
   - What aspects of discussions should be highlighted?
   - How should the tone/mood section be structured?
   - What context should be prioritized or filtered out?
3. Create test cases based on user feedback and present for approval
4. Document all user decisions in code comments and docstrings
5. Implement functions based on approved design
6. Present implementation for final review before marking complete

## 6. Create integration test for journal entry generation [pending]
### Dependencies: 5.1, 5.2, 5.3, 5.4, 5.5
### Description: Create an integration test that verifies the full journal entry generation process, with explicit user collaboration at each step
### Details:
Create an integration test in `tests/test_journal_integration.py` that verifies the complete flow: collecting context, creating a journal entry with all sections, and writing to a file.

Collaboration steps:
1. Present proposed integration test approach to user
2. Ask specific questions about user preferences:
   - What end-to-end scenarios are most important to test?
   - What sample inputs should be used for realistic testing?
   - What aspects of the output should be validated?
   - How should the integration test handle dependencies?
3. Create test cases based on user feedback and present for approval
4. Document all user decisions in code comments and docstrings
5. Implement integration test based on approved design
6. Present implementation for final review before marking complete

## 7. Implement edge case handling and error recovery [pending]
### Dependencies: 5.1, 5.2, 5.3, 5.4, 5.5
### Description: Add robust error handling and edge case management to all journal functions, with explicit user collaboration at each step
### Details:
Update all functions to handle edge cases like missing data, API failures, or permission issues. Add appropriate error handling, logging, and fallback mechanisms.

Collaboration steps:
1. Present proposed error handling approach to user
2. Ask specific questions about user preferences:
   - How should errors be communicated to the user?
   - What fallback behavior is preferred for missing data?
   - What level of logging detail is appropriate?
   - Which errors should be fatal vs. non-fatal?
3. Create test cases based on user feedback and present for approval
4. Document all user decisions in code comments and docstrings
5. Implement error handling based on approved design
6. Present implementation for final review before marking complete

## 8. Create CLI command for manual journal generation [pending]
### Dependencies: 5.6, 5.7
### Description: Implement a command-line interface for manually generating journal entries, with explicit user collaboration at each step
### Details:
Create a CLI command in `src/mcp_journal/cli.py` that allows users to manually generate journal entries for specific commits or time periods. Connect this to the journal generation functionality.

Collaboration steps:
1. Present proposed CLI design to user
2. Ask specific questions about user preferences:
   - What command syntax is most intuitive?
   - What options and flags should be available?
   - How should output be displayed in the terminal?
   - What confirmation steps should be included?
3. Create test cases based on user feedback and present for approval
4. Document all user decisions in code comments and docstrings
5. Implement CLI based on approved design
6. Present implementation for final review before marking complete

