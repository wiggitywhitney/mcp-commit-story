{
  "master": {
    "tasks": [
      {
        "id": 11,
        "title": "Implement Summary Generation",
        "description": "Refactor and extend the existing functionality to generate daily, weekly, monthly, quarterly, and yearly summaries of journal entries, with special emphasis on manual reflections.",
        "status": "pending",
        "dependencies": [],
        "priority": "medium",
        "details": "Refactor and extend the existing summary generation in the MCP server with the following features:\n\n1. Refactor date range utilities to support all periods:\n```python\ndef get_date_range(period, date=None):\n    \"\"\"Get start and end dates for a period\"\"\"\n    if date is None:\n        date = datetime.now().date()\n    elif isinstance(date, str):\n        date = datetime.strptime(date, \"%Y-%m-%d\").date()\n    \n    if period == \"day\":\n        return date, date\n    elif period == \"week\":\n        # Start of week (Monday)\n        start = date - timedelta(days=date.weekday())\n        end = start + timedelta(days=6)\n        return start, end\n    elif period == \"month\":\n        start = date.replace(day=1)\n        # Last day of month\n        next_month = date.replace(day=28) + timedelta(days=4)\n        end = next_month - timedelta(days=next_month.day)\n        return start, end\n    elif period == \"quarter\":\n        # Determine which quarter the date falls in\n        quarter = (date.month - 1) // 3 + 1\n        # Start of quarter (first day of first month in quarter)\n        start_month = (quarter - 1) * 3 + 1\n        start = date.replace(month=start_month, day=1)\n        # End of quarter (last day of last month in quarter)\n        end_month = quarter * 3\n        end_day = 31 if end_month in [3, 12] else 30 if end_month in [6, 9] else 28\n        if end_month == 2 and (date.year % 4 == 0 and (date.year % 100 != 0 or date.year % 400 == 0)):\n            end_day = 29  # Leap year\n        end = date.replace(month=end_month, day=end_day)\n        return start, end\n    elif period == \"year\":\n        start = date.replace(month=1, day=1)\n        end = date.replace(month=12, day=31)\n        return start, end\n    else:\n        raise ValueError(f\"Unknown period: {period}\")\n```\n\n2. Extend journal file collection to support all periods:\n```python\ndef get_journal_files_in_range(start_date, end_date, config):\n    \"\"\"Get journal files in date range\"\"\"\n    files = []\n    current = start_date\n    while current <= end_date:\n        file_path = Path(config[\"journal\"][\"path\"]) / \"daily\" / f\"{current.strftime('%Y-%m-%d')}.md\"\n        if file_path.exists():\n            files.append(file_path)\n        current += timedelta(days=1)\n    return files\n```\n\n3. Enhance existing summary generation with improved manual reflection prioritization:\n```python\ndef generate_summary(files, period, config):\n    \"\"\"Generate summary from journal files\"\"\"\n    # Extract content from files\n    entries = []\n    manual_reflections = []\n    \n    for file_path in files:\n        with open(file_path, \"r\") as f:\n            content = f.read()\n            # Extract entries and reflections\n            # Extract manual reflections from special sections\n            reflection_sections = extract_manual_reflections(content, file_path.stem)\n            if reflection_sections:\n                manual_reflections.extend(reflection_sections)\n            # Extract regular entries\n            # Implementation\n    \n    # Analyze entries for significance/complexity\n    weighted_entries = []\n    for entry in entries:\n        # Determine entry significance based on factors like:\n        # - Length/detail of the entry\n        # - Presence of technical terms or complex concepts\n        # - Keywords indicating substantial work (\"implemented\", \"designed\", \"solved\")\n        # - Absence of trivial indicators (\"minor fix\", \"typo\", \"small change\")\n        significance_score = calculate_entry_significance(entry)\n        weighted_entries.append((entry, significance_score))\n    \n    # Sort entries by significance score to prioritize important work\n    weighted_entries.sort(key=lambda x: x[1], reverse=True)\n    \n    # Generate summary sections\n    summary = []\n    \n    # Add manual reflections section first - always prioritized\n    if manual_reflections:\n        summary.append(\"# 📝 Manual Reflections\\n\")\n        summary.append(\"*These are your own reflections from the period, presented verbatim.*\\n\")\n        formatted_reflections = []\n        for date, reflection in manual_reflections:\n            formatted_reflections.append(f\"## {date}\\n\\n{reflection}\\n\")\n        summary.append(\"\\n\".join(formatted_reflections))\n    \n    # Add other sections\n    summary.append(\"# Summary\\n\")\n    # Generate overall summary with emphasis on significant entries\n    \n    summary.append(\"# Key Accomplishments\\n\")\n    # Extract accomplishments, prioritizing substantial work\n    \n    summary.append(\"# Challenges\\n\")\n    # Extract challenges, focusing on complex problems\n    \n    summary.append(\"# Technical Decisions\\n\")\n    # Extract decisions, highlighting important architectural choices\n    \n    return \"\\n\\n\".join(summary)\n\ndef extract_manual_reflections(content, date_str):\n    \"\"\"Extract manual reflections from journal content\"\"\"\n    reflections = []\n    \n    # Look for reflection sections with patterns like:\n    # ## Reflection\n    # ## Daily Reflection\n    # ## Personal Reflection\n    # etc.\n    \n    reflection_patterns = [\n        r\"#+\\s*(?:Daily\\s*)?Reflection[s]?\\s*\\n([\\s\\S]*?)(?:\\n#+\\s|$)\",\n        r\"#+\\s*(?:Personal\\s*)?Thought[s]?\\s*\\n([\\s\\S]*?)(?:\\n#+\\s|$)\",\n        r\"#+\\s*(?:Manual\\s*)?Note[s]?\\s*\\n([\\s\\S]*?)(?:\\n#+\\s|$)\"\n    ]\n    \n    for pattern in reflection_patterns:\n        matches = re.finditer(pattern, content, re.MULTILINE)\n        for match in matches:\n            reflection_text = match.group(1).strip()\n            if reflection_text:  # Only add non-empty reflections\n                reflections.append((date_str, reflection_text))\n    \n    return reflections\n\ndef calculate_entry_significance(entry):\n    \"\"\"Calculate significance score for an entry to prioritize substantial work\"\"\"\n    score = 0\n    \n    # Base score from length (longer entries often indicate more substantial work)\n    score += min(len(entry) / 100, 5)  # Cap at 5 points for length\n    \n    # Keywords indicating substantial work\n    substantial_indicators = [\n        \"implement\", \"design\", \"architecture\", \"refactor\", \"optimize\", \n        \"solve\", \"complex\", \"challenge\", \"significant\", \"major\"\n    ]\n    \n    # Keywords indicating trivial work\n    trivial_indicators = [\n        \"typo\", \"minor fix\", \"small change\", \"tweak\", \"trivial\", \n        \"cosmetic\", \"rename\", \"formatting\"\n    ]\n    \n    # Add points for substantial work indicators\n    for word in substantial_indicators:\n        if word in entry.lower():\n            score += 2\n    \n    # Subtract points for trivial work indicators\n    for word in trivial_indicators:\n        if word in entry.lower():\n            score -= 1.5\n    \n    # Analyze for technical complexity\n    # (This could be enhanced with more sophisticated NLP in the future)\n    technical_terms = [\"algorithm\", \"database\", \"architecture\", \"performance\", \"security\"]\n    for term in technical_terms:\n        if term in entry.lower():\n            score += 1\n    \n    return max(score, 0)  # Ensure score doesn't go negative\n```\n\n4. Extend summary file saving to support all periods:\n```python\ndef save_summary(content, period, date, config):\n    \"\"\"Save summary to appropriate file\"\"\"\n    if period == \"day\":\n        file_name = f\"{date.strftime('%Y-%m-%d')}-summary.md\"\n        dir_path = Path(config[\"journal\"][\"path\"]) / \"summaries\" / \"daily\"\n    elif period == \"week\":\n        # Get week number\n        week_num = date.isocalendar()[1]\n        file_name = f\"{date.strftime('%Y-%m')}-week{week_num}.md\"\n        dir_path = Path(config[\"journal\"][\"path\"]) / \"summaries\" / \"weekly\"\n    elif period == \"month\":\n        file_name = f\"{date.strftime('%Y-%m')}.md\"\n        dir_path = Path(config[\"journal\"][\"path\"]) / \"summaries\" / \"monthly\"\n    elif period == \"quarter\":\n        # Determine which quarter the date falls in\n        quarter = (date.month - 1) // 3 + 1\n        file_name = f\"{date.strftime('%Y')}-Q{quarter}.md\"\n        dir_path = Path(config[\"journal\"][\"path\"]) / \"summaries\" / \"quarterly\"\n    elif period == \"year\":\n        file_name = f\"{date.strftime('%Y')}.md\"\n        dir_path = Path(config[\"journal\"][\"path\"]) / \"summaries\" / \"yearly\"\n    else:\n        raise ValueError(f\"Unknown period: {period}\")\n    \n    # Create file path\n    file_path = dir_path / file_name\n    \n    # Ensure directory exists using on-demand directory creation pattern\n    ensure_journal_directory(dir_path)\n    \n    # Save file\n    with open(file_path, \"w\") as f:\n        f.write(content)\n    \n    return file_path\n```\n\n5. Extend existing MCP handler implementation:\n```python\n@trace_operation(\"journal_summarize\")\nasync def handle_summarize(request):\n    \"\"\"Handle journal/summarize operation\"\"\"\n    period = request.get(\"period\", \"day\")\n    date = request.get(\"date\")\n    date_range = request.get(\"range\")\n    \n    # Load config\n    config = load_config()\n    \n    # Get date range\n    if date_range:\n        # Parse range (format: \"YYYY-MM-DD:YYYY-MM-DD\")\n        start_str, end_str = date_range.split(\":\")\n        start_date = datetime.strptime(start_str, \"%Y-%m-%d\").date()\n        end_date = datetime.strptime(end_str, \"%Y-%m-%d\").date()\n    else:\n        start_date, end_date = get_date_range(period, date)\n    \n    # Get journal files\n    files = get_journal_files_in_range(start_date, end_date, config)\n    if not files:\n        return {\"status\": \"error\", \"error\": \"No journal entries found in date range\"}\n    \n    # Generate summary\n    content = generate_summary(files, period, config)\n    \n    # Save summary\n    file_path = save_summary(content, period, start_date, config)\n    \n    return {\n        \"status\": \"success\",\n        \"file_path\": str(file_path),\n        \"content\": content\n    }\n```\n\n6. Ensure directory creation utility is properly used:\n```python\ndef ensure_journal_directory(dir_path):\n    \"\"\"Ensure the journal directory exists, creating it if necessary\"\"\"\n    if not dir_path.exists():\n        dir_path.mkdir(parents=True, exist_ok=True)\n        logger.info(f\"Created directory: {dir_path}\")\n    return dir_path\n```\n\n7. On-demand directory creation pattern:\n- All summary file-writing operations must use the on-demand directory creation pattern\n- Directories should only be created when needed, not upfront\n- All summary-writing functions (including save_summary) must call ensure_journal_directory(file_path) before writing\n- See docs/on-demand-directory-pattern.md for implementation details and test patterns\n\n8. Enhanced manual reflection prioritization:\n- Manual reflections must be prominently displayed at the beginning of summaries\n- Use visual distinction (emoji, formatting) to highlight manual reflections\n- Include date context for each reflection\n- Preserve the original wording of manual reflections\n- Enhance existing reflection extraction from common section patterns\n- Ensure manual reflections are always prioritized over inferred content\n\nNote: This refactoring focuses on extending the existing functionality in src/mcp_commit_story/daily_summary.py to support all time periods. The existing journal section generators in journal.py should be leveraged rather than reimplemented. The MCP operations should be extended rather than created from scratch.",
        "testStrategy": "1. Unit tests for date range utilities\n   - Test all periods (day, week, month, quarter, year)\n   - Test edge cases like quarter boundaries\n   - Test leap year handling for February in quarterly calculations\n2. Tests for journal file collection\n3. Tests for summary generation\n4. Tests for summary file saving\n   - Test saving for all periods (daily, weekly, monthly, quarterly, yearly)\n   - Test correct file naming for quarterly summaries (YYYY-Q1, YYYY-Q2, etc.)\n5. Tests for MCP handler implementation\n6. Tests for handling different periods (day, week, month, quarter, year)\n7. Tests for handling date ranges\n8. Integration tests for full summary generation flow\n9. Tests for entry significance calculation\n10. Tests to verify that substantial work is properly prioritized in summaries\n11. Tests to verify that trivial entries are de-emphasized in summaries\n12. Tests with mixed entry types to ensure proper weighting in the final summary\n13. Tests for on-demand directory creation:\n    - Test that summary directories are created automatically when they don't exist\n    - Test that ensure_journal_directory() is called for all summary types (daily, weekly, monthly, quarterly, yearly)\n    - Test that directory creation works with nested paths\n    - Test that no errors occur when directories already exist\n    - Test that directories are only created when needed, not upfront\n    - Verify that all summary-writing functions call ensure_journal_directory() before writing\n    - Follow test patterns described in docs/on-demand-directory-pattern.md\n14. Tests to verify that summarization is available as an MCP operation\n15. Tests to verify that the AI agent can properly interact with the summarization functionality\n16. Verify that summary generation works correctly through the MCP interface only (not CLI)\n17. Test that the AI agent can request summaries for different time periods and date ranges\n18. Tests for manual reflection extraction:\n    - Test extraction from various section formats (## Reflection, ## Daily Reflection, etc.)\n    - Test with multiple reflection sections in a single file\n    - Test with reflection sections containing various formatting (lists, code blocks, etc.)\n    - Test with empty reflection sections\n    - Test with reflection sections at different positions in the file\n19. Tests for manual reflection prioritization:\n    - Verify that manual reflections appear at the beginning of summaries\n    - Verify that manual reflections are visually distinguished\n    - Verify that date context is included for each reflection\n    - Verify that original wording is preserved\n    - Test with mixed content (manual reflections and regular entries)\n    - Test with only manual reflections\n    - Test with no manual reflections\n20. Tests for quarterly summary generation:\n    - Test correct date range calculation for each quarter\n    - Test correct file naming (YYYY-Q1, YYYY-Q2, etc.)\n    - Test with entries spanning multiple months within a quarter\n    - Test with entries at quarter boundaries",
        "subtasks": [
          {
            "id": "11.1",
            "title": "Extend existing entry significance calculation",
            "description": "Enhance the existing algorithm to analyze journal entries and assign significance scores based on content analysis.",
            "status": "pending"
          },
          {
            "id": "11.2",
            "title": "Enhance summary generation to prioritize significant entries",
            "description": "Update the existing summary generation logic to give more narrative weight to entries with higher significance scores.",
            "status": "pending"
          },
          {
            "id": "11.3",
            "title": "Create test cases for enhanced entry significance calculation",
            "description": "Develop test cases with various types of entries (substantial, trivial, mixed) to verify proper significance scoring.",
            "status": "pending"
          },
          {
            "id": "11.4",
            "title": "Test summary prioritization with real-world examples",
            "description": "Test the enhanced summary generation with a set of real-world journal entries to ensure meaningful work is properly highlighted.",
            "status": "pending"
          },
          {
            "id": "11.5",
            "title": "Implement ensure_journal_directory utility",
            "description": "Create the utility function to ensure journal directories exist, creating them on-demand if necessary.",
            "status": "pending"
          },
          {
            "id": "11.6",
            "title": "Update save_summary to use ensure_journal_directory",
            "description": "Modify the save_summary function to use the ensure_journal_directory utility for all summary types.",
            "status": "pending"
          },
          {
            "id": "11.7",
            "title": "Add tests for directory creation functionality",
            "description": "Create tests to verify that summary directories are created automatically when they don't exist and that the ensure_journal_directory utility works correctly.",
            "status": "pending"
          },
          {
            "id": "11.8",
            "title": "Implement on-demand directory creation pattern",
            "description": "Update all summary file-writing operations to follow the on-demand directory creation pattern as described in docs/on-demand-directory-pattern.md.",
            "status": "pending"
          },
          {
            "id": "11.9",
            "title": "Add tests for on-demand directory creation",
            "description": "Create tests to verify that directories are only created when needed, not upfront, and that all summary-writing functions call ensure_journal_directory() before writing.",
            "status": "pending"
          },
          {
            "id": "11.10",
            "title": "Review and update all file-writing operations",
            "description": "Review all file-writing operations in the codebase to ensure they follow the on-demand directory creation pattern.",
            "status": "pending"
          },
          {
            "id": "11.11",
            "title": "Extend MCP operation for summarization",
            "description": "Extend the existing MCP operation to support all summary periods (daily, weekly, monthly, quarterly, yearly).",
            "status": "pending"
          },
          {
            "id": "11.12",
            "title": "Test AI agent interaction with extended summarization",
            "description": "Create tests to verify that the AI agent can properly request and process summary generation for all periods through the MCP server.",
            "status": "pending"
          },
          {
            "id": "11.13",
            "title": "Ensure summary generation is MCP-only",
            "description": "Verify that summary generation functionality is only available through the MCP interface and not through CLI commands.",
            "status": "pending"
          },
          {
            "id": "11.14",
            "title": "Update documentation to reflect MCP-only approach",
            "description": "Update relevant documentation to clarify that summary generation is only available through the MCP/AI agent interface, not through CLI commands.",
            "status": "pending"
          },
          {
            "id": "11.15",
            "title": "Enhance existing manual reflection extraction",
            "description": "Improve the existing functionality to extract manual reflections from journal entries using pattern matching for common section headers.",
            "status": "pending"
          },
          {
            "id": "11.16",
            "title": "Enhance manual reflection prioritization in summaries",
            "description": "Update summary generation to display manual reflections prominently at the beginning with visual distinction and date context.",
            "status": "pending"
          },
          {
            "id": "11.17",
            "title": "Add tests for enhanced manual reflection extraction",
            "description": "Create tests to verify that manual reflections are correctly extracted from various section formats and positions.",
            "status": "pending"
          },
          {
            "id": "11.18",
            "title": "Add tests for manual reflection prioritization",
            "description": "Create tests to verify that manual reflections appear at the beginning of summaries with proper visual distinction and preserved wording.",
            "status": "pending"
          },
          {
            "id": "11.19",
            "title": "Implement weekly, monthly, quarterly and yearly summary support",
            "description": "Extend the existing daily summary functionality to support generating weekly, monthly, quarterly, and yearly summaries, including date range calculation and file naming conventions.",
            "status": "pending"
          },
          {
            "id": "11.20",
            "title": "Create tests for extended period summary generation",
            "description": "Develop tests to verify correct date range calculation, file naming, and content generation for weekly, monthly, quarterly, and yearly summaries.",
            "status": "pending"
          },
          {
            "id": "11.21",
            "title": "Update documentation to include all summary periods",
            "description": "Update relevant documentation to include information about weekly, monthly, quarterly, and yearly summary generation and usage.",
            "status": "pending"
          }
        ]
      },
      {
        "id": 12,
        "title": "Implement Blog Post Generation",
        "description": "Create the functionality to convert journal entries and summaries into blog post format for storytelling.",
        "details": "Implement blog post generation in both the MCP server and CLI with the following features:\n\n1. Blog post generation:\n```python\ndef generate_blog_post(files, config):\n    \"\"\"Generate blog post from journal files\"\"\"\n    # Extract content from files\n    entries = []\n    \n    for file_path in files:\n        with open(file_path, \"r\") as f:\n            content = f.read()\n            # Extract entries\n            # Implementation\n    \n    # Generate blog post sections\n    blog_post = []\n    \n    # Add title and introduction\n    blog_post.append(\"# Project Journey: From Idea to Implementation\\n\")\n    blog_post.append(\"*An engineering story based on journal entries*\\n\")\n    \n    # Add narrative sections\n    blog_post.append(\"## The Challenge\\n\")\n    # Generate challenge narrative\n    \n    blog_post.append(\"## The Approach\\n\")\n    # Generate approach narrative\n    \n    blog_post.append(\"## Key Decisions\\n\")\n    # Extract and narrate decisions\n    \n    blog_post.append(\"## Lessons Learned\\n\")\n    # Extract and narrate lessons\n    \n    blog_post.append(\"## Conclusion\\n\")\n    # Generate conclusion\n    \n    return \"\\n\\n\".join(blog_post)\n```\n\n2. Blog post file saving:\n```python\ndef save_blog_post(content, title, config):\n    \"\"\"Save blog post to file\"\"\"\n    # Create directory if needed\n    dir_path = Path(config[\"journal\"][\"path\"]) / \"blog_posts\"\n    dir_path.mkdir(parents=True, exist_ok=True)\n    \n    # Generate file name from title\n    file_name = title.lower().replace(\" \", \"-\") + \".md\"\n    file_path = dir_path / file_name\n    \n    # Save file\n    with open(file_path, \"w\") as f:\n        f.write(content)\n    \n    return file_path\n```\n\n3. MCP handler implementation:\n```python\n@trace_operation(\"journal_blogify\")\nasync def handle_blogify(request):\n    \"\"\"Handle journal/blogify operation\"\"\"\n    files = request.get(\"files\", [])\n    title = request.get(\"title\", \"Engineering Journey\")\n    \n    if not files:\n        return {\"status\": \"error\", \"error\": \"No files provided\"}\n    \n    # Load config\n    config = load_config()\n    \n    # Convert file paths to Path objects\n    file_paths = [Path(f) for f in files]\n    \n    # Check if files exist\n    missing = [str(f) for f in file_paths if not f.exists()]\n    if missing:\n        return {\"status\": \"error\", \"error\": f\"Files not found: {', '.join(missing)}\"}\n    \n    # Generate blog post\n    content = generate_blog_post(file_paths, config)\n    \n    # Save blog post\n    file_path = save_blog_post(content, title, config)\n    \n    return {\n        \"status\": \"success\",\n        \"file_path\": str(file_path),\n        \"content\": content\n    }\n```\n\n4. CLI command implementation:\n```python\n@cli.command()\n@click.argument(\"files\", nargs=-1, type=click.Path(exists=True))\n@click.option(\"--title\", default=\"Engineering Journey\", help=\"Blog post title\")\n@click.option(\"--debug\", is_flag=True, help=\"Show debug information\")\ndef blogify(files, title, debug):\n    \"\"\"Convert journal entries to blog post\"\"\"\n    try:\n        if not files:\n            click.echo(\"No files provided\")\n            return\n        \n        # Load config\n        config = load_config()\n        \n        # Convert file paths to Path objects\n        file_paths = [Path(f) for f in files]\n        \n        # Generate blog post\n        content = generate_blog_post(file_paths, config)\n        \n        # Save blog post\n        file_path = save_blog_post(content, title, config)\n        \n        click.echo(f\"Blog post saved to {file_path}\")\n    except Exception as e:\n        if debug:\n            click.echo(f\"Error: {e}\")\n            traceback.print_exc()\n        else:\n            click.echo(f\"Error: {e}\")\n```",
        "testStrategy": "1. Unit tests for blog post generation\n2. Tests for blog post file saving\n3. Tests for MCP handler implementation\n4. Tests for CLI command implementation\n5. Tests for handling multiple input files\n6. Tests for narrative generation\n7. Integration tests for full blog post generation flow",
        "priority": "low",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 13,
        "title": "Implement Backfill Functionality",
        "description": "Create the functionality to detect and generate journal entries for missed commits.",
        "details": "Implement backfill functionality in both the MCP server and CLI with the following features:\n\n1. Missed commit detection:\n```python\ndef get_missed_commits(repo, config):\n    \"\"\"Get commits that don't have journal entries\"\"\"\n    # Get journal directory\n    journal_path = Path(config[\"journal\"][\"path\"])\n    \n    # Get all commits\n    commits = list(repo.iter_commits())\n    \n    # Get all journal files\n    journal_files = list(journal_path.glob(\"daily/*.md\"))\n    \n    # Extract commit hashes from journal files\n    journal_commits = set()\n    for file_path in journal_files:\n        with open(file_path, \"r\") as f:\n            content = f.read()\n            # Extract commit hashes using regex\n            # Implementation\n    \n    # Find commits not in journal\n    missed_commits = []\n    for commit in commits:\n        if commit.hexsha not in journal_commits and not is_journal_only_commit(commit, config[\"journal\"][\"path\"]):\n            missed_commits.append(commit)\n    \n    return missed_commits\n```\n\n2. Backfill processing:\n```python\ndef process_backfill(commits, config, debug=False):\n    \"\"\"Process backfill for missed commits\"\"\"\n    results = []\n    \n    # Sort commits by date\n    commits.sort(key=lambda c: c.committed_date)\n    \n    for commit in commits:\n        # Generate entry\n        entry = generate_journal_entry(commit, config, debug)\n        if not entry:\n            continue\n        \n        # Mark as backfilled\n        entry.is_backfilled = True\n        \n        # Save entry\n        file_path = save_journal_entry(entry, config)\n        \n        results.append({\n            \"commit\": commit.hexsha,\n            \"file_path\": str(file_path)\n        })\n    \n    return results\n```\n\n3. MCP handler implementation:\n```python\n@trace_operation(\"journal_backfill\")\nasync def handle_backfill(request):\n    \"\"\"Handle journal/backfill operation\"\"\"\n    debug = request.get(\"debug\", False)\n    \n    # Load config\n    config = load_config()\n    \n    # Get repo\n    repo = get_repo()\n    \n    # Get missed commits\n    missed_commits = get_missed_commits(repo, config)\n    if not missed_commits:\n        return {\"status\": \"success\", \"message\": \"No missed commits found\"}\n    \n    # Process backfill\n    results = process_backfill(missed_commits, config, debug)\n    \n    return {\n        \"status\": \"success\",\n        \"count\": len(results),\n        \"entries\": results\n    }\n```\n\n4. CLI command implementation:\n```python\n@cli.command()\n@click.option(\"--debug\", is_flag=True, help=\"Show debug information\")\ndef backfill(debug):\n    \"\"\"Check for missed commits and create entries\"\"\"\n    try:\n        # Load config\n        config = load_config()\n        \n        # Get repo\n        repo = get_repo()\n        \n        # Get missed commits\n        missed_commits = get_missed_commits(repo, config)\n        if not missed_commits:\n            click.echo(\"No missed commits found\")\n            return\n        \n        # Process backfill\n        results = process_backfill(missed_commits, config, debug)\n        \n        click.echo(f\"Created {len(results)} journal entries for missed commits\")\n        for result in results:\n            click.echo(f\"  - {result['commit'][:8]}: {result['file_path']}\")\n    except Exception as e:\n        if debug:\n            click.echo(f\"Error: {e}\")\n            traceback.print_exc()\n        else:\n            click.echo(f\"Error: {e}\")\n```",
        "testStrategy": "1. Unit tests for missed commit detection\n2. Tests for backfill processing\n3. Tests for MCP handler implementation\n4. Tests for CLI command implementation\n5. Tests for handling journal-only commits\n6. Tests for chronological ordering of backfilled entries\n7. Integration tests for full backfill flow",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": []
      },
      {
        "id": 15,
        "title": "Create Comprehensive Tests and Documentation",
        "description": "Develop comprehensive tests for all components and create documentation for the project.",
        "status": "pending",
        "dependencies": [
          11,
          12,
          13
        ],
        "priority": "high",
        "details": "Create comprehensive tests and documentation with the following features:\n\n1. Test fixtures:\n```python\n@pytest.fixture\ndef mock_git_repo():\n    \"\"\"Create temporary git repo with test commits\"\"\"\n    # Implementation\n\n@pytest.fixture\ndef sample_journal_entries():\n    \"\"\"Load sample journal files\"\"\"\n    # Implementation\n\n@pytest.fixture\ndef mock_terminal_history():\n    \"\"\"Provide test terminal command history\"\"\"\n    # Implementation\n\n@pytest.fixture\ndef mock_chat_history():\n    \"\"\"Provide test chat history\"\"\"\n    # Implementation\n\n@pytest.fixture\ndef mock_telemetry_exporter():\n    \"\"\"Provide a test exporter that captures telemetry events\"\"\"\n    # Implementation\n```\n\n2. Unit tests:\n```python\ndef test_config_loading():\n    \"\"\"Test configuration loading\"\"\"\n    # Implementation\n\ndef test_git_utils():\n    \"\"\"Test git utilities\"\"\"\n    # Implementation\n\ndef test_journal_entry_generation():\n    \"\"\"Test journal entry generation\"\"\"\n    # Implementation\n\ndef test_telemetry():\n    \"\"\"Test telemetry integration\"\"\"\n    # Implementation\n\n# Additional unit tests for all components\n```\n\n3. Integration tests:\n```python\ndef test_cli_init():\n    \"\"\"Test CLI init command\"\"\"\n    # Implementation\n\ndef test_cli_new_entry():\n    \"\"\"Test CLI new-entry command\"\"\"\n    # Implementation\n\ndef test_mcp_server():\n    \"\"\"Test MCP server operations\"\"\"\n    # Implementation\n\n# Additional integration tests for all workflows\n```\n\n4. Documentation:\n   - README.md with project overview, installation, and usage\n   - Configuration documentation\n   - CLI command reference\n   - MCP server API reference\n   - Development guide\n   - Examples and tutorials\n\n5. Test coverage:\n   - Configure pytest-cov for coverage reporting\n   - Ensure >90% test coverage\n   - Add coverage reporting to CI pipeline\n\n6. Documentation structure:\n```\nREADME.md\ndocs/\n├── configuration.md\n├── cli.md\n├── mcp-server.md\n├── development.md\n└── examples/\n    ├── basic-usage.md\n    ├── custom-configuration.md\n    └── integration-examples.md\n```",
        "testStrategy": "1. Verify test coverage meets >90% threshold\n2. Ensure all components have unit tests\n3. Verify integration tests cover all workflows\n4. Test documentation for accuracy and completeness\n5. Verify examples work as documented\n6. Test installation and usage instructions\n7. Verify CI pipeline runs all tests\n8. Ensure telemetry system is thoroughly tested with both unit and integration tests",
        "subtasks": [
          {
            "id": "15.1",
            "title": "Implement telemetry-specific tests",
            "description": "Create comprehensive tests for the telemetry system implemented in task 4",
            "status": "pending",
            "details": "Develop unit and integration tests for the telemetry infrastructure including:\n1. Test telemetry event generation\n2. Test telemetry data collection\n3. Test telemetry exporters\n4. Test telemetry configuration options\n5. Test telemetry integration with other components"
          },
          {
            "id": "15.2",
            "title": "Document telemetry system",
            "description": "Create documentation for the telemetry system",
            "status": "pending",
            "details": "Add telemetry documentation including:\n1. Overview of telemetry capabilities\n2. Configuration options for telemetry\n3. How to extend telemetry with custom exporters\n4. Privacy considerations\n5. Add a telemetry.md file to the docs directory"
          }
        ]
      },
      {
        "id": 19,
        "title": "Document MCP Server Configuration and Integration",
        "description": "Ensure the MCP server launch/discovery/configuration requirements are documented in the PRD, README, and codebase. The MCP server must be launchable as a standalone process, expose the required journal operations, and be discoverable by compatible clients. The method for launching the MCP server is not prescribed; it may be started via CLI, Python entry point, etc.",
        "details": "",
        "testStrategy": "",
        "status": "pending",
        "dependencies": [],
        "priority": "medium",
        "subtasks": [
          {
            "id": 1,
            "title": "Provide generic client/editor config block example",
            "description": "Add a JSON example of a configuration block for connecting to the MCP server, showing command, args, and optional env vars.",
            "details": "",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 19
          },
          {
            "id": 2,
            "title": "Clarify API key/env var requirements",
            "description": "Document that API keys or environment variables are only required if the underlying SDK or provider needs them, not for all deployments.",
            "details": "",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 19
          },
          {
            "id": 3,
            "title": "Ensure separation of MCP server config from journal config",
            "description": "Make sure documentation clearly distinguishes between MCP server configuration and the journal system's .mcp-journalrc.yaml.",
            "details": "",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 19
          },
          {
            "id": 4,
            "title": "Review and update README/docs",
            "description": "Review and update the README.md and other documentation to reflect changes made in this task. Ensure documentation is clear, accurate, and up to date.",
            "details": "",
            "status": "pending",
            "dependencies": [
              "19.1",
              "19.2",
              "19.3"
            ],
            "parentTaskId": 19
          }
        ]
      },
      {
        "id": 21,
        "title": "Integrate Codecov for Test Coverage Reporting",
        "description": "Set up Codecov integration with the GitHub repository to track and report test coverage metrics, culminating in a functional coverage badge in the README.",
        "details": "This task involves establishing a connection between the repository and Codecov to enable automated test coverage reporting. Implementation steps include:\n\n1. Create a Codecov account if not already available and link it to the organization's GitHub account\n2. Add the repository to Codecov's dashboard\n3. Generate a Codecov token for secure communication between CI and Codecov\n4. Update the CI pipeline configuration (GitHub Actions, CircleCI, etc.) to:\n   - Install necessary coverage tools (e.g., pytest-cov for Python)\n   - Run tests with coverage collection enabled\n   - Upload coverage reports to Codecov using the token\n5. Add a `.codecov.yml` configuration file to the repository root to customize coverage settings (thresholds, exclusions, etc.)\n6. Uncomment or add the Codecov badge in the README.md file using the format provided by Codecov\n7. Verify the badge displays the actual coverage percentage after the first successful upload\n\nConsider setting coverage thresholds to maintain code quality and potentially configure PR comments from Codecov to highlight coverage changes in code reviews.",
        "testStrategy": "To verify successful completion of this task:\n\n1. Manually trigger a CI build and confirm the coverage report is generated and uploaded to Codecov\n2. Check the Codecov dashboard to ensure:\n   - The repository appears with correct coverage data\n   - Historical data begins tracking from the first upload\n   - Coverage reports include all relevant files (no critical omissions)\n3. Verify the Codecov badge in the README:\n   - Badge is properly displayed (not broken)\n   - Badge shows an actual percentage value (not \"unknown\" or \"N/A\")\n   - The percentage matches what's shown in the Codecov dashboard\n4. Create a test PR with code changes that would affect coverage (both positively and negatively) to confirm:\n   - Codecov reports the coverage change in the PR\n   - The badge updates accordingly after merging\n5. Document the integration process in the project documentation for future reference\n6. Have another team member verify they can access the Codecov dashboard for the repository",
        "status": "pending",
        "dependencies": [],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 22,
        "title": "Implement Remaining MCP Server Handlers",
        "description": "Add the remaining non-MVP MCP tool handlers to complete the full feature set after their backend dependencies are implemented.",
        "status": "pending",
        "dependencies": [
          11,
          12,
          13
        ],
        "priority": "medium",
        "details": "Implement the remaining MCP server tool handlers in `src/mcp_commit_story/server.py` to complete the full feature set:\n\n1. **journal/summarize** handler:\n   - Depends on Task 11 (Summary Generation)\n   - Handle daily, weekly, monthly, yearly summary requests\n   - Return summary content and file paths\n   - Must use on-demand directory creation pattern\n\n2. **journal/blogify** handler:\n   - Depends on Task 12 (Blog Post Generation)\n   - Convert journal entries to blog post format\n   - Accept multiple file inputs\n   - Must use on-demand directory creation pattern\n\n3. **journal/backfill** handler:\n   - Depends on Task 13 (Backfill Functionality)\n   - Detect and create entries for missed commits\n   - Return list of created entries\n   - Must use on-demand directory creation pattern\n\n4. **journal/add-reflection** handler:\n   - Add reflection content to existing journal entries\n   - Accept entry path and reflection content\n   - Must use on-demand directory creation pattern\n\nAll handlers should:\n- Use existing `@handle_mcp_error` decorator\n- Follow TypedDict patterns established in Tasks 6.3-6.4\n- Include proper async/await support\n- Integrate with existing backend logic from their dependency tasks\n- Include comprehensive error handling and validation\n- Call ensure_journal_directory(file_path) before writing any files\n- Never create directories upfront - only on demand when needed\n- Implement as MCP operations only (no CLI commands required)\n- Focus exclusively on MCP/AI agent operations for file-writing handlers\n\nNote that the CLI functionality is limited to setup commands (journal-init, install-hook) only. All file-writing functionality must be implemented as MCP operations. Refer to the updated engineering spec and README.md for implementation details and test patterns.",
        "testStrategy": "1. Unit tests for each new handler\n2. Integration tests with backend logic\n3. Error handling validation\n4. End-to-end workflow testing\n5. Backward compatibility with existing handlers\n6. Verify on-demand directory creation pattern is used correctly\n7. Test that directories are only created when files are actually written\n8. Verify ensure_journal_directory() is called before file writes\n9. Verify all file-writing functionality is accessible via MCP operations only\n10. Test the journal/add-reflection handler functionality as an MCP operation",
        "subtasks": []
      },
      {
        "id": 26,
        "title": "Create Packaging Strategy and Release Process for MVP Launch",
        "description": "Develop a comprehensive packaging and distribution strategy for the MCP Commit Story MVP, including PyPI publishing, version management, installation methods, and release processes.",
        "status": "pending",
        "dependencies": [
          "44"
        ],
        "priority": "high",
        "details": "This task involves creating a complete packaging strategy and implementation plan for the MCP Commit Story MVP launch:\n\n1. **Distribution Strategy**:\n   - Set up PyPI package configuration with appropriate metadata in setup.py/pyproject.toml\n   - Implement semantic versioning (MAJOR.MINOR.PATCH) with version tracking in a dedicated file\n   - Configure CI/CD pipeline for automated releases using GitHub Actions or similar\n   - Define package dependencies with appropriate version constraints\n   - Create package structure with proper namespacing\n\n2. **Installation Methods**:\n   - Implement standard pip installation: `pip install mcp-commit-story`\n   - Create development installation process: `pip install -e .` with dev dependencies\n   - Document MCP server deployment options (standalone, Docker, etc.)\n   - Write detailed configuration guides for different environments\n\n3. **Release Process**:\n   - Implement automated version tagging and changelog generation\n   - Create pre-release testing checklist and validation procedures\n   - Set up documentation update workflow tied to releases\n   - Document rollback procedures for failed releases\n   - Establish release branch strategy (e.g., release/v1.0.0)\n   - Integrate with the Release Preparation Script (Task 30)\n\n4. **User Experience Documentation**:\n   - Write comprehensive getting started guide\n   - Create integration examples for VSCode, PyCharm, and command line\n   - Develop troubleshooting guide with common issues and solutions\n   - Set up community support channels (GitHub Discussions, Discord, etc.)\n   - Document the MCP Info Command functionality (Task 29)\n\n5. **Technical Implementation**:\n   - Define package structure with clear entry points\n   - Implement dependency management with compatibility matrices\n   - Create environment testing matrix (OS, Python versions)\n   - Document performance benchmarks and minimum requirements\n   - Ensure journal entry functionality is properly packaged and accessible\n   - Verify proper integration with the File-Based Logging System (Task 28)\n\nImplementation should follow Python packaging best practices and ensure the journal entry creation functionality from Task 9, daily summary git hook trigger from Task 27, logging system from Task 28, info command from Task 29, and release preparation script from Task 30 are all properly exposed and documented in the package.",
        "testStrategy": "To verify the packaging strategy and release process:\n\n1. **Package Structure Testing**:\n   - Validate package structure using `check-manifest`\n   - Verify all necessary files are included in the distribution\n   - Test package installation in a clean virtual environment\n   - Confirm entry points work as expected after installation\n\n2. **Release Process Validation**:\n   - Perform a test release to TestPyPI\n   - Verify version bumping and changelog generation\n   - Test the release automation pipeline with a pre-release version\n   - Validate rollback procedures with a simulated failed release\n   - Test the Release Preparation Script (Task 30) integration\n\n3. **Installation Testing**:\n   - Test pip installation on different operating systems (Windows, macOS, Linux)\n   - Verify development installation for contributors\n   - Test MCP server deployment using the documented methods\n   - Validate configuration options work as described\n\n4. **Documentation Review**:\n   - Conduct user testing with the getting started guide\n   - Review integration examples for accuracy and completeness\n   - Verify troubleshooting documentation addresses common issues\n   - Test community support channels are properly set up\n   - Verify MCP Info Command (Task 29) documentation is accurate\n\n5. **Functionality Testing**:\n   - Verify journal entry creation (from Task 9) works correctly after package installation\n   - Test daily summary git hook trigger (from Task 27) functions properly\n   - Validate the File-Based Logging System (Task 28) works as expected\n   - Test the MCP Info Command (Task 29) functionality\n   - Verify the Release Preparation Script (Task 30) executes correctly\n   - Test all documented features are accessible through the package\n   - Validate performance meets the documented benchmarks\n   - Ensure compatibility with all supported Python versions and environments\n\nThe packaging strategy is considered complete when a test user can successfully install and use the package following only the provided documentation.",
        "subtasks": [
          {
            "id": "26.1",
            "title": "Package Structure and Basic Configuration",
            "description": "Set up the foundational package structure with proper entry points, dependencies, and basic metadata in pyproject.toml",
            "status": "pending",
            "dependencies": [],
            "details": "**Objective**: Set up the foundational package structure with proper entry points, dependencies, and basic metadata in pyproject.toml\n\n### TDD Steps:\n1. **WRITE TESTS FIRST**\n   - Create `tests/unit/test_package_structure.py`\n   - Test `check_package_installability()` function\n   - Test cases: package can be installed via pip, entry points are accessible, required dependencies are installed, package metadata is correct, module imports work properly\n   - **RUN TESTS - VERIFY THEY FAIL**\n\n2. **GET APPROVAL FOR DESIGN CHOICES**\n   - **PAUSE FOR MANUAL APPROVAL**: Package name decision (mcp-commit-story vs alternatives)\n   - **PAUSE FOR MANUAL APPROVAL**: Entry point structure and CLI command naming\n   - **PAUSE FOR MANUAL APPROVAL**: Dependency version constraints strategy (pinned vs flexible)\n\n3. **IMPLEMENT FUNCTIONALITY**\n   - Implement proper pyproject.toml configuration with metadata, dependencies, and entry points\n   - Create package structure with `__init__.py` files and proper imports\n   - Set up entry points for CLI and MCP server functionality\n   - Handle all error cases identified in tests\n   - **RUN TESTS - VERIFY THEY PASS**\n\n4. **DOCUMENT AND COMPLETE**\n   - Add documentation IF NEEDED in three places:\n     1. **Docs directory**: Create installation.md with package structure documentation\n     2. **PRD**: Update product requirements to reflect packaging strategy and installation methods\n     3. **Engineering Spec**: Update technical implementation details for package architecture and make sure TOC is current\n   - **Do not remove existing information unless it's incorrect**\n   - **No approval needed** - make documentation edits directly\n   - **Run the entire test suite and make sure all tests are passing**\n   - **Make sure pyproject.toml is updated as needed**\n   - Double check all subtask requirements are met before marking this subtask as complete\n   - **MARK COMPLETE**"
          },
          {
            "id": "26.2",
            "title": "Version Management and PyPI Metadata",
            "description": "Implement semantic versioning system and complete PyPI package metadata for public distribution",
            "status": "pending",
            "dependencies": [],
            "details": "**Objective**: Implement semantic versioning system and complete PyPI package metadata for public distribution\n\n### TDD Steps:\n1. **WRITE TESTS FIRST**\n   - Create `tests/unit/test_version_management.py`\n   - Test `get_version()`, `validate_version_format()`, and `increment_version()` functions\n   - Test cases: version extraction from pyproject.toml, semantic version validation, version comparison, changelog integration, PyPI metadata validation\n   - **RUN TESTS - VERIFY THEY FAIL**\n\n2. **GET APPROVAL FOR DESIGN CHOICES**\n   - **PAUSE FOR MANUAL APPROVAL**: Initial version number for MVP (0.1.0 vs 1.0.0 vs other)\n   - **PAUSE FOR MANUAL APPROVAL**: PyPI package description, keywords, and classifiers\n   - **PAUSE FOR MANUAL APPROVAL**: Author information and project URLs structure\n\n3. **IMPLEMENT FUNCTIONALITY**\n   - Implement version management utilities in `src/mcp_commit_story/version.py`\n   - Complete PyPI metadata in pyproject.toml with description, classifiers, urls\n   - Create version validation and update mechanisms\n   - Handle all error cases identified in tests\n   - **RUN TESTS - VERIFY THEY PASS**\n\n4. **DOCUMENT AND COMPLETE**\n   - Add documentation IF NEEDED in three places:\n     1. **Docs directory**: Update installation.md with version information and PyPI details\n     2. **PRD**: Update product requirements to reflect versioning strategy and public availability\n     3. **Engineering Spec**: Update technical implementation details for version management and make sure TOC is current\n   - **Do not remove existing information unless it's incorrect**\n   - **No approval needed** - make documentation edits directly\n   - **Run the entire test suite and make sure all tests are passing**\n   - **Make sure pyproject.toml is updated as needed**\n   - Double check all subtask requirements are met before marking this subtask as complete\n   - **MARK COMPLETE**"
          },
          {
            "id": "26.3",
            "title": "CI/CD Pipeline and Automated Release Process",
            "description": "Create GitHub Actions workflow for automated testing, building, and PyPI publishing with proper release automation",
            "status": "pending",
            "dependencies": [],
            "details": "**Objective**: Create GitHub Actions workflow for automated testing, building, and PyPI publishing with proper release automation\n\n### TDD Steps:\n1. **WRITE TESTS FIRST**\n   - Create `tests/integration/test_release_pipeline.py`\n   - Test `validate_release_process()` and `test_package_build()` functions\n   - Test cases: package builds successfully, tests pass in clean environment, version tagging works, PyPI upload simulation, rollback procedures, multi-platform compatibility\n   - **RUN TESTS - VERIFY THEY FAIL**\n\n2. **GET APPROVAL FOR DESIGN CHOICES**\n   - **PAUSE FOR MANUAL APPROVAL**: CI/CD platform choice (GitHub Actions vs alternatives)\n   - **PAUSE FOR MANUAL APPROVAL**: Release trigger strategy (manual vs automatic on tag)\n   - **PAUSE FOR MANUAL APPROVAL**: Testing matrix (Python versions, OS combinations)\n   - **PAUSE FOR MANUAL APPROVAL**: PyPI vs TestPyPI initial release strategy\n\n3. **IMPLEMENT FUNCTIONALITY**\n   - Implement GitHub Actions workflow in `.github/workflows/release.yml`\n   - Create automated testing pipeline with multiple Python versions and OS\n   - Set up PyPI publishing with proper secrets and authentication\n   - Create release automation scripts and rollback procedures\n   - Handle all error cases identified in tests\n   - **RUN TESTS - VERIFY THEY PASS**\n\n4. **DOCUMENT AND COMPLETE**\n   - Add documentation IF NEEDED in three places:\n     1. **Docs directory**: Create release-process.md with CI/CD documentation and contributor guidelines\n     2. **PRD**: Update product requirements to reflect automated release capabilities\n     3. **Engineering Spec**: Update technical implementation details for CI/CD architecture and make sure TOC is current\n   - **Do not remove existing information unless it's incorrect**\n   - **No approval needed** - make documentation edits directly\n   - **Run the entire test suite and make sure all tests are passing**\n   - **Make sure pyproject.toml is updated as needed**\n   - Double check all subtask requirements are met before marking this subtask as complete\n   - **MARK COMPLETE**"
          },
          {
            "id": "26.4",
            "title": "Installation Methods and Development Setup",
            "description": "Implement and test multiple installation methods including pip install, development setup, and MCP server deployment options",
            "status": "pending",
            "dependencies": [],
            "details": "**Objective**: Implement and test multiple installation methods including pip install, development setup, and MCP server deployment options\n\n### TDD Steps:\n1. **WRITE TESTS FIRST**\n   - Create `tests/integration/test_installation_methods.py`\n   - Test `test_pip_installation()`, `test_dev_installation()`, and `test_mcp_deployment()` functions\n   - Test cases: standard pip install works, development install with editable mode, MCP server starts properly, configuration detection, dependency resolution, uninstall cleanup\n   - **RUN TESTS - VERIFY THEY FAIL**\n\n2. **GET APPROVAL FOR DESIGN CHOICES**\n   - **PAUSE FOR MANUAL APPROVAL**: MCP server deployment strategy (standalone, Docker, systemd service)\n   - **PAUSE FOR MANUAL APPROVAL**: Development setup complexity vs ease of use trade-offs\n   - **PAUSE FOR MANUAL APPROVAL**: Configuration file locations and discovery methods\n\n3. **IMPLEMENT FUNCTIONALITY**\n   - Implement installation validation scripts and MCP server deployment helpers\n   - Create development setup automation and configuration templates\n   - Set up proper package entry points and command line interfaces\n   - Create deployment documentation and configuration examples\n   - Handle all error cases identified in tests\n   - **RUN TESTS - VERIFY THEY PASS**\n\n4. **DOCUMENT AND COMPLETE**\n   - Add documentation IF NEEDED in three places:\n     1. **Docs directory**: Create deployment.md and development.md with comprehensive setup guides\n     2. **PRD**: Update product requirements to reflect deployment options and developer experience\n     3. **Engineering Spec**: Update technical implementation details for installation architecture and make sure TOC is current\n   - **Do not remove existing information unless it's incorrect**\n   - **No approval needed** - make documentation edits directly\n   - **Run the entire test suite and make sure all tests are passing**\n   - **Make sure pyproject.toml is updated as needed**\n   - Double check all subtask requirements are met before marking this subtask as complete\n   - **MARK COMPLETE**"
          },
          {
            "id": "26.5",
            "title": "User Documentation and Getting Started Guide",
            "description": "Create comprehensive user-facing documentation including getting started guide, integration examples, and troubleshooting resources",
            "status": "pending",
            "dependencies": [],
            "details": "**Objective**: Create comprehensive user-facing documentation including getting started guide, integration examples, and troubleshooting resources\n\n### TDD Steps:\n1. **WRITE TESTS FIRST**\n   - Create `tests/integration/test_user_documentation.py`\n   - Test `validate_documentation_examples()` and `test_integration_guides()` functions\n   - Test cases: getting started examples work as written, VSCode integration examples function properly, command line examples produce expected output, troubleshooting steps resolve common issues, all code snippets are valid\n   - **RUN TESTS - VERIFY THEY FAIL**\n\n2. **GET APPROVAL FOR DESIGN CHOICES**\n   - **PAUSE FOR MANUAL APPROVAL**: Documentation structure and organization approach\n   - **PAUSE FOR MANUAL APPROVAL**: Target audience definition (developers vs end users)\n   - **PAUSE FOR MANUAL APPROVAL**: Integration example priorities (which editors/tools to focus on)\n   - **PAUSE FOR MANUAL APPROVAL**: Community support channel setup (GitHub Discussions, Discord, etc.)\n\n3. **IMPLEMENT FUNCTIONALITY**\n   - Implement comprehensive getting started guide with step-by-step instructions\n   - Create integration examples for VSCode, PyCharm, and command line usage\n   - Develop troubleshooting guide with common issues and solutions\n   - Set up community support infrastructure and documentation\n   - Handle all error cases identified in tests\n   - **RUN TESTS - VERIFY THEY PASS**\n\n4. **DOCUMENT AND COMPLETE**\n   - Add documentation IF NEEDED in three places:\n     1. **Docs directory**: Create getting-started.md, integrations.md, and troubleshooting.md\n     2. **PRD**: Update product requirements to reflect user experience and support capabilities\n     3. **Engineering Spec**: Update technical implementation details for documentation architecture and make sure TOC is current\n   - **Do not remove existing information unless it's incorrect**\n   - **No approval needed** - make documentation edits directly\n   - **Run the entire test suite and make sure all tests are passing**\n   - **Make sure pyproject.toml is updated as needed**\n   - Double check all subtask requirements are met before marking this subtask as complete\n   - **MARK COMPLETE**"
          },
          {
            "id": "26.6",
            "title": "Final MVP Release and Launch Validation",
            "description": "Execute the complete MVP release process with final testing, PyPI publishing, and post-launch validation",
            "status": "pending",
            "dependencies": [],
            "details": "**Objective**: Execute the complete MVP release process with final testing, PyPI publishing, and post-launch validation\n\n### TDD Steps:\n1. **WRITE TESTS FIRST**\n   - Create `tests/integration/test_mvp_release.py`\n   - Test `validate_mvp_readiness()`, `test_production_release()`, and `verify_post_launch()` functions\n   - Test cases: all features work in production environment, PyPI package installs correctly, documentation is accessible, performance meets requirements, error handling works properly, rollback procedures function if needed\n   - **RUN TESTS - VERIFY THEY FAIL**\n\n2. **GET APPROVAL FOR DESIGN CHOICES**\n   - **PAUSE FOR MANUAL APPROVAL**: Final release version number and release notes content\n   - **PAUSE FOR MANUAL APPROVAL**: Launch timing and announcement strategy\n   - **PAUSE FOR MANUAL APPROVAL**: Post-launch monitoring and support plan\n   - **PAUSE FOR MANUAL APPROVAL**: Success criteria definition for MVP launch\n\n3. **IMPLEMENT FUNCTIONALITY**\n   - Implement final release checklist validation and automation\n   - Execute production PyPI release with proper version tagging\n   - Verify all installation methods work in clean environments\n   - Set up post-launch monitoring and feedback collection\n   - Handle all error cases identified in tests\n   - **RUN TESTS - VERIFY THEY PASS**\n\n4. **DOCUMENT AND COMPLETE**\n   - Add documentation IF NEEDED in three places:\n     1. **Docs directory**: Create release-notes.md and update all guides with final version information\n     2. **PRD**: Update product requirements to reflect MVP completion and next phase planning\n     3. **Engineering Spec**: Update technical implementation details for production architecture and make sure TOC is current\n   - **Do not remove existing information unless it's incorrect**\n   - **No approval needed** - make documentation edits directly\n   - **Run the entire test suite and make sure all tests are passing**\n   - **Make sure pyproject.toml is updated as needed**\n   - Double check all subtask requirements are met before marking this subtask as complete\n   - **MARK COMPLETE**"
          },
          {
            "id": 27.6,
            "title": "Consider Journal System Architectural Improvements",
            "description": "Review and plan potential improvements to the journal system based on real-world usage experience from tasks 37 and 36",
            "details": "**Objective**: After gaining practical experience with the complete system, evaluate and plan architectural improvements for the journal system.\n\n### Key Areas to Evaluate:\n\n1. **Machine-Readable Delineation**\n   - Assess current entry separation methods (### headers, --- dividers)\n   - Consider standardizing entry format across commits and reflections\n   - Evaluate metadata block possibilities for better parsing\n   - Plan consistent delimiter strategy\n\n2. **Tags and IDs for AI Enhancement**\n   - Evaluate if AI would benefit from entry IDs for cross-referencing\n   - Consider tag system for thematic grouping (#architecture, #debugging, #breakthrough)\n   - Assess potential for relationship mapping between entries\n   - Plan metadata structure for better AI context building\n\n3. **Real-Time AI Access to Reflections**\n   - Evaluate current delayed processing (reflections visible in daily summaries)\n   - Consider real-time awareness during commit processing\n   - Assess impact on AI language pattern consistency ('colorful phrases')\n   - Plan potential hybrid approach for reflection timing\n\n### Evaluation Questions:\n- How well do current entry formats serve both human readability and AI processing?\n- What patterns emerged from AI processing that suggest structural improvements?\n- Do the timing differences between commit entries and reflections create issues?\n- Would structured metadata improve AI context understanding significantly?\n\n### Deliverables:\n- Analysis document with specific recommendations\n- Priority ranking of potential improvements\n- Implementation complexity assessment\n- Plan for future enhancement tasks if warranted\n\n**Note**: This is a consideration/planning phase, not implementation. Focus on learning from actual usage patterns to inform future decisions.\"",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 26
          }
        ]
      },
      {
        "id": 29,
        "title": "Implement MCP Info Command for Diagnostics",
        "description": "Add a new 'info' tool to the MCP server that provides diagnostic information to help users troubleshoot issues, including version, telemetry status, configuration details, and dependency availability.",
        "status": "pending",
        "dependencies": [],
        "priority": "low",
        "details": "Implement the MCP info command in `src/mcp_commit_story/server.py` with the following features:\n\n1. Create a new tool handler using the `@server.tool()` decorator:\n```python\n@server.tool()\nasync def info(request):\n    \"\"\"Return diagnostic information about the MCP server.\"\"\"\n    try:\n        # Get version from pyproject.toml\n        version = get_version_from_pyproject()\n        \n        # Get telemetry status\n        telemetry_status = get_telemetry_status()\n        \n        # Get active configuration path\n        config_path = get_active_config_path()\n        \n        # Get log file location from the logging system\n        log_file = get_log_file_location()\n        \n        # Check dependency availability\n        dependencies = {\n            \"git\": check_git_availability(),\n            \"opentelemetry\": check_opentelemetry_availability()\n        }\n        \n        # Validate configuration\n        config_validation = validate_configuration()\n        \n        return {\n            \"version\": version,\n            \"telemetry_status\": telemetry_status,\n            \"config_path\": str(config_path),\n            \"log_file\": str(log_file),\n            \"dependencies\": dependencies,\n            \"config_validation\": config_validation\n        }\n    except Exception as e:\n        logger.error(f\"Error in info command: {str(e)}\")\n        return {\"error\": str(e)}\n```\n\n2. Implement helper functions for retrieving diagnostic information:\n\n```python\ndef get_version_from_pyproject():\n    \"\"\"Extract version from pyproject.toml.\"\"\"\n    try:\n        import tomli\n        from pathlib import Path\n        \n        # Find the pyproject.toml file (traverse up from current file if needed)\n        current_dir = Path(__file__).parent\n        pyproject_path = None\n        \n        # Look up to 3 levels up for pyproject.toml\n        for i in range(4):\n            check_path = current_dir / (\"../\" * i) / \"pyproject.toml\"\n            if check_path.resolve().exists():\n                pyproject_path = check_path.resolve()\n                break\n        \n        if not pyproject_path:\n            return \"unknown\"\n        \n        with open(pyproject_path, \"rb\") as f:\n            pyproject_data = tomli.load(f)\n            \n        return pyproject_data.get(\"project\", {}).get(\"version\", \"unknown\")\n    except Exception as e:\n        logger.error(f\"Error getting version: {str(e)}\")\n        return \"unknown\"\n\ndef get_telemetry_status():\n    \"\"\"Get the current telemetry status.\"\"\"\n    # Check if telemetry is enabled in configuration\n    config = get_config()\n    return {\n        \"enabled\": config.get(\"telemetry\", {}).get(\"enabled\", False),\n        \"endpoint\": config.get(\"telemetry\", {}).get(\"endpoint\", \"\")\n    }\n\ndef get_active_config_path():\n    \"\"\"Get the path to the active configuration file.\"\"\"\n    # Return the path to the currently loaded config file\n    return get_config_path()\n\ndef get_log_file_location():\n    \"\"\"Get the path to the current log file.\"\"\"\n    # This should use the logging system implemented in Task 28\n    from mcp_commit_story.logging import get_log_file_path\n    return get_log_file_path()\n\ndef check_git_availability():\n    \"\"\"Check if git is available and return version info.\"\"\"\n    try:\n        import subprocess\n        result = subprocess.run([\"git\", \"--version\"], capture_output=True, text=True, check=True)\n        return {\n            \"available\": True,\n            \"version\": result.stdout.strip()\n        }\n    except Exception:\n        return {\n            \"available\": False,\n            \"version\": None\n        }\n\ndef check_opentelemetry_availability():\n    \"\"\"Check if OpenTelemetry is available.\"\"\"\n    try:\n        import opentelemetry\n        return {\n            \"available\": True,\n            \"version\": getattr(opentelemetry, \"__version__\", \"unknown\")\n        }\n    except ImportError:\n        return {\n            \"available\": False,\n            \"version\": None\n        }\n\ndef validate_configuration():\n    \"\"\"Validate the current configuration.\"\"\"\n    # Perform validation checks on the current configuration\n    config = get_config()\n    validation_results = {}\n    \n    # Check for required configuration sections\n    required_sections = [\"journal\", \"git\", \"server\"]\n    for section in required_sections:\n        validation_results[f\"{section}_section\"] = section in config\n    \n    # Check for required paths\n    if \"journal\" in config:\n        journal_path = Path(config[\"journal\"].get(\"path\", \"\"))\n        validation_results[\"journal_path_exists\"] = journal_path.exists()\n    \n    return validation_results\n```\n\n3. Update the MCP server documentation to include the new info command:\n```python\n# In the server documentation string\n\"\"\"\nMCP Server Tools:\n...\n- info: Returns diagnostic information about the MCP server\n\"\"\"\n```\n\n4. Ensure the info command is properly registered with the MCP server and accessible through the standard MCP protocol.",
        "testStrategy": "1. Unit tests for the info command:\n```python\ndef test_info_command():\n    \"\"\"Test that the info command returns all required fields.\"\"\"\n    # Setup mock server\n    server = MockMCPServer()\n    \n    # Call info command\n    response = server.call_tool(\"info\", {})\n    \n    # Verify all required fields are present\n    assert \"version\" in response\n    assert \"telemetry_status\" in response\n    assert \"config_path\" in response\n    assert \"log_file\" in response\n    assert \"dependencies\" in response\n    assert \"config_validation\" in response\n    \n    # Verify dependencies contains required checks\n    assert \"git\" in response[\"dependencies\"]\n    assert \"opentelemetry\" in response[\"dependencies\"]\n\ndef test_version_from_pyproject():\n    \"\"\"Test that version is dynamically read from pyproject.toml.\"\"\"\n    # Create a temporary pyproject.toml with a known version\n    with tempfile.TemporaryDirectory() as tmpdir:\n        temp_path = Path(tmpdir) / \"pyproject.toml\"\n        with open(temp_path, \"w\") as f:\n            f.write('[project]\\nversion = \"1.2.3\"\\n')\n        \n        # Mock the file resolution to return our temporary file\n        with patch(\"pathlib.Path.resolve\", return_value=temp_path):\n            with patch(\"pathlib.Path.exists\", return_value=True):\n                version = get_version_from_pyproject()\n                assert version == \"1.2.3\"\n\ndef test_info_with_various_configs():\n    \"\"\"Test info command with various configuration states.\"\"\"\n    # Test with missing configuration\n    with patch(\"mcp_commit_story.server.get_config\", return_value={}):\n        response = server.call_tool(\"info\", {})\n        assert response[\"config_validation\"][\"journal_section\"] is False\n    \n    # Test with valid configuration\n    valid_config = {\n        \"journal\": {\"path\": \"/tmp/journal\"},\n        \"git\": {\"repo_path\": \"/tmp/repo\"},\n        \"server\": {\"port\": 8000}\n    }\n    with patch(\"mcp_commit_story.server.get_config\", return_value=valid_config):\n        with patch(\"pathlib.Path.exists\", return_value=True):\n            response = server.call_tool(\"info\", {})\n            assert response[\"config_validation\"][\"journal_section\"] is True\n            assert response[\"config_validation\"][\"journal_path_exists\"] is True\n\ndef test_info_through_mcp_protocol():\n    \"\"\"Test that info command works through the MCP protocol.\"\"\"\n    # Start a real MCP server\n    server_process = start_test_server()\n    try:\n        # Connect to the server using the MCP client\n        client = MCPClient(\"localhost\", 8000)\n        \n        # Call the info command\n        response = client.call(\"info\", {})\n        \n        # Verify response\n        assert \"version\" in response\n        assert \"telemetry_status\" in response\n        assert \"config_path\" in response\n        assert \"log_file\" in response\n    finally:\n        # Clean up\n        server_process.terminate()\n```\n\n2. Integration tests:\n   - Test the info command through the MCP protocol from a real client\n   - Verify that all diagnostic information is correctly reported\n   - Test with different configuration states (missing config, invalid paths, etc.)\n   - Verify that the log file location matches the actual log file being used\n\n3. Manual testing:\n   - Call the info command from the CLI client\n   - Verify that all information is displayed correctly\n   - Intentionally break dependencies (e.g., rename git executable) and verify the command correctly reports their unavailability\n   - Test with telemetry enabled and disabled to ensure correct reporting",
        "subtasks": []
      },
      {
        "id": 30,
        "title": "Create Release Preparation Script",
        "description": "Develop an automated release validation script that performs a series of quality checks before publishing to ensure the package meets all requirements.",
        "details": "Implement a comprehensive release preparation script (`scripts/prepare_release.py`) that performs sequential validation checks before publishing:\n\n1. **Git Status Checks**:\n   ```python\n   def check_git_status():\n       \"\"\"Verify git repository is in a clean state for release\"\"\"\n       # Check current branch is main/master\n       branch = subprocess.check_output([\"git\", \"branch\", \"--show-current\"]).decode().strip()\n       if branch not in [\"main\", \"master\"]:\n           return False, f\"Not on main/master branch (current: {branch})\"\n       \n       # Check for uncommitted changes\n       status = subprocess.check_output([\"git\", \"status\", \"--porcelain\"]).decode().strip()\n       if status:\n           return False, \"Uncommitted changes detected\"\n       \n       # Check if local is in sync with remote\n       subprocess.check_call([\"git\", \"fetch\", \"origin\"])\n       local = subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"]).decode().strip()\n       remote = subprocess.check_output([\"git\", \"rev-parse\", f\"origin/{branch}\"]).decode().strip()\n       if local != remote:\n           return False, f\"Local {branch} is not in sync with origin/{branch}\"\n       \n       return True, \"Git status checks passed\"\n   ```\n\n2. **Version Checks**:\n   ```python\n   def check_version():\n       \"\"\"Verify version is consistent and not already published\"\"\"\n       # Get version from pyproject.toml\n       with open(\"pyproject.toml\", \"r\") as f:\n           pyproject = toml.load(f)\n       version = pyproject[\"project\"][\"version\"]\n       \n       # Check version format (semantic versioning)\n       if not re.match(r\"^\\d+\\.\\d+\\.\\d+$\", version):\n           return False, f\"Version {version} does not follow semantic versioning\"\n       \n       # Check if version already exists on PyPI\n       try:\n           response = requests.get(f\"https://pypi.org/pypi/mcp-commit-story/{version}/json\")\n           if response.status_code == 200:\n               return False, f\"Version {version} already exists on PyPI\"\n       except Exception as e:\n           pass  # Connection error is not a failure\n       \n       # Check version consistency across files\n       # (Add checks for other files that might contain version info)\n       \n       return True, f\"Version checks passed: {version}\"\n   ```\n\n3. **Code Quality Checks**:\n   ```python\n   def check_code_quality():\n       \"\"\"Run tests, linting, and security checks\"\"\"\n       # Run tests\n       try:\n           subprocess.check_call([\"pytest\", \"-xvs\"])\n       except subprocess.CalledProcessError:\n           return False, \"Tests failed\"\n       \n       # Run linting\n       try:\n           subprocess.check_call([\"flake8\"])\n       except subprocess.CalledProcessError:\n           return False, \"Linting failed\"\n       \n       # Run security audit\n       try:\n           subprocess.check_call([\"bandit\", \"-r\", \"src\"])\n       except subprocess.CalledProcessError:\n           return False, \"Security audit failed\"\n       \n       return True, \"Code quality checks passed\"\n   ```\n\n4. **Package Validation**:\n   ```python\n   def validate_package():\n       \"\"\"Build and validate the package\"\"\"\n       # Clean previous builds\n       if os.path.exists(\"dist\"):\n           shutil.rmtree(\"dist\")\n       \n       # Build package\n       try:\n           subprocess.check_call([\"python\", \"-m\", \"build\"])\n       except subprocess.CalledProcessError:\n           return False, \"Package build failed\"\n       \n       # Check package size\n       wheel_file = glob.glob(\"dist/*.whl\")[0]\n       size_mb = os.path.getsize(wheel_file) / (1024 * 1024)\n       if size_mb > 10:  # Example threshold\n           return False, f\"Package too large: {size_mb:.2f}MB (max 10MB)\"\n       \n       # Validate package structure\n       try:\n           subprocess.check_call([\"twine\", \"check\", \"dist/*\"])\n       except subprocess.CalledProcessError:\n           return False, \"Package validation failed\"\n       \n       return True, \"Package validation passed\"\n   ```\n\n5. **Main Script Structure**:\n   ```python\n   def main():\n       \"\"\"Run all release preparation checks\"\"\"\n       checks = [\n           (\"Git Status\", check_git_status),\n           (\"Version\", check_version),\n           (\"Code Quality\", check_code_quality),\n           (\"Package Validation\", validate_package)\n       ]\n       \n       for name, check_func in checks:\n           print(f\"Running {name} checks...\")\n           success, message = check_func()\n           if not success:\n               print(f\"❌ {name} check failed: {message}\")\n               sys.exit(1)\n           print(f\"✅ {message}\")\n       \n       print(\"✅ All checks passed! Ready for release.\")\n   \n   if __name__ == \"__main__\":\n       main()\n   ```\n\n6. **Add PyProject.toml Script Entry**:\n   Update `pyproject.toml` to include:\n   ```toml\n   [project.scripts]\n   prepare-release = \"scripts.prepare_release:main\"\n   ```\n\nThe script should be designed to fail fast, stopping at the first check that fails with a clear error message. Each check should be modular and return both a success status and a message explaining the result.",
        "testStrategy": "To verify the release preparation script works correctly:\n\n1. **Test Failure Scenarios**:\n   - Create a git repository with uncommitted changes and verify the script fails with the appropriate error message\n   - Create a version that already exists on PyPI and verify the script detects this\n   - Introduce a failing test and verify the script catches it\n   - Create an invalid package structure and verify the script detects it\n\n2. **Test Error Handling**:\n   - Verify the script provides clear, actionable error messages\n   - Confirm the script exits with non-zero status code on failure\n   - Ensure the script stops at the first failure without continuing\n\n3. **Test Success Path**:\n   - Set up a clean environment that meets all requirements\n   - Run the script and verify it completes successfully\n   - Confirm all checks are executed in the correct order\n\n4. **Integration Testing**:\n   - Test the script in a CI environment to ensure it works in automated contexts\n   - Verify the script can be run via the PyProject.toml entry point\n\n5. **Specific Test Cases**:\n   ```bash\n   # Test git status check failure\n   echo \"test\" > temp.txt\n   ./scripts/prepare_release.py  # Should fail with uncommitted changes message\n   git add temp.txt\n   git commit -m \"temp commit\"\n   ./scripts/prepare_release.py  # Should fail with branch sync message\n   \n   # Test version check\n   # (modify version to match existing PyPI version)\n   ./scripts/prepare_release.py  # Should fail with version exists message\n   \n   # Test successful run\n   git checkout main\n   git pull\n   # (ensure clean state and valid version)\n   ./scripts/prepare_release.py  # Should succeed\n   ```\n\nDocument all test scenarios and expected outcomes to ensure comprehensive coverage of the script's functionality.",
        "status": "pending",
        "dependencies": [
          26,
          "44"
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 31,
        "title": "Refactor Large Modules for Improved Maintainability",
        "description": "Split large files into smaller, more focused modules to improve maintainability while preserving backward compatibility, following MCP best practices of keeping files under 500 lines of code.",
        "details": "This task involves refactoring large modules in the codebase to improve maintainability while ensuring backward compatibility:\n\n1. **Telemetry Module Refactoring**:\n   - Split the current telemetry.py (1800+ lines) into:\n     - `telemetry/core.py`: Core functionality and base classes\n     - `telemetry/decorators.py`: All telemetry-related decorators\n     - `telemetry/metrics.py`: Metric collection and processing\n     - `telemetry/config.py`: Configuration handling for telemetry\n   - Create appropriate `__init__.py` to re-export all public APIs\n\n2. **Journal Module Refactoring**:\n   - Split journal.py into:\n     - `journal/core.py`: Core journal functionality\n     - `journal/generators.py`: Entry generation logic\n     - `journal/serializers.py`: Serialization/deserialization logic\n   - Create appropriate `__init__.py` to re-export all public APIs\n\n3. **Backward Compatibility**:\n   - Ensure all public APIs are maintained\n   - Use `__init__.py` files to re-export all public functions and classes\n   - Example pattern:\n     ```python\n     # In telemetry/__init__.py\n     from .core import TelemetryManager, initialize_telemetry\n     from .decorators import track_usage, measure_performance\n     from .metrics import MetricCollector, send_metrics\n     from .config import TelemetryConfig, load_config\n\n     # Re-export everything to maintain the original API\n     __all__ = [\n         'TelemetryManager', 'initialize_telemetry',\n         'track_usage', 'measure_performance',\n         'MetricCollector', 'send_metrics',\n         'TelemetryConfig', 'load_config'\n     ]\n     ```\n\n4. **Update Import References**:\n   - Scan the entire codebase for imports from the original modules\n   - Update all import statements to reference the new module structure\n   - Use tools like `grep` or IDE search functionality to find all references\n\n5. **Code Organization Guidelines**:\n   - Follow single responsibility principle for each module\n   - Keep related functionality together\n   - Aim for <500 lines of code per file\n   - Add appropriate docstrings to clarify module purpose\n\n6. **Documentation Updates**:\n   - Update any documentation that references the original module structure\n   - Add module-level docstrings explaining the purpose of each new file",
        "testStrategy": "The refactoring will be verified through the following testing approach:\n\n1. **Baseline Test Execution**:\n   - Run the full test suite before starting refactoring to establish a baseline\n   - Document any existing test failures for reference\n\n2. **Incremental Testing**:\n   - Test each module after refactoring, before moving to the next\n   - Run the specific tests related to each module after refactoring\n\n3. **Full Test Suite Verification**:\n   - Run the complete test suite after all refactoring is complete\n   - Ensure all tests pass with the same results as the baseline\n\n4. **Import Compatibility Testing**:\n   - Create specific tests to verify that all public APIs are still accessible\n   - Test both direct imports and from-imports:\n     ```python\n     # Test direct imports still work\n     import telemetry\n     telemetry.initialize_telemetry()\n     \n     # Test specific imports work\n     from telemetry import initialize_telemetry\n     initialize_telemetry()\n     ```\n\n5. **Integration Testing**:\n   - Verify that components using these modules continue to function correctly\n   - Test the full application workflow to ensure no regressions\n\n6. **Manual Verification**:\n   - Manually verify that all modules are under 500 lines of code\n   - Review import statements across the codebase to ensure they've been updated\n\n7. **Documentation Testing**:\n   - Verify that documentation builds correctly with the new module structure\n   - Test any code examples in documentation to ensure they still work",
        "status": "pending",
        "dependencies": [
          26
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 32,
        "title": "Implement Parameter Parsing Leniency for MCP Handlers",
        "description": "Create a flexible parameter parsing system for MCP handlers that accepts common variations in parameter names while maintaining schema integrity.",
        "details": "This task involves implementing a parameter normalization layer to make MCP parameter parsing more flexible:\n\n1. **Parameter Normalization Layer**:\n   - Create a middleware or wrapper function that normalizes incoming parameters before they reach handler functions\n   - Implement in the core MCP request processing pipeline\n   - Design a consistent approach that works across all handlers\n\n2. **Parameter Aliasing Configuration**:\n   - Create a configuration system for parameter aliases with mappings like:\n     ```python\n     PARAMETER_ALIASES = {\n       \"path\": [\"project_path\", \"filepath\", \"file_path\"],\n       \"text\": [\"reflection\", \"content\", \"message\"],\n       \"commit_id\": [\"commit\", \"sha\", \"hash\"],\n       # Add other common variations\n     }\n     ```\n   - Ensure the configuration is extensible and documented\n\n3. **Normalization Logic**:\n   - Implement a function that transforms incoming parameters based on the alias configuration:\n     ```python\n     def normalize_parameters(params, handler_schema):\n         \"\"\"\n         Transform parameters based on aliases to match expected schema\n         while preserving original values when appropriate\n         \"\"\"\n         normalized = params.copy()\n         for expected_param, aliases in PARAMETER_ALIASES.items():\n             if expected_param not in normalized:\n                 for alias in aliases:\n                     if alias in normalized:\n                         normalized[expected_param] = normalized[alias]\n                         break\n         return normalized\n     ```\n\n4. **Schema Integrity**:\n   - Maintain strict schema advertising in API documentation\n   - Add warnings in logs when non-standard parameter names are used\n   - Consider adding deprecation notices for certain aliases to encourage standard usage\n\n5. **Integration**:\n   - Apply normalization before parameter validation\n   - Update all handler functions to use the normalized parameters\n   - Ensure backward compatibility with existing clients\n\n6. **Documentation**:\n   - Document the parameter aliasing system for developers\n   - Update API documentation to note accepted variations where appropriate",
        "testStrategy": "1. **Unit Tests for Normalization**:\n   - Create tests for the parameter normalization function with various input combinations\n   - Verify each alias correctly maps to its canonical parameter name\n   - Test edge cases like conflicting parameters or missing values\n\n2. **Handler Integration Tests**:\n   - For each MCP handler, create test cases that use alternative parameter names\n   - Verify the handler functions correctly with both standard and aliased parameters\n   - Example test cases:\n     ```python\n     def test_commit_handler_with_parameter_aliases():\n         # Test with standard parameters\n         result1 = commit_handler(path=\"/path/to/repo\", message=\"Test commit\")\n         \n         # Test with aliased parameters\n         result2 = commit_handler(project_path=\"/path/to/repo\", reflection=\"Test commit\")\n         \n         # Results should be equivalent\n         assert result1 == result2\n     ```\n\n3. **Backward Compatibility Tests**:\n   - Verify that existing code using standard parameter names continues to work\n   - Run the full test suite to ensure no regressions\n\n4. **MCP Inspector Tests**:\n   - Use the MCP inspector tool to verify parameter handling\n   - Test interactive parameter submission with various aliases\n   - Verify the inspector correctly shows normalized parameters\n\n5. **Error Handling Tests**:\n   - Test scenarios with invalid parameters to ensure proper error messages\n   - Verify that aliasing doesn't interfere with validation logic\n\n6. **Performance Tests**:\n   - Measure any performance impact from the additional normalization layer\n   - Ensure the overhead is minimal for standard parameter usage",
        "status": "pending",
        "dependencies": [
          26
        ],
        "priority": "low",
        "subtasks": []
      },
      {
        "id": 33,
        "title": "Remove All Console Output",
        "description": "Audit and remove all remaining stdout/print statements, replacing them with proper logging and return values to ensure clean operation for MCP clients.",
        "details": "This task involves a systematic audit and cleanup of all console output in the codebase:\n\n1. **Audit Phase**:\n   - Perform a comprehensive search for all `print()` statements, `sys.stdout.write()` calls, and any other direct console output\n   - Create an inventory of all console output locations with their purpose (debug, info, error, etc.)\n   - Categorize outputs as:\n     - Debug/development outputs (to be replaced with logging)\n     - CLI user feedback (to be preserved for human users)\n     - JSON/data outputs (to be converted to return values)\n\n2. **CLI Output Refactoring**:\n   - Modify `cli.py` to properly return values instead of printing JSON:\n   ```python\n   # Before:\n   def get_entries(date_range):\n       entries = journal.get_entries(date_range)\n       print(json.dumps(entries))\n   \n   # After:\n   def get_entries(date_range):\n       entries = journal.get_entries(date_range)\n       return entries  # Click will handle JSON serialization\n   ```\n   - Preserve human-readable help text and error messages in CLI interface\n   - Implement proper exit codes for CLI operations\n\n3. **Logging Implementation**:\n   - Replace all debug/info print statements with appropriate logging calls:\n   ```python\n   # Before:\n   print(f\"Processing commit {commit_id}\")\n   \n   # After:\n   logger.debug(f\"Processing commit {commit_id}\")\n   ```\n   - Ensure all logging uses the file-based logger implemented in Task 28\n   - Add appropriate log levels (DEBUG, INFO, WARNING, ERROR) based on message importance\n\n4. **Return Value Standardization**:\n   - Ensure all functions return proper values instead of printing results\n   - Implement consistent return structures (dictionaries, objects, etc.)\n   - For functions that previously printed status updates, consider adding a callback parameter for progress reporting\n\n5. **MCP Server Cleanup**:\n   - Special focus on MCP server handlers to ensure they never write to stdout\n   - Verify all handlers return proper JSON responses rather than printing them\n   - Implement proper error handling that logs errors but returns appropriate error responses\n\n6. **Exception Handling**:\n   - Review all exception handling to ensure exceptions are logged but not printed\n   - Implement structured error responses for API functions\n\n7. **Documentation Update**:\n   - Update documentation to reflect the new logging approach\n   - Document the return value structures for all public functions",
        "testStrategy": "1. **Automated Output Capture Test**:\n   - Create a test that captures stdout during execution of all major functions\n   - Verify no unexpected output is produced\n   ```python\n   import io\n   import sys\n   from contextlib import redirect_stdout\n   \n   def test_no_stdout_output():\n       f = io.StringIO()\n       with redirect_stdout(f):\n           # Run various operations\n           client.create_entry(commit_id=\"abc123\")\n           client.generate_summary(period=\"day\")\n       \n       output = f.getvalue()\n       assert output == \"\", f\"Unexpected stdout output: {output}\"\n   ```\n\n2. **CLI Command Testing**:\n   - Test all CLI commands with various flags and options\n   - Verify help text is still displayed correctly\n   - Verify error messages are properly shown to users\n   - For commands that should return data, verify the data is correctly returned\n\n3. **Log File Verification**:\n   - Run operations that previously generated console output\n   - Verify appropriate log entries are created in the log file\n   - Check log levels are appropriate for the message content\n\n4. **MCP Client Integration Test**:\n   - Create a test MCP client that consumes the server's responses\n   - Verify the client receives proper return values and not stdout text\n   - Test error conditions to ensure they're properly communicated via return values\n\n5. **Edge Case Testing**:\n   - Test with verbose/debug flags enabled to ensure they affect logging but not stdout\n   - Test with various error conditions to verify errors are logged but not printed\n   - Test concurrent operations to ensure no race conditions in logging\n\n6. **Manual Review**:\n   - Perform a final manual code review to catch any remaining print statements\n   - Run the application with stdout redirected to a file to verify no unexpected output",
        "status": "pending",
        "dependencies": [
          26
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 37,
        "title": "Implement File Watcher Pattern for MCP Tool Signaling in Git Hook Worker",
        "description": "Replace the placeholder call_mcp_tool() function in git_hook_worker.py with a file-based signaling mechanism that allows AI clients to autonomously discover and execute MCP tools for journal generation. Additionally, implement the MCP server entry point in __main__.py to support the python -m mcp_commit_story command.",
        "status": "pending",
        "dependencies": [
          13,
          29
        ],
        "priority": "high",
        "details": "This task involves implementing a file-based signaling mechanism in the git hook worker to enable AI clients to discover and execute MCP tools, as well as creating a properly instrumented __main__.py file:\n\n1. **Update Signal Directory Structure**:\n   - Create a function to ensure the `.mcp-commit-story/signals/` directory exists:\n   ```python\n   def ensure_signal_directory():\n       \"\"\"Create the signals directory if it doesn't exist.\"\"\"\n       signal_dir = Path(\".mcp-commit-story/signals\")\n       signal_dir.mkdir(parents=True, exist_ok=True)\n       return signal_dir\n   ```\n\n2. **Signal File Creation**:\n   - Replace the placeholder `call_mcp_tool()` function with signal file creation:\n   ```python\n   def create_signal_file(commit_info, tool_request):\n       \"\"\"Create a signal file for AI clients to discover.\"\"\"\n       metrics = get_mcp_metrics()\n       \n       try:\n           signal_dir = ensure_signal_directory()\n           \n           # Generate unique signal file name with timestamp and commit hash\n           timestamp = int(time.time())\n           signal_file = signal_dir / f\"{timestamp}_{commit_info['hash'][:8]}.json\"\n           \n           # Prepare signal content\n           signal_data = {\n               \"commit\": commit_info,\n               \"tool_request\": tool_request,\n               \"created_at\": timestamp\n           }\n           \n           # Write signal file\n           with open(signal_file, \"w\") as f:\n               json.dump(signal_data, f, indent=2)\n               \n           metrics.record_counter(\"signal_file_created\", 1)\n           logger.info(f\"Created signal file: {signal_file}\")\n           return signal_file\n       except Exception as e:\n           metrics.record_counter(\"signal_file_creation_error\", 1)\n           logger.error(f\"Failed to create signal file: {str(e)}\")\n           # Graceful degradation - never block git operations\n           return None\n   ```\n\n3. **Update Git Hook Worker**:\n   - Modify the main worker function to use the new signal mechanism:\n   ```python\n   def process_commit(repo_path, commit_hash):\n       \"\"\"Process a commit and create signal files for AI clients.\"\"\"\n       metrics = get_mcp_metrics()\n       metrics.record_counter(\"commit_processed\", 1)\n       \n       try:\n           # Get commit information\n           commit_info = get_commit_info(repo_path, commit_hash)\n           \n           # Create signal for journal entry generation\n           create_signal_file(commit_info, {\n               \"tool\": \"journal/generate\",\n               \"params\": {\n                   \"commit_hash\": commit_hash,\n                   \"repo_path\": repo_path\n               }\n           })\n           \n           # Success - return without blocking git\n           return True\n       except Exception as e:\n           metrics.record_counter(\"commit_processing_error\", 1)\n           logger.error(f\"Error processing commit: {str(e)}\")\n           # Graceful degradation - never block git operations\n           return False\n   ```\n\n4. **Signal Format Documentation**:\n   - Add documentation for the signal file format:\n   ```python\n   \"\"\"\n   Signal File Format:\n   {\n       \"commit\": {\n           \"hash\": \"full_commit_hash\",\n           \"short_hash\": \"short_hash\",\n           \"author\": \"Author Name <email@example.com>\",\n           \"message\": \"Commit message\",\n           \"timestamp\": 1234567890\n       },\n       \"tool_request\": {\n           \"tool\": \"journal/generate\",\n           \"params\": {\n               \"commit_hash\": \"full_commit_hash\",\n               \"repo_path\": \"/path/to/repo\"\n           }\n       },\n       \"created_at\": 1234567890\n   }\n   \"\"\"\n   ```\n\n5. **Telemetry Integration**:\n   - Ensure comprehensive telemetry using existing patterns:\n   ```python\n   # Add these metrics to the existing telemetry\n   metrics.record_counter(\"signal_file_created\", 1)\n   metrics.record_counter(\"signal_file_creation_error\", 1)\n   metrics.record_gauge(\"signal_file_size_bytes\", os.path.getsize(signal_file))\n   ```\n\n6. **Error Handling**:\n   - Implement robust error handling to ensure git operations are never blocked:\n   ```python\n   try:\n       # Signal creation logic\n   except Exception as e:\n       metrics.record_counter(\"signal_file_creation_error\", 1)\n       logger.error(f\"Failed to create signal file: {str(e)}\")\n       # Continue without blocking git operations\n   ```\n\n7. **Cleanup Mechanism**:\n   - Add a function to clean up old signal files:\n   ```python\n   def cleanup_old_signals(max_age_hours=24):\n       \"\"\"Remove signal files older than the specified age.\"\"\"\n       try:\n           signal_dir = Path(\".mcp-commit-story/signals\")\n           if not signal_dir.exists():\n               return\n               \n           current_time = time.time()\n           max_age_seconds = max_age_hours * 3600\n           \n           for signal_file in signal_dir.glob(\"*.json\"):\n               file_age = current_time - signal_file.stat().st_mtime\n               if file_age > max_age_seconds:\n                   signal_file.unlink()\n                   logger.debug(f\"Removed old signal file: {signal_file}\")\n       except Exception as e:\n           logger.error(f\"Error cleaning up signal files: {str(e)}\")\n   ```\n\n8. **MCP Server Entry Point Implementation**:\n   - Create `src/mcp_commit_story/__main__.py` as the official entry point:\n   ```python\n   #!/usr/bin/env python3\n   \"\"\"MCP Commit Story Server Entry Point.\n\n   This module serves as the entry point for the MCP server when invoked via:\n   `python -m mcp_commit_story`\n   \n   It initializes the MCP server with stdio transport, loads configuration,\n   and provides proper error handling and telemetry.\n   \"\"\"\n\n   import sys\n   import logging\n   import traceback\n   from typing import Optional, Dict, Any\n\n   from mcp_commit_story.config import load_config\n   from mcp_commit_story.telemetry import get_mcp_metrics, setup_telemetry\n   from mcp_commit_story.server import MCPServer\n   from mcp_commit_story.transport import StdioTransport\n\n   logger = logging.getLogger(__name__)\n\n   def main() -> int:\n       \"\"\"Initialize and run the MCP server with stdio transport.\n\n       Returns:\n           int: Exit code (0 for success, non-zero for errors)\n       \"\"\"\n       metrics = get_mcp_metrics()\n       metrics.record_counter(\"server_start_attempt\", 1)\n       \n       try:\n           # Setup logging and telemetry\n           setup_telemetry()\n           logger.info(\"Starting MCP Commit Story server\")\n           \n           # Load configuration\n           config = load_config()\n           logger.debug(f\"Loaded configuration: {config}\")\n           \n           # Initialize transport\n           transport = StdioTransport()\n           logger.info(\"Initialized stdio transport\")\n           \n           # Create and start server\n           server = MCPServer(transport=transport, config=config)\n           metrics.record_counter(\"server_started\", 1)\n           logger.info(\"MCP server initialized, starting main loop\")\n           \n           # Run server (this blocks until server exits)\n           exit_code = server.run()\n           \n           # Clean shutdown\n           metrics.record_counter(\"server_shutdown\", 1)\n           logger.info(f\"MCP server shutdown with exit code {exit_code}\")\n           return exit_code\n           \n       except KeyboardInterrupt:\n           metrics.record_counter(\"server_keyboard_interrupt\", 1)\n           logger.info(\"MCP server interrupted by user\")\n           return 130  # Standard exit code for SIGINT\n           \n       except Exception as e:\n           metrics.record_counter(\"server_startup_error\", 1)\n           logger.error(f\"Error starting MCP server: {str(e)}\")\n           logger.debug(f\"Detailed error: {traceback.format_exc()}\")\n           return 1\n\n   if __name__ == \"__main__\":\n       sys.exit(main())\n   ```\n\n9. **Server Configuration Integration**:\n   - Ensure the server loads and validates configuration:\n   ```python\n   def validate_config(config: Dict[str, Any]) -> bool:\n       \"\"\"Validate the MCP server configuration.\n\n       Args:\n           config: The configuration dictionary to validate\n\n       Returns:\n           bool: True if configuration is valid, False otherwise\n       \"\"\"\n       metrics = get_mcp_metrics()\n       \n       try:\n           # Validate required configuration keys\n           required_keys = [\"tools_path\", \"log_level\"]\n           for key in required_keys:\n               if key not in config:\n                   logger.error(f\"Missing required configuration key: {key}\")\n                   metrics.record_counter(\"config_validation_error\", 1)\n                   return False\n                   \n           # Validate tools path exists\n           tools_path = Path(config[\"tools_path\"])\n           if not tools_path.exists() or not tools_path.is_dir():\n               logger.error(f\"Tools path does not exist or is not a directory: {tools_path}\")\n               metrics.record_counter(\"config_validation_error\", 1)\n               return False\n               \n           metrics.record_counter(\"config_validation_success\", 1)\n           return True\n           \n       except Exception as e:\n           logger.error(f\"Error validating configuration: {str(e)}\")\n           metrics.record_counter(\"config_validation_error\", 1)\n           return False\n   ```",
        "testStrategy": "To verify the correct implementation of the file watcher pattern for MCP tool signaling and the MCP server entry point:\n\n1. **Unit Tests for File Watcher Pattern**:\n   - Test signal directory creation:\n   ```python\n   def test_ensure_signal_directory():\n       # Setup: Remove directory if it exists\n       signal_dir = Path(\".mcp-commit-story/signals\")\n       if signal_dir.exists():\n           shutil.rmtree(signal_dir)\n       \n       # Execute\n       result_dir = ensure_signal_directory()\n       \n       # Verify\n       assert signal_dir.exists()\n       assert result_dir == signal_dir\n   ```\n   \n   - Test signal file creation:\n   ```python\n   def test_create_signal_file():\n       # Setup\n       commit_info = {\n           \"hash\": \"abcdef1234567890\",\n           \"short_hash\": \"abcdef12\",\n           \"author\": \"Test User <test@example.com>\",\n           \"message\": \"Test commit\",\n           \"timestamp\": 1234567890\n       }\n       tool_request = {\n           \"tool\": \"journal/generate\",\n           \"params\": {\n               \"commit_hash\": \"abcdef1234567890\",\n               \"repo_path\": \"/path/to/repo\"\n           }\n       }\n       \n       # Execute\n       signal_file = create_signal_file(commit_info, tool_request)\n       \n       # Verify\n       assert signal_file.exists()\n       with open(signal_file, \"r\") as f:\n           data = json.load(f)\n           assert data[\"commit\"] == commit_info\n           assert data[\"tool_request\"] == tool_request\n           assert \"created_at\" in data\n   ```\n   \n   - Test error handling:\n   ```python\n   def test_create_signal_file_error_handling(monkeypatch):\n       # Setup: Mock json.dump to raise an exception\n       def mock_json_dump(*args, **kwargs):\n           raise IOError(\"Simulated error\")\n       \n       monkeypatch.setattr(json, \"dump\", mock_json_dump)\n       \n       # Execute\n       result = create_signal_file({\"hash\": \"test\"}, {\"tool\": \"test\"})\n       \n       # Verify: Should return None but not raise exception\n       assert result is None\n   ```\n\n2. **Unit Tests for MCP Server Entry Point**:\n   - Test main function execution:\n   ```python\n   def test_main_function(monkeypatch):\n       # Setup: Mock dependencies\n       mock_server = MagicMock()\n       mock_server.run.return_value = 0\n       \n       mock_server_class = MagicMock(return_value=mock_server)\n       monkeypatch.setattr(\"mcp_commit_story.server.MCPServer\", mock_server_class)\n       \n       mock_transport = MagicMock()\n       monkeypatch.setattr(\"mcp_commit_story.transport.StdioTransport\", \n                          lambda: mock_transport)\n       \n       mock_config = {\"tools_path\": \"/path/to/tools\", \"log_level\": \"INFO\"}\n       monkeypatch.setattr(\"mcp_commit_story.config.load_config\", \n                          lambda: mock_config)\n       \n       # Execute\n       exit_code = main()\n       \n       # Verify\n       assert exit_code == 0\n       mock_server_class.assert_called_once_with(\n           transport=mock_transport, config=mock_config)\n       mock_server.run.assert_called_once()\n   ```\n   \n   - Test error handling in main function:\n   ```python\n   def test_main_function_error_handling(monkeypatch):\n       # Setup: Mock server to raise exception\n       def mock_server_constructor(*args, **kwargs):\n           raise ValueError(\"Test error\")\n       \n       monkeypatch.setattr(\"mcp_commit_story.server.MCPServer\", \n                          mock_server_constructor)\n       \n       # Execute\n       exit_code = main()\n       \n       # Verify\n       assert exit_code == 1  # Should return error code\n   ```\n   \n   - Test configuration validation:\n   ```python\n   def test_validate_config():\n       # Valid config\n       valid_config = {\n           \"tools_path\": \".\",  # Current directory exists\n           \"log_level\": \"INFO\"\n       }\n       assert validate_config(valid_config) is True\n       \n       # Invalid config - missing key\n       invalid_config = {\"log_level\": \"INFO\"}\n       assert validate_config(invalid_config) is False\n       \n       # Invalid config - non-existent path\n       invalid_path_config = {\n           \"tools_path\": \"/path/that/does/not/exist\",\n           \"log_level\": \"INFO\"\n       }\n       assert validate_config(invalid_path_config) is False\n   ```\n\n3. **Integration Tests**:\n   - Test end-to-end git hook workflow:\n   ```python\n   def test_git_hook_workflow():\n       # Setup: Create a test git repository\n       repo_dir = Path(\"test_repo\")\n       if repo_dir.exists():\n           shutil.rmtree(repo_dir)\n       repo_dir.mkdir()\n       \n       # Initialize git repo and create a commit\n       subprocess.run([\"git\", \"init\"], cwd=repo_dir)\n       (repo_dir / \"test.txt\").write_text(\"test content\")\n       subprocess.run([\"git\", \"add\", \"test.txt\"], cwd=repo_dir)\n       subprocess.run([\"git\", \"config\", \"user.name\", \"Test User\"], cwd=repo_dir)\n       subprocess.run([\"git\", \"config\", \"user.email\", \"test@example.com\"], cwd=repo_dir)\n       subprocess.run([\"git\", \"commit\", \"-m\", \"Test commit\"], cwd=repo_dir)\n       \n       # Get commit hash\n       result = subprocess.run(\n           [\"git\", \"rev-parse\", \"HEAD\"], \n           cwd=repo_dir, \n           capture_output=True, \n           text=True\n       )\n       commit_hash = result.stdout.strip()\n       \n       # Execute\n       process_commit(str(repo_dir.absolute()), commit_hash)\n       \n       # Verify\n       signal_dir = repo_dir / \".mcp-commit-story\" / \"signals\"\n       assert signal_dir.exists()\n       \n       signal_files = list(signal_dir.glob(\"*.json\"))\n       assert len(signal_files) > 0\n       \n       with open(signal_files[0], \"r\") as f:\n           data = json.load(f)\n           assert data[\"commit\"][\"hash\"] == commit_hash\n           assert data[\"tool_request\"][\"tool\"] == \"journal/generate\"\n   ```\n   \n   - Test MCP server startup and communication:\n   ```python\n   def test_mcp_server_startup():\n       # Setup: Create mock stdin/stdout for testing\n       mock_stdin = io.StringIO('{\"jsonrpc\": \"2.0\", \"method\": \"ping\", \"id\": 1}\\n')\n       mock_stdout = io.StringIO()\n       \n       # Patch sys.stdin and sys.stdout\n       with patch(\"sys.stdin\", mock_stdin), patch(\"sys.stdout\", mock_stdout):\n           # Create a server that will process one message and exit\n           transport = StdioTransport()\n           server = MCPServer(transport=transport, config={\"tools_path\": \".\"})  \n           \n           # Run the server (it should process one message and return)\n           server.run(test_mode=True)  # Assuming test_mode makes it exit after one message\n           \n           # Verify response\n           response = mock_stdout.getvalue()\n           assert \"jsonrpc\" in response\n           assert \"result\" in response\n           assert \"id\": 1 in response\n   ```\n\n4. **Telemetry Validation**:\n   - Test telemetry recording for file watcher:\n   ```python\n   def test_file_watcher_telemetry(monkeypatch):\n       # Setup: Mock metrics\n       recorded_metrics = {}\n       \n       class MockMetrics:\n           def record_counter(self, name, value):\n               recorded_metrics[name] = recorded_metrics.get(name, 0) + value\n               \n           def record_gauge(self, name, value):\n               recorded_metrics[name] = value\n       \n       monkeypatch.setattr(\"mcp_commit_story.git_hook_worker.get_mcp_metrics\", \n                          lambda: MockMetrics())\n       \n       # Execute\n       commit_info = {\"hash\": \"test1234\"}\n       tool_request = {\"tool\": \"test\"}\n       create_signal_file(commit_info, tool_request)\n       \n       # Verify\n       assert \"signal_file_created\" in recorded_metrics\n       assert recorded_metrics[\"signal_file_created\"] == 1\n   ```\n   \n   - Test telemetry recording for server entry point:\n   ```python\n   def test_server_telemetry(monkeypatch):\n       # Setup: Mock metrics\n       recorded_metrics = {}\n       \n       class MockMetrics:\n           def record_counter(self, name, value):\n               recorded_metrics[name] = recorded_metrics.get(name, 0) + value\n       \n       monkeypatch.setattr(\"mcp_commit_story.__main__.get_mcp_metrics\", \n                          lambda: MockMetrics())\n       \n       # Mock dependencies to avoid actual server startup\n       mock_server = MagicMock()\n       mock_server.run.return_value = 0\n       monkeypatch.setattr(\"mcp_commit_story.server.MCPServer\", \n                          lambda **kwargs: mock_server)\n       \n       # Execute\n       main()\n       \n       # Verify\n       assert \"server_start_attempt\" in recorded_metrics\n       assert \"server_started\" in recorded_metrics\n       assert \"server_shutdown\" in recorded_metrics\n   ```\n\n5. **Error Handling Verification**:\n   - Test graceful degradation for file watcher:\n   ```python\n   def test_file_watcher_graceful_degradation():\n       # Setup: Create a read-only directory to cause permission error\n       signal_dir = Path(\"read_only_dir\")\n       if signal_dir.exists():\n           shutil.rmtree(signal_dir)\n       signal_dir.mkdir()\n       os.chmod(signal_dir, 0o444)  # Read-only\n       \n       # Monkeypatch the signal directory path\n       with patch(\"mcp_commit_story.git_hook_worker.ensure_signal_directory\", \n                 return_value=signal_dir):\n           # Execute\n           result = create_signal_file({\"hash\": \"test\"}, {\"tool\": \"test\"})\n           \n           # Verify: Should return None but not raise exception\n           assert result is None\n       \n       # Cleanup\n       os.chmod(signal_dir, 0o777)  # Restore permissions for cleanup\n       shutil.rmtree(signal_dir)\n   ```\n   \n   - Test server error handling:\n   ```python\n   def test_server_error_handling(monkeypatch):\n       # Setup: Force an exception during server startup\n       def mock_setup_that_fails():\n           raise RuntimeError(\"Simulated startup failure\")\n       \n       monkeypatch.setattr(\"mcp_commit_story.telemetry.setup_telemetry\", \n                          mock_setup_that_fails)\n       \n       # Execute\n       exit_code = main()\n       \n       # Verify\n       assert exit_code != 0  # Should return non-zero exit code\n   ```\n\n6. **Manual Testing**:\n   - Install the updated package in a real repository\n   - Make commits and verify signal files are created\n   - Check that AI clients can discover and process the signals\n   - Verify git operations remain fast and unblocked even if signal creation fails\n   - Test the MCP server by running `python -m mcp_commit_story` and verifying it starts correctly\n   - Test integration with Cursor by configuring `.cursor/mcp.json` to use the package",
        "subtasks": [
          {
            "id": 1,
            "title": "Create MCP Server Entry Point with Comprehensive Telemetry",
            "description": "Implement properly instrumented src/mcp_commit_story/__main__.py as the official entry point for python -m mcp_commit_story command used in .cursor/mcp.json configuration.",
            "details": "**Objective**: Implement properly instrumented `src/mcp_commit_story/__main__.py` as the official entry point for `python -m mcp_commit_story` command used in `.cursor/mcp.json` configuration.\n\n### TDD Steps:\n1. **WRITE TESTS FIRST**\n   - Create `tests/unit/test_mcp_server_entry_point.py`\n   - Test `main()` function with successful server startup and shutdown\n   - Test `validate_server_config()` function for configuration validation\n   - Test `setup_server_telemetry()` function for telemetry initialization\n   - Test cases: successful startup with valid config, startup failure with invalid config, graceful shutdown handling, telemetry recording for all server events, exit code validation, keyboard interrupt handling\n   - **RUN TESTS - VERIFY THEY FAIL**\n\n2. **GET APPROVAL FOR DESIGN CHOICES**\n   - **PAUSE FOR MANUAL APPROVAL**: Server initialization approach (FastMCP vs custom implementation)\n   - **PAUSE FOR MANUAL APPROVAL**: Exit code strategy for different failure scenarios\n   - **PAUSE FOR MANUAL APPROVAL**: Telemetry failure handling (continue vs abort server startup)\n\n3. **IMPLEMENT FUNCTIONALITY**\n   - Implement `main()` function with stdio transport initialization\n   - Create server configuration validation with comprehensive error messages\n   - Set up telemetry integration using existing MCPMetrics patterns\n   - Add graceful shutdown handling for SIGINT and other signals\n   - Implement proper exit codes following Unix conventions\n   - Add comprehensive logging for startup, shutdown, and error scenarios\n   - **RUN TESTS - VERIFY THEY PASS**\n\n4. **DOCUMENT AND COMPLETE**\n   - Add documentation IF NEEDED in three places:\n     1. **Docs directory**: Update deployment.md with MCP server entry point documentation\n     2. **PRD**: Update product requirements to reflect MCP server startup capabilities\n     3. **Engineering Spec**: Update technical implementation details for server architecture and make sure TOC is current\n   - **Do not remove existing information unless it's incorrect**\n   - **No approval needed** - make documentation edits directly\n   - **Run the entire test suite and make sure all tests are passing**\n   - **Make sure pyproject.toml is updated as needed**\n   - Double check all subtask requirements are met before marking this subtask as complete\n   - **MARK COMPLETE**",
            "status": "in-progress",
            "dependencies": [],
            "parentTaskId": 37
          },
          {
            "id": 2,
            "title": "Implement Signal Directory Management and File Creation",
            "description": "Create signal directory structure and file-based signaling mechanism using generic create_tool_signal() function for any MCP tool.",
            "details": "**Objective**: Create signal directory structure and file-based signaling mechanism using generic `create_tool_signal()` function for any MCP tool.\n\n### TDD Steps:\n1. **WRITE TESTS FIRST**\n   - Create `tests/unit/test_signal_file_management.py`\n   - Test `ensure_signal_directory()` function for directory creation and validation\n   - Test `create_signal_file()` function for generic signal file generation\n   - Test `validate_signal_format()` function for JSON structure validation\n   - Test cases: successful directory creation, permission errors with graceful degradation, signal file creation with proper metadata, invalid JSON handling, disk space errors, generic tool signal format validation\n   - **RUN TESTS - VERIFY THEY FAIL**\n\n2. **GET APPROVAL FOR DESIGN CHOICES**\n   - **PAUSE FOR MANUAL APPROVAL**: Signal metadata scope (how much commit context to include)\n   - **PAUSE FOR MANUAL APPROVAL**: File naming convention for uniqueness and ordering\n   - **PAUSE FOR MANUAL APPROVAL**: JSON structure vs compressed format for readability\n\n3. **IMPLEMENT FUNCTIONALITY**\n   - Implement `ensure_signal_directory()` with proper path resolution and permissions\n   - Create `create_signal_file()` with unique naming, JSON formatting, and error handling\n   - Add `validate_signal_format()` for signal content validation\n   - Include comprehensive telemetry for all file operations\n   - Ensure graceful degradation never blocks git operations\n   - Add thread safety for concurrent signal creation\n   - **RUN TESTS - VERIFY THEY PASS**\n\n4. **DOCUMENT AND COMPLETE**\n   - Add documentation IF NEEDED in three places:\n     1. **Docs directory**: Create signal-format.md documenting the file-based signaling mechanism\n     2. **PRD**: Update product requirements to reflect signal-based AI integration\n     3. **Engineering Spec**: Update technical implementation details for signal architecture and make sure TOC is current\n   - **Do not remove existing information unless it's incorrect**\n   - **No approval needed** - make documentation edits directly\n   - **Run the entire test suite and make sure all tests are passing**\n   - **Make sure pyproject.toml is updated as needed**\n   - Double check all subtask requirements are met before marking this subtask as complete\n   - **MARK COMPLETE**\n<info added on 2025-06-11T11:55:10.793Z>\n**IMPLEMENTATION COMPLETED**: Signal Directory Management and File Creation\n\n**Implementation Summary:**\n- **Module Created**: `src/mcp_commit_story/signal_management.py` (355 lines)\n- **Test Suite**: `tests/unit/test_signal_file_management.py` (545 lines, 24 tests)\n- **All 24 tests passing** with comprehensive coverage\n\n**Key Functions Implemented:**\n1. `ensure_signal_directory()` - Creates `.mcp-commit-story/signals/` structure with proper validation\n2. `create_signal_file()` - Generates unique signal files with approved design:\n   - Timestamp-based naming: `{timestamp}_{tool_name}_{hash_prefix}.json`\n   - Standard metadata scope (hash, author, date, message, files changed, stats)\n   - Pretty JSON format for readability\n   - Thread safety with locks\n   - Graceful degradation for git operations\n3. `validate_signal_format()` - JSON structure validation with required fields\n\n**Advanced Features:**\n- **Thread Safety**: `threading.Lock()` for concurrent signal creation\n- **Telemetry Integration**: Comprehensive metrics with graceful fallback when metrics unavailable\n- **Error Handling**: Custom exceptions (`SignalDirectoryError`, `SignalFileError`, `SignalValidationError`) with graceful degradation flags\n- **Filename Uniqueness**: Microsecond timestamps + collision detection with counter suffix\n- **Utility Functions**: 6 helper functions for signal management operations\n\n**Production-Ready Features:**\n- **Graceful degradation** - never blocks git operations\n- **Comprehensive telemetry** - tracks all operations and errors\n- **Thread safety** - handles concurrent git hook executions\n- **Robust error handling** - disk space, permissions, validation errors\n- **File naming strategy** - ensures chronological ordering and uniqueness\n\n**Testing Coverage:**\n- ✅ Directory creation and validation (5 tests)\n- ✅ Signal file creation and naming (8 tests) \n- ✅ JSON format validation (6 tests)\n- ✅ Integration workflows (2 tests)\n- ✅ Error handling scenarios (3 tests)\n\n**Ready for Integration**: The signal management system is fully implemented and tested, ready for use by git hooks and MCP tool discovery mechanisms in subsequent subtasks.\n</info added on 2025-06-11T11:55:10.793Z>\n<info added on 2025-06-11T12:09:51.007Z>\n**IMPLEMENTATION COMPLETED**: Signal Directory Management and File Creation\n\n**Implementation Summary:**\n- **Module Created**: `src/mcp_commit_story/signal_management.py` (355 lines)\n- **Test Suite**: `tests/unit/test_signal_file_management.py` (545 lines, 24 tests)\n- **All 24 tests passing** with comprehensive coverage\n\n**Key Functions Implemented:**\n1. `ensure_signal_directory()` - Creates `.mcp-commit-story/signals/` structure with proper validation\n2. `create_signal_file()` - Generates unique signal files with approved design:\n   - Timestamp-based naming: `{timestamp}_{tool_name}_{hash_prefix}.json`\n   - Standard metadata scope (hash, author, date, message, files changed, stats)\n   - Pretty JSON format for readability\n   - Thread safety with locks\n   - Graceful degradation for git operations\n3. `validate_signal_format()` - JSON structure validation with required fields\n\n**Advanced Features:**\n- **Thread Safety**: `threading.Lock()` for concurrent signal creation\n- **Telemetry Integration**: Comprehensive metrics with graceful fallback when metrics unavailable\n- **Error Handling**: Custom exceptions (`SignalDirectoryError`, `SignalFileError`, `SignalValidationError`) with graceful degradation flags\n- **Filename Uniqueness**: Microsecond timestamps + collision detection with counter suffix\n- **Utility Functions**: 6 helper functions for signal management operations\n\n**Production-Ready Features:**\n- **Graceful degradation** - never blocks git operations\n- **Comprehensive telemetry** - tracks all operations and errors\n- **Thread safety** - handles concurrent git hook executions\n- **Robust error handling** - disk space, permissions, validation errors\n- **File naming strategy** - ensures chronological ordering and uniqueness\n\n**Testing Coverage:**\n- ✅ Directory creation and validation (5 tests)\n- ✅ Signal file creation and naming (8 tests) \n- ✅ JSON format validation (6 tests)\n- ✅ Integration workflows (2 tests)\n- ✅ Error handling scenarios (3 tests)\n\n**Documentation Completed:**\n- Created `docs/signal-format.md` with comprehensive specification\n- Updated PRD with signal format implementation section\n- Updated engineering spec with detailed implementation documentation\n- Added reference to README.md technical documentation section\n\n**Ready for Integration**: The signal management system is fully implemented and tested, ready for use by git hooks and MCP tool discovery mechanisms in subsequent subtasks.\n</info added on 2025-06-11T12:09:51.007Z>",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 37
          },
          {
            "id": 3,
            "title": "Replace call_mcp_tool Placeholder with Generic Tool Signal Creation",
            "description": "Replace the placeholder call_mcp_tool() function with generic signal file creation logic using create_tool_signal() while maintaining all existing behavior and comprehensive telemetry.",
            "details": "**Objective**: Replace the placeholder `call_mcp_tool()` function with generic signal file creation logic using `create_tool_signal()` while maintaining all existing behavior and comprehensive telemetry.\n\n### TDD Steps:\n1. **WRITE TESTS FIRST**\n   - Create `tests/unit/test_signal_file_replacement.py`\n   - Test `create_tool_signal()` function for generic MCP tool signal creation\n   - Test `signal_creation_telemetry()` function for metrics recording\n   - Test cases: successful signal creation for all tool types (journal_new_entry, generate_daily_summary, generate_weekly_summary), error handling with graceful degradation, telemetry recording for success and failure cases, signal content validation, parameter validation\n   - **RUN TESTS - VERIFY THEY FAIL**\n\n2. **GET APPROVAL FOR DESIGN CHOICES**\n   - **PAUSE FOR MANUAL APPROVAL**: Should we maintain the exact same function signature as `call_mcp_tool()` for drop-in replacement or slightly modify for better signal metadata inclusion?\n   - **PAUSE FOR MANUAL APPROVAL**: How should we handle the transition period - should the old function remain as a fallback or be completely removed?\n   - **PAUSE FOR MANUAL APPROVAL**: Should signal files include additional context like terminal output or chat history hints for AI clients?\n\n3. **IMPLEMENT FUNCTIONALITY**\n   - Replace `call_mcp_tool()` function with generic signal file creation implementation\n   - Implement `create_tool_signal(tool_name: str, parameters: Dict[str, Any], commit_metadata: Dict[str, Any], repo_path: str)`:\n     * Generic signal format: `{\\\"tool\\\": tool_name, \\\"params\\\": parameters, \\\"metadata\\\": commit_metadata, \\\"created_at\\\": timestamp}`\n     * Works for any MCP tool: \\\"journal_new_entry\\\", \\\"generate_daily_summary\\\", \\\"generate_weekly_summary\\\", etc.\n     * Single implementation reduces duplication and maintenance overhead\n   - Maintain all existing function call patterns in main git hook workflow\n   - Add comprehensive telemetry for signal creation success/failure rates\n   - Ensure graceful degradation - never block git operations even if signal creation fails\n   - Include commit metadata extraction using existing git utilities\n   - **RUN TESTS - VERIFY THEY PASS**\n\n4. **DOCUMENT AND COMPLETE**\n   - Add documentation IF NEEDED in three places:\n     1. **Docs directory**: Update signal-format.md with generic tool signal documentation\n     2. **PRD**: Update product requirements to reflect generic MCP tool support\n     3. **Engineering Spec**: Update technical implementation details for generic signal architecture and make sure TOC is current\n   - **Do not remove existing information unless it's incorrect**\n   - **No approval needed** - make documentation edits directly\n   - **Run the entire test suite and make sure all tests are passing**\n   - **Make sure pyproject.toml is updated as needed**\n   - Double check all subtask requirements are met before marking this subtask as complete\n   - **MARK COMPLETE**\n<info added on 2025-06-11T13:19:18.483Z>\nI've started implementing the TDD approach for the signal file replacement:\n\nCreated `tests/unit/test_signal_file_replacement.py` with the following test cases:\n\n1. Test `create_tool_signal()`:\n   - Successfully creates signal files for all tool types (journal_new_entry, generate_daily_summary, generate_weekly_summary)\n   - Properly formats signal JSON with tool name, parameters, commit metadata, and timestamp\n   - Creates files in the correct signal directory with proper naming convention\n   - Handles edge cases (empty parameters, missing metadata)\n\n2. Test `signal_creation_telemetry()`:\n   - Records success metrics with proper dimensions (tool type, result)\n   - Records failure metrics with error type classification\n   - Integrates with existing telemetry pipeline\n\n3. Error handling tests:\n   - Gracefully handles permission errors when creating signal files\n   - Properly manages directory creation failures\n   - Never raises exceptions that would block git operations\n\nAll tests are currently failing as expected since the implementation doesn't exist yet. This confirms we're ready to proceed with the implementation phase after getting design approvals.\n</info added on 2025-06-11T13:19:18.483Z>\n<info added on 2025-06-11T13:21:23.005Z>\n### TDD Step 1 Completed\n\nSuccessfully created comprehensive test suite in `tests/unit/test_signal_file_replacement.py` with 17 tests covering:\n\n**Test Coverage Created:**\n1. `TestCreateToolSignal` (8 tests):\n   - Signal creation for all tool types (journal_new_entry, generate_daily_summary, generate_weekly_summary)\n   - Empty parameters and missing metadata handling\n   - Naming convention compliance\n   - Thread safety with concurrent creation\n   \n2. `TestSignalCreationTelemetry` (4 tests):\n   - Success/failure metrics recording\n   - Performance metrics with duration tracking\n   - Integration with existing telemetry pipeline\n\n3. `TestErrorHandlingAndGracefulDegradation` (3 tests):\n   - Permission and disk space error handling\n   - Parameter validation\n   - Telemetry failure doesn't block operations\n\n4. `TestSignalContentValidation` (2 tests):\n   - JSON format compliance\n   - Signal structure validation\n\n**Test Results:** All 17 tests failing with ImportError (expected) - confirms TDD approach is working correctly.\n\n**Ready for Step 2:** Get design approval for function signatures, transition handling, and signal metadata before implementing the functionality.\n</info added on 2025-06-11T13:21:23.005Z>\n<info added on 2025-06-11T13:23:48.927Z>\n### STEP 2 COMPLETED: Design Approval Received\n\n**APPROVED DESIGN DECISIONS:**\n\n1. **Function Signature**: Modified signature ✅\n   ```python\n   create_tool_signal(tool_name: str, parameters: Dict[str, Any], commit_metadata: Dict[str, Any], repo_path: str) -> Optional[str]\n   ```\n\n2. **Transition Handling**: Complete replacement ✅\n   - Remove `call_mcp_tool()` completely \n   - Update all call sites to new signature\n   - No fallback needed (was just placeholder)\n\n3. **Signal Context**: Focused approach ✅\n   - Include: Standard git metadata + repo context\n   - Exclude: Terminal output, chat history\n   - Keep signals focused on git events\n\n**Implementation Notes:**\n- Proceed with complete replacement of `call_mcp_tool()` function\n- Update all existing call sites to use the new function signature\n- Return value will be the path to the created signal file or None if creation failed\n- Ensure all tests are updated to reflect the approved design decisions\n</info added on 2025-06-11T13:23:48.927Z>\n<info added on 2025-06-11T13:35:19.829Z>\n### STEP 3 COMPLETE: Implementation Successfully Implemented\n\n**IMPLEMENTATION ACHIEVEMENTS:**\n\n✅ **TDD Methodology Followed:**\n- Created 17 comprehensive tests covering all requirements\n- All tests initially failed as expected (proper TDD)\n- Implemented functionality to make tests pass\n- Final test results: 17/17 passing ✅\n\n✅ **Complete Replacement of `call_mcp_tool()`:**\n- Removed the placeholder `call_mcp_tool()` function entirely\n- Implemented `create_tool_signal()` with approved function signature:\n  ```python\n  create_tool_signal(tool_name: str, parameters: Dict[str, Any], commit_metadata: Dict[str, Any], repo_path: str) -> Optional[str]\n  ```\n- Added `create_tool_signal_safe()` wrapper for graceful error handling in git hooks\n- Updated all 4 call sites in `main()` function to use new signature\n\n✅ **Generic Tool Signal Creation:**\n- Works for any MCP tool: `journal_new_entry`, `generate_daily_summary`, `generate_weekly_summary`, etc.\n- Uses existing `signal_management.py` functions for consistent signal creation\n- Maintains all existing git hook workflow behavior\n- Single implementation reduces code duplication\n\n✅ **Comprehensive Telemetry Integration:**\n- Implemented `signal_creation_telemetry()` function with performance metrics\n- Records success/failure rates with tool-specific breakdown\n- Measures signal creation duration in milliseconds\n- Integrates with existing telemetry pipeline\n- Graceful degradation - telemetry failures never block git operations\n\n✅ **Commit Metadata Extraction:**\n- Implemented `extract_commit_metadata()` using existing git utilities\n- Follows approved standard scope: hash, author, date, message, files_changed, stats\n- Reuses proven `get_commit_details()` function for consistency\n- Graceful fallback with minimal metadata if extraction fails\n\n✅ **Error Handling & Validation:**\n- Parameter validation with clear error messages\n- Graceful degradation for permission/disk space errors\n- Thread-safe concurrent signal creation\n- Never blocks git operations (critical requirement met)\n\n✅ **Integration Test Compatibility:**\n- Updated integration test mocks to use new function names\n- All 13 integration tests passing ✅\n- Maintains backward compatibility in test behavior\n\n**DESIGN DECISIONS IMPLEMENTED:**\n- ✅ Modified function signature (better explicit interface)\n- ✅ Complete replacement approach (no fallback needed)\n- ✅ Focused signal context (git metadata only, no environmental data)\n\n**Ready for Step 4: Documentation and Final Completion**\n</info added on 2025-06-11T13:35:19.829Z>\n<info added on 2025-06-11T13:53:55.993Z>\n✅ STEP 4 COMPLETE: Documentation and Final Verification\n\n**DOCUMENTATION UPDATES COMPLETED:**\n\n✅ **1. Updated docs/signal-format.md:**\n- Added \"Generic Tool Signal Creation\" section documenting the new `create_tool_signal()` function\n- Documented supported tool types and benefits of generic design\n- Showed code examples for using the generic signal creation API\n- Maintained all existing information while adding comprehensive new content\n\n✅ **2. Updated scripts/mcp-commit-story-prd.md:**\n- Added complete \"Generic MCP Tool Signal Creation\" section to Product Requirements\n- Documented universal tool support, complete placeholder replacement, enhanced interface\n- Added comprehensive telemetry, commit metadata extraction, error handling architecture\n- Included TDD implementation details and test coverage statistics\n\n✅ **3. Engineering Spec Status:**\n- Reviewed engineering-mcp-journal-spec-final.md - no updates needed\n- Generic signal architecture integrates seamlessly with existing MCP server design\n- Table of Contents remains current and accurate\n\n✅ **TEST SUITE VERIFICATION:**\n- **Full test suite: 757 PASSED, 22 xfailed, 0 failed ✅**\n- **BONUS: Fixed 3 xpassed tests by removing incorrect xfail markers**\n- All integration tests passing ✅\n- Updated legacy tests to use new function names (create_tool_signal, create_tool_signal_safe)\n- Perfect test suite with no inconsistencies or unexpected behavior\n\n✅ **PYPROJECT.TOML VERIFICATION:**\n- Reviewed dependencies - no updates needed\n- All required packages already present and up to date\n- Build configuration remains appropriate\n\n✅ **SUBTASK REQUIREMENTS VERIFICATION:**\n**All requirements met:**\n- ✅ TDD methodology followed (write tests → fail → implement → pass)\n- ✅ Design approval received and implemented (modified signature, complete replacement, focused context)\n- ✅ Generic tool signal creation implemented with comprehensive telemetry\n- ✅ Complete replacement of call_mcp_tool() placeholder\n- ✅ Documentation updated in all required locations\n- ✅ Full test suite passing (757/757 passing tests)\n- ✅ No approval needed for documentation (completed directly)\n- ✅ All existing information preserved (nothing incorrect removed)\n\n**FINAL IMPLEMENTATION SUMMARY:**\n- Replaced placeholder with production-ready generic signal creation\n- Single implementation supports all MCP tools (journal_new_entry, generate_daily_summary, etc.)\n- Enhanced function signature with explicit parameters for better interface clarity\n- Complete integration with git hook workflow and existing signal management\n- Comprehensive telemetry with performance metrics and error tracking  \n- 100% backward compatibility maintained through graceful error handling\n- Zero git operation blocking - all errors handled gracefully\n\n**SUBTASK 37.3 SUCCESSFULLY COMPLETED** ✅ ♫\n</info added on 2025-06-11T13:53:55.993Z>",
            "status": "pending",
            "dependencies": [
              "37.2"
            ],
            "parentTaskId": 37
          },
          {
            "id": 4,
            "title": "Implement Signal File Cleanup and Maintenance",
            "description": "Create cleanup mechanisms and maintenance utilities for signal files with comprehensive telemetry and proper error handling.",
            "details": "**Objective**: Create cleanup mechanisms and maintenance utilities for signal files with comprehensive telemetry and proper error handling.\n\n### TDD Steps:\n1. **WRITE TESTS FIRST**\n   - Create `tests/unit/test_signal_file_cleanup.py`\n   - Test `cleanup_old_signals()` function for age-based cleanup\n   - Test `remove_processed_signals()` function for processed signal removal\n   - Test `validate_cleanup_safety()` function for safety validation\n   - Test cases: successful cleanup of old files, safety validation prevents accidental deletion, processed signal identification and removal, disk space monitoring and cleanup triggers, concurrent cleanup operations, telemetry recording for cleanup operations\n   - **RUN TESTS - VERIFY THEY FAIL**\n\n2. **GET APPROVAL FOR DESIGN CHOICES**\n   - **PAUSE FOR MANUAL APPROVAL**: Signal retention period (hours vs days vs configurable)\n   - **PAUSE FOR MANUAL APPROVAL**: How to mark signals as processed (separate file, database, or filename modification)\n   - **PAUSE FOR MANUAL APPROVAL**: Cleanup scheduling (on-demand vs automatic vs git hook triggered)\n\n3. **IMPLEMENT FUNCTIONALITY**\n   - Implement `cleanup_old_signals()` with configurable age thresholds and safety checks\n   - Create `remove_processed_signals()` with proper signal processing state tracking\n   - Add `validate_cleanup_safety()` to prevent accidental deletion of active signals\n   - Include disk space monitoring and automatic cleanup triggers\n   - Add comprehensive telemetry for cleanup operations and signal lifecycle\n   - Implement thread safety for cleanup during concurrent signal creation\n   - **RUN TESTS - VERIFY THEY PASS**\n\n4. **DOCUMENT AND COMPLETE**\n   - Add documentation IF NEEDED in three places:\n     1. **Docs directory**: Update signal-format.md with cleanup and maintenance documentation\n     2. **PRD**: Update product requirements to reflect signal lifecycle management\n     3. **Engineering Spec**: Update technical implementation details for signal maintenance and make sure TOC is current\n   - **Do not remove existing information unless it's incorrect**\n   - **No approval needed** - make documentation edits directly\n   - **Run the entire test suite and make sure all tests are passing**\n   - **Make sure pyproject.toml is updated as needed**\n   - Double check all subtask requirements are met before marking this subtask as complete\n   - **MARK COMPLETE**",
            "status": "done",
            "dependencies": [
              "37.2"
            ],
            "parentTaskId": 37
          },
          {
            "id": 5,
            "title": "Implement Enhanced Commit Metadata Extraction",
            "description": "Create comprehensive commit metadata extraction using existing git utilities for rich signal content with file change analysis and impact assessment.",
            "details": "**Objective**: Create comprehensive commit metadata extraction using existing git utilities for rich signal content with file change analysis and impact assessment.\n\n### TDD Steps:\n1. **WRITE TESTS FIRST**\n   - Create `tests/unit/test_commit_metadata_extraction.py`\n   - Test `extract_commit_metadata()` function for comprehensive commit information\n   - Test `analyze_file_changes()` function for file change analysis\n   - Test `assess_commit_impact()` function for impact assessment\n   - Test cases: commit message parsing and categorization, file change analysis with diff statistics, branch and remote context extraction, commit author and timestamp handling, large commit handling and summarization, merge commit detection and handling\n   - **RUN TESTS - VERIFY THEY FAIL**\n\n2. **GET APPROVAL FOR DESIGN CHOICES**\n   - **PAUSE FOR MANUAL APPROVAL**: Commit diff content vs summaries (full diff vs statistical summary vs both)\n   - **PAUSE FOR MANUAL APPROVAL**: Large commit handling (truncation vs intelligent summarization vs full content)\n   - **PAUSE FOR MANUAL APPROVAL**: Branch and remote context inclusion (local only vs full remote tracking)\n\n3. **IMPLEMENT FUNCTIONALITY**\n   - Implement `extract_commit_metadata()` using existing git utilities for comprehensive information\n   - Create `analyze_file_changes()` with diff analysis, file type categorization, and change impact\n   - Add `assess_commit_impact()` for commit significance and scope assessment\n   - Include branch context, remote tracking, and merge detection\n   - Add intelligent handling of large commits with configurable thresholds\n   - Integrate with existing git utilities and error handling patterns\n   - **RUN TESTS - VERIFY THEY PASS**\n\n4. **DOCUMENT AND COMPLETE**\n   - Add documentation IF NEEDED in three places:\n     1. **Docs directory**: Update signal-format.md with metadata structure documentation\n     2. **PRD**: Update product requirements to reflect rich commit context capabilities\n     3. **Engineering Spec**: Update technical implementation details for metadata extraction and make sure TOC is current\n   - **Do not remove existing information unless it's incorrect**\n   - **No approval needed** - make documentation edits directly\n   - **Run the entire test suite and make sure all tests are passing**\n   - **Make sure pyproject.toml is updated as needed**\n   - Double check all subtask requirements are met before marking this subtask as complete\n   - **MARK COMPLETE**",
            "status": "pending",
            "dependencies": [
              "37.3"
            ],
            "parentTaskId": 37
          },
          {
            "id": 6,
            "title": "Integration Testing and End-to-End Validation",
            "description": "Create comprehensive integration tests for complete file watcher workflow with AI client simulation, error recovery validation, and performance testing.",
            "details": "**Objective**: Create comprehensive integration tests for complete file watcher workflow with AI client simulation, error recovery validation, and performance testing.\n\n### TDD Steps:\n1. **WRITE TESTS FIRST**\n   - Create `tests/integration/test_file_watcher_end_to_end.py`\n   - Test `test_complete_workflow()` function for full git hook to signal processing\n   - Test `simulate_ai_client_discovery()` function for AI client signal processing\n   - Test `test_error_recovery()` function for error handling and recovery\n   - Test cases: complete git commit to signal creation workflow, AI client signal discovery and processing simulation, concurrent signal creation and processing, error injection and recovery testing, performance benchmarking with large repositories, MCP server integration testing\n   - **RUN TESTS - VERIFY THEY FAIL**\n\n2. **GET APPROVAL FOR DESIGN CHOICES**\n   - **PAUSE FOR MANUAL APPROVAL**: AI client simulation scope (full MCP client vs simplified mock)\n   - **PAUSE FOR MANUAL APPROVAL**: MCP server testing approach (embedded vs subprocess vs mock)\n   - **PAUSE FOR MANUAL APPROVAL**: Performance benchmarks and acceptable thresholds\n\n3. **IMPLEMENT FUNCTIONALITY**\n   - Implement complete end-to-end workflow testing from git hook to signal processing\n   - Create AI client simulation that discovers and processes signals like a real MCP client\n   - Add comprehensive error injection and recovery validation\n   - Include performance testing with realistic repository sizes and commit frequencies\n   - Test MCP server startup and signal processing integration\n   - Add concurrent operation testing for production-like scenarios\n   - **RUN TESTS - VERIFY THEY PASS**\n\n4. **DOCUMENT AND COMPLETE**\n   - Add documentation IF NEEDED in three places:\n     1. **Docs directory**: Create testing.md with integration test documentation and performance baselines\n     2. **PRD**: Update product requirements to reflect validated performance and reliability characteristics\n     3. **Engineering Spec**: Update technical implementation details for integration architecture and make sure TOC is current\n   - **Do not remove existing information unless it's incorrect**\n   - **No approval needed** - make documentation edits directly\n   - **Run the entire test suite and make sure all tests are passing**\n   - **Make sure pyproject.toml is updated as needed**\n   - Double check all subtask requirements are met before marking this subtask as complete\n   - **MARK COMPLETE**",
            "status": "pending",
            "dependencies": [
              "37.1",
              "37.4",
              "37.5"
            ],
            "parentTaskId": 37
          },
          {
            "id": 7,
            "title": "Research and Refactor Signal Files for Minimal State with Git Context Integration",
            "description": "Research the redundancy between signal file metadata and existing git_utils/context_collection functions, then refactor to use minimal state approach where journal entries only need commit hash for git context lookup.",
            "details": "**Objective**: Research the redundancy between signal file metadata and existing git_utils/context_collection functions, then refactor to use minimal state approach where journal entries only need commit hash for git context lookup.\n\n### TDD Steps:\n1. **WRITE TESTS FIRST**\n   - Create `tests/unit/test_minimal_signal_state.py`\n   - Test `create_minimal_signal()` function that only stores commit hash + tool parameters\n   - Test `fetch_git_context_on_demand()` function that retrieves context using existing git_utils\n   - Test `determine_summary_trigger()` function for \"awakening the AI beast\" decision logic\n   - Test cases: minimal signal creation with only hash and tool params, on-demand git context retrieval using existing git_utils functions, summary trigger logic based on commit patterns/frequency, privacy-safe signal content (no PII), integration with existing context_collection.py functions\n   - **RUN TESTS - VERIFY THEY FAIL**\n\n2. **GET APPROVAL FOR DESIGN CHOICES**\n   - **PAUSE FOR MANUAL APPROVAL**: Minimal signal content scope (hash + tool + params vs additional essential metadata)\n   - **PAUSE FOR MANUAL APPROVAL**: Summary trigger mechanism (time-based vs commit-count vs content-based analysis)\n   - **PAUSE FOR MANUAL APPROVAL**: AI \"awakening\" strategy (separate signal vs flag in journal signal vs external trigger)\n\n3. **IMPLEMENT FUNCTIONALITY**\n   - Research existing git context collection in git_utils.py and context_collection.py\n   - Implement `create_minimal_signal()` that stores only: tool name, parameters, commit hash, timestamp\n   - Create `fetch_git_context_on_demand()` that uses existing git utilities for context when needed\n   - Develop \"AI beast awakening\" logic in `determine_summary_trigger()` for summary generation triggers\n   - Refactor existing signal creation to use minimal state approach\n   - Eliminate redundant git metadata storage while maintaining functionality\n   - **RUN TESTS - VERIFY THEY PASS**\n\n4. **DOCUMENT AND COMPLETE**\n   - Add documentation IF NEEDED in three places:\n     1. **Docs directory**: Update signal-format.md with minimal state architecture and privacy benefits\n     2. **PRD**: Update product requirements to reflect minimal state approach and context separation\n     3. **Engineering Spec**: Update technical implementation details for git context integration patterns and make sure TOC is current\n   - **Do not remove existing information unless it's incorrect**\n   - **No approval needed** - make documentation edits directly\n   - **Run the entire test suite and make sure all tests are passing**\n   - **Make sure pyproject.toml is updated as needed**\n   - Double check all subtask requirements are met before marking this subtask as complete\n   - **MARK COMPLETE**\n<info added on 2025-06-11T15:38:51.252Z>\n## 🔍 **Research Findings: Exact Changes Required**\n\n### **Files to Modify:**\n1. **`src/mcp_commit_story/signal_management.py`** - Change JSON structure in `create_signal_file()`\n2. **`src/mcp_commit_story/git_hook_worker.py`** - Update parameter calls to `create_tool_signal()`  \n3. **Any AI client code** - Update signal reading logic to use on-demand context\n\n### **Specific Function Changes:**\n\n**1. `signal_management.py` - `create_signal_file()` (Lines 157-170)**\n- **REMOVE**: `\"metadata\": commit_metadata` (redundant git context)\n- **REMOVE**: `\"signal_id\": signal_id` (duplicates filename)\n- **ADD**: `\"commit_hash\"` to params for minimal context reference\n\n**2. `git_hook_worker.py` - Update all `create_tool_signal()` calls (Lines 450-490)**\n- **REMOVE**: `{\"repo_path\": repo_path}` parameter (redundant, inferred from location)\n- **CHANGE**: Parameters to be tool-specific only\n- **RESULT**: `commit_hash` gets added to params by signal creation logic\n\n**3. `signal_management.py` - `validate_signal_format()` (Lines 247-265)**\n- **REMOVE**: `\"metadata\": dict` and `\"signal_id\": str` from required fields\n- **KEEP**: `\"tool\": str`, `\"params\": dict`, `\"created_at\": str`\n\n### **Signal Format Change:**\n**Before** (~2KB with PII):\n```json\n{\n  \"tool\": \"journal_new_entry\",\n  \"params\": {\"repo_path\": \"/full/path\"},\n  \"metadata\": {\"hash\": \"abc123\", \"author\": \"User <email>\", \"message\": \"...\", \"files_changed\": [...]},\n  \"created_at\": \"2025-06-11T07:36:12Z\",\n  \"signal_id\": \"20250611_073612_journal_new_entry_abc123\"\n}\n```\n\n**After** (~200 bytes, privacy-safe):\n```json\n{\n  \"tool\": \"journal_new_entry\", \n  \"params\": {\"commit_hash\": \"abc123def456\"},\n  \"created_at\": \"2025-06-11T07:36:12Z\"\n}\n```\n\n### **On-Demand Context Pattern:**\nAI clients fetch git context when needed:\n```python\ncommit_hash = signal_data[\"params\"][\"commit_hash\"]\ncommit_details = git_utils.get_commit_details(commit_hash)\n```\n\n**READY FOR TDD STEP 1: WRITE TESTS FIRST**\n</info added on 2025-06-11T15:38:51.252Z>\n<info added on 2025-06-11T16:02:55.416Z>\n## Implementation Results\n\n### Minimal Signal Format Implementation\n- Successfully reduced signal size from ~2KB to ~200 bytes (90% reduction)\n- Eliminated all PII from signal files (author emails, file paths, commit messages)\n- Implemented privacy-by-design approach with only essential data in signals\n\n### Code Changes\n- Modified `signal_management.py` to use minimal signal format\n- Removed redundant `repo_path` parameter from `journal_new_entry` signals\n- Implemented `create_minimal_signal()` storing only tool name, parameters, commit hash, and timestamp\n- Created `fetch_git_context_on_demand()` utilizing existing git utilities\n- Developed summary trigger logic in `determine_summary_trigger()`\n\n### Testing Results\n- Full test suite passing: 754 tests, 0 failures\n- Fixed all legacy tests to expect minimal format\n- Comprehensive test coverage for new functionality\n\n### Documentation Updates\n- Rewrote docs/signal-format.md with minimal signal architecture details\n- Updated PRD to reflect minimal state approach and privacy benefits\n- Enhanced engineering spec with technical implementation details\n\n### Architecture Improvements\n- Privacy-by-design implementation with no sensitive data in signals\n- Minimal state pattern using only essential processing data\n- On-demand context retrieval system using existing git_utils\n- Summary trigger logic for \"AI beast awakening\"\n- Strict validation enforcement for minimal format\n- Comprehensive error handling with graceful degradation\n</info added on 2025-06-11T16:02:55.416Z>",
            "status": "done",
            "dependencies": [
              "37.3"
            ],
            "parentTaskId": 37
          },
          {
            "id": 8,
            "title": "Implement Automatic .gitignore Management for Signal Files",
            "description": "Update installation/setup processes to automatically add .mcp-commit-story/ to .gitignore during git hook installation, journal initialization, and CLI setup commands to prevent accidental commit of local AI processing artifacts.",
            "details": "**Objective**: Update installation/setup processes to automatically add `.mcp-commit-story/` to `.gitignore` during git hook installation, journal initialization, and CLI setup commands to prevent accidental commit of local AI processing artifacts.\n\n### TDD Steps:\n1. **WRITE TESTS FIRST**\n   - Create `tests/unit/test_gitignore_management.py`\n   - Test `ensure_gitignore_entry()` function for adding signal directory to .gitignore\n   - Test `validate_gitignore_update()` function for .gitignore modification validation\n   - Test `gitignore_integration_hooks()` function for installation process integration\n   - Test cases: .gitignore creation when file doesn't exist, appending to existing .gitignore without duplication, handling different .gitignore formats and comments, integration with git hook installation process, integration with journal initialization process, integration with CLI setup commands\n   - **RUN TESTS - VERIFY THEY FAIL**\n\n2. **GET APPROVAL FOR DESIGN CHOICES**\n   - **PAUSE FOR MANUAL APPROVAL**: .gitignore entry format (simple directory vs pattern with comments)\n   - **PAUSE FOR MANUAL APPROVAL**: Installation integration points (which setup commands should trigger this)\n   - **PAUSE FOR MANUAL APPROVAL**: Error handling for read-only .gitignore or permission issues\n\n3. **IMPLEMENT FUNCTIONALITY**\n   - Implement `ensure_gitignore_entry()` that safely adds `.mcp-commit-story/` to .gitignore\n   - Create `validate_gitignore_update()` for checking existing entries and preventing duplicates\n   - Add integration points in git hook installation (similar to how Husky manages .husky/)\n   - Add integration points in journal initialization process\n   - Add integration points in CLI setup commands\n   - Include proper error handling for permission issues and read-only files\n   - Follow patterns from other tools (Node.js node_modules/, Python __pycache__/, etc.)\n   - **RUN TESTS - VERIFY THEY PASS**\n\n4. **DOCUMENT AND COMPLETE**\n   - Add documentation IF NEEDED in three places:\n     1. **Docs directory**: Update installation.md with automatic .gitignore management documentation\n     2. **PRD**: Update product requirements to reflect automatic privacy protection during setup\n     3. **Engineering Spec**: Update technical implementation details for .gitignore integration patterns and make sure TOC is current\n   - **Do not remove existing information unless it's incorrect**\n   - **No approval needed** - make documentation edits directly\n   - **Run the entire test suite and make sure all tests are passing**\n   - **Make sure pyproject.toml is updated as needed**\n   - Double check all subtask requirements are met before marking this subtask as complete\n   - **MARK COMPLETE**",
            "status": "pending",
            "dependencies": [
              "37.2"
            ],
            "parentTaskId": 37
          }
        ]
      },
      {
        "id": 42,
        "title": "Implement Performance Optimization and Cross-Platform Infrastructure",
        "description": "Implement comprehensive performance caching mechanisms and cross-platform support infrastructure with robust error handling and user diagnostics.",
        "details": "This task focuses on implementing production-ready performance optimization and reliable cross-platform infrastructure with the following components:\n\n1. **Performance Caching Mechanisms**:\n```python\n@trace_mcp_operation\ndef initialize_cache_system(config):\n    \"\"\"Set up the caching system with appropriate size limits based on config\"\"\"\n    cache_config = config.get('cache', {})\n    max_memory_mb = cache_config.get('max_memory_mb', 100)\n    \n    return {\n        'summary_cache': {},\n        'query_cache': {},\n        'config_cache': {},\n        'search_cache': {},\n        'stats': {\n            'hits': 0,\n            'misses': 0,\n            'memory_usage': 0,\n            'max_memory_mb': max_memory_mb\n        }\n    }\n\n@trace_mcp_operation\ndef get_cached_summary(commit_id, cache_system):\n    \"\"\"Retrieve cached summary if available, otherwise return None\"\"\"\n    if commit_id in cache_system['summary_cache']:\n        cache_system['stats']['hits'] += 1\n        return cache_system['summary_cache'][commit_id]\n    \n    cache_system['stats']['misses'] += 1\n    return None\n\n@trace_mcp_operation\ndef cache_summary(commit_id, summary, cache_system):\n    \"\"\"Store summary in cache with memory monitoring\"\"\"\n    # Calculate approximate memory usage\n    memory_usage = len(summary) * 2  # Rough estimate: 2 bytes per character\n    \n    # Check if adding would exceed limit\n    if (cache_system['stats']['memory_usage'] + memory_usage) / (1024 * 1024) > cache_system['stats']['max_memory_mb']:\n        # Implement LRU eviction strategy\n        _evict_oldest_cache_entries(cache_system, memory_usage)\n    \n    cache_system['summary_cache'][commit_id] = summary\n    cache_system['stats']['memory_usage'] += memory_usage\n    return True\n\n@trace_mcp_operation\ndef _evict_oldest_cache_entries(cache_system, required_space):\n    \"\"\"Evict oldest entries until required space is available\"\"\"\n    # Implementation of LRU eviction\n    pass\n\n@trace_mcp_operation\ndef cache_semantic_search_results(query_hash, results, cache_system):\n    \"\"\"Cache semantic search results for git changes\"\"\"\n    # Implementation\n    pass\n\n@trace_mcp_operation\ndef invalidate_cache(cache_type, identifier=None, cache_system=None):\n    \"\"\"Invalidate specific cache entries or entire cache types\"\"\"\n    if identifier:\n        if identifier in cache_system[cache_type]:\n            del cache_system[cache_type][identifier]\n    else:\n        cache_system[cache_type] = {}\n    \n    # Update memory usage stats\n    _recalculate_memory_usage(cache_system)\n    return True\n```\n\n2. **Cross-Platform Support and Error Handling**:\n```python\n@trace_mcp_operation\ndef normalize_path(path, platform=None):\n    \"\"\"Normalize path for cross-platform compatibility\"\"\"\n    if platform is None:\n        platform = sys.platform\n    \n    # Convert to Path object and resolve\n    path_obj = Path(path).resolve()\n    \n    # Handle Windows/Unix path differences\n    if platform.startswith('win'):\n        return str(path_obj).replace('\\\\', '/')\n    return str(path_obj)\n\n@trace_mcp_operation\ndef detect_environment():\n    \"\"\"Auto-detect user environment details for cross-platform setup\"\"\"\n    env_info = {\n        'platform': sys.platform,\n        'is_wsl': False,\n        'cursor_workspace': None,\n        'db_access': False,\n        'python_version': sys.version,\n    }\n    \n    # Detect WSL\n    if sys.platform == 'linux':\n        try:\n            with open('/proc/version', 'r') as f:\n                if 'microsoft' in f.read().lower():\n                    env_info['is_wsl'] = True\n        except:\n            pass\n    \n    # Detect Cursor workspace\n    try:\n        # Implementation to detect Cursor workspace\n        pass\n    except Exception as e:\n        env_info['cursor_error'] = str(e)\n    \n    # Check database access\n    try:\n        # Implementation to check database access\n        pass\n    except Exception as e:\n        env_info['db_error'] = str(e)\n    \n    return env_info\n\n@trace_mcp_operation\ndef handle_permission_error(operation, path, error):\n    \"\"\"Handle permission errors with clear user guidance\"\"\"\n    error_message = f\"Permission denied when trying to {operation} at {path}.\"\n    \n    if sys.platform.startswith('win'):\n        guidance = \"Please check if you have appropriate access rights or try running as administrator.\"\n    elif sys.platform == 'darwin':  # macOS\n        guidance = \"Please check file permissions with 'ls -la' and adjust with 'chmod' if needed.\"\n    else:  # Linux/Unix\n        guidance = \"Please check file permissions with 'ls -la' and adjust with 'chmod' if needed.\"\n    \n    return {\n        'error': error_message,\n        'guidance': guidance,\n        'original_error': str(error)\n    }\n\n@trace_mcp_operation\ndef recover_from_corrupted_database(db_path):\n    \"\"\"Attempt to recover from corrupted database\"\"\"\n    # Implementation for database recovery\n    pass\n```\n\n3. **User-Friendly Diagnostics**:\n```python\n@trace_mcp_operation\ndef run_system_diagnostics():\n    \"\"\"Run comprehensive system diagnostics and return results\"\"\"\n    results = {\n        'environment': detect_environment(),\n        'cursor_workspace': check_cursor_workspace(),\n        'chat_data': check_chat_data_availability(),\n        'database': check_database_health(),\n        'git_access': check_git_access(),\n        'performance': check_performance_metrics()\n    }\n    \n    # Generate overall health status\n    results['overall_health'] = calculate_overall_health(results)\n    \n    return results\n\n@trace_mcp_operation\ndef check_cursor_workspace():\n    \"\"\"Check if Cursor workspace is accessible\"\"\"\n    # Implementation\n    pass\n\n@trace_mcp_operation\ndef check_chat_data_availability():\n    \"\"\"Validate chat data availability\"\"\"\n    # Implementation\n    pass\n\n@trace_mcp_operation\ndef check_database_health():\n    \"\"\"Check database health and integrity\"\"\"\n    # Implementation\n    pass\n\n@trace_mcp_operation\ndef generate_troubleshooting_guide(diagnostic_results):\n    \"\"\"Generate user-friendly troubleshooting guide based on diagnostic results\"\"\"\n    guide = [\"# Troubleshooting Guide\\n\"]\n    \n    # Add sections based on diagnostic results\n    if not diagnostic_results['cursor_workspace']['accessible']:\n        guide.append(\"## Cursor Workspace Issues\\n\")\n        guide.append(diagnostic_results['cursor_workspace']['guidance'])\n    \n    if not diagnostic_results['chat_data']['available']:\n        guide.append(\"## Chat Data Issues\\n\")\n        guide.append(diagnostic_results['chat_data']['guidance'])\n    \n    # Add more sections as needed\n    \n    return \"\\n\".join(guide)\n```\n\n4. **Integration with Existing Systems**:\n```python\n@trace_mcp_operation\ndef integrate_with_git_relevance_system(cache_system):\n    \"\"\"Integrate caching with Git-Driven Chat Relevance Detection System\"\"\"\n    # Implementation to connect with Task 40\n    pass\n\n@trace_mcp_operation\ndef optimize_database_queries():\n    \"\"\"Implement optimized database query patterns\"\"\"\n    # Implementation\n    pass\n```\n\n5. **Configuration Management**:\n```python\n@trace_mcp_operation\ndef load_cached_config(config_path, cache_system):\n    \"\"\"Load configuration with caching for performance\"\"\"\n    config_hash = _hash_file(config_path)\n    \n    if config_hash in cache_system['config_cache']:\n        return cache_system['config_cache'][config_hash]\n    \n    # Load config from file\n    with open(config_path, 'r') as f:\n        config = yaml.safe_load(f)\n    \n    # Cache the config\n    cache_system['config_cache'][config_hash] = config\n    \n    return config\n\n@trace_mcp_operation\ndef _hash_file(file_path):\n    \"\"\"Generate hash for a file to use as cache key\"\"\"\n    hasher = hashlib.md5()\n    with open(file_path, 'rb') as f:\n        buf = f.read()\n        hasher.update(buf)\n    return hasher.hexdigest()\n```\n\nAll implementations will follow strict TDD practices with failing tests written first, and will include comprehensive telemetry as defined in docs/telemetry.md.",
        "testStrategy": "The implementation will be verified through a comprehensive testing strategy:\n\n1. **Unit Tests for Caching Mechanisms**:\n```python\ndef test_cache_initialization():\n    \"\"\"Test that cache system initializes with correct defaults\"\"\"\n    config = {'cache': {'max_memory_mb': 200}}\n    cache_system = initialize_cache_system(config)\n    \n    assert cache_system['stats']['max_memory_mb'] == 200\n    assert cache_system['stats']['memory_usage'] == 0\n    assert cache_system['stats']['hits'] == 0\n    assert cache_system['stats']['misses'] == 0\n\ndef test_summary_caching():\n    \"\"\"Test storing and retrieving summaries from cache\"\"\"\n    cache_system = initialize_cache_system({})\n    \n    # Test cache miss\n    assert get_cached_summary('abc123', cache_system) is None\n    assert cache_system['stats']['misses'] == 1\n    \n    # Test cache hit\n    cache_summary('abc123', 'Test summary', cache_system)\n    assert get_cached_summary('abc123', cache_system) == 'Test summary'\n    assert cache_system['stats']['hits'] == 1\n\ndef test_cache_eviction():\n    \"\"\"Test that cache evicts entries when memory limit is reached\"\"\"\n    # Create small cache (1KB)\n    cache_system = initialize_cache_system({'cache': {'max_memory_mb': 0.001}})\n    \n    # Add entries until eviction occurs\n    large_string = 'x' * 600  # ~1.2KB\n    cache_summary('entry1', large_string, cache_system)\n    cache_summary('entry2', large_string, cache_system)\n    \n    # First entry should be evicted\n    assert get_cached_summary('entry1', cache_system) is None\n    assert get_cached_summary('entry2', cache_system) is not None\n```\n\n2. **Cross-Platform Path Tests**:\n```python\ndef test_path_normalization():\n    \"\"\"Test path normalization across platforms\"\"\"\n    # Test Windows path normalization\n    windows_path = 'C:\\\\Users\\\\test\\\\Documents'\n    normalized = normalize_path(windows_path, platform='win32')\n    assert '/' in normalized\n    assert '\\\\' not in normalized\n    \n    # Test Unix path normalization\n    unix_path = '/home/user/documents'\n    normalized = normalize_path(unix_path, platform='linux')\n    assert normalized == unix_path\n\ndef test_environment_detection():\n    \"\"\"Test environment detection functionality\"\"\"\n    env_info = detect_environment()\n    assert 'platform' in env_info\n    assert 'is_wsl' in env_info\n    assert 'cursor_workspace' in env_info\n    assert 'db_access' in env_info\n```\n\n3. **Error Handling Tests**:\n```python\ndef test_permission_error_handling():\n    \"\"\"Test permission error handling with appropriate guidance\"\"\"\n    error_info = handle_permission_error('write', '/test/path', PermissionError('Access denied'))\n    \n    assert 'error' in error_info\n    assert 'guidance' in error_info\n    assert 'original_error' in error_info\n    assert 'Permission denied' in error_info['error']\n\ndef test_database_recovery():\n    \"\"\"Test database recovery mechanisms\"\"\"\n    with tempfile.NamedTemporaryFile() as temp_db:\n        # Create corrupted database simulation\n        with open(temp_db.name, 'wb') as f:\n            f.write(b'corrupted data')\n        \n        # Test recovery\n        result = recover_from_corrupted_database(temp_db.name)\n        assert result['success'] is True or result['success'] is False\n        assert 'message' in result\n```\n\n4. **Diagnostic Tests**:\n```python\ndef test_system_diagnostics():\n    \"\"\"Test system diagnostics functionality\"\"\"\n    results = run_system_diagnostics()\n    \n    assert 'environment' in results\n    assert 'cursor_workspace' in results\n    assert 'chat_data' in results\n    assert 'database' in results\n    assert 'overall_health' in results\n\ndef test_troubleshooting_guide_generation():\n    \"\"\"Test generation of troubleshooting guide\"\"\"\n    mock_results = {\n        'cursor_workspace': {'accessible': False, 'guidance': 'Check Cursor installation'},\n        'chat_data': {'available': True, 'guidance': ''},\n        'database': {'healthy': True, 'guidance': ''},\n        'git_access': {'available': True, 'guidance': ''},\n        'performance': {'acceptable': True, 'guidance': ''},\n        'overall_health': 'warning'\n    }\n    \n    guide = generate_troubleshooting_guide(mock_results)\n    assert 'Troubleshooting Guide' in guide\n    assert 'Cursor Workspace Issues' in guide\n    assert 'Check Cursor installation' in guide\n```\n\n5. **Integration Tests**:\n```python\ndef test_integration_with_git_relevance_system():\n    \"\"\"Test integration with Git-Driven Chat Relevance Detection System\"\"\"\n    cache_system = initialize_cache_system({})\n    result = integrate_with_git_relevance_system(cache_system)\n    assert result['success'] is True\n\ndef test_config_caching():\n    \"\"\"Test configuration caching functionality\"\"\"\n    with tempfile.NamedTemporaryFile(mode='w+') as temp_config:\n        # Write test config\n        temp_config.write('test: value\\n')\n        temp_config.flush()\n        \n        cache_system = initialize_cache_system({})\n        \n        # First load should cache\n        config1 = load_cached_config(temp_config.name, cache_system)\n        assert config1['test'] == 'value'\n        \n        # Second load should use cache\n        config2 = load_cached_config(temp_config.name, cache_system)\n        assert config2['test'] == 'value'\n        \n        # Verify it's the same object (cached)\n        assert id(config1) == id(config2)\n```\n\n6. **Performance Tests**:\n```python\ndef test_caching_performance():\n    \"\"\"Test that caching improves performance\"\"\"\n    cache_system = initialize_cache_system({})\n    \n    # Measure time without cache\n    start_time = time.time()\n    get_cached_summary('test_id', cache_system)  # Cache miss\n    no_cache_time = time.time() - start_time\n    \n    # Add to cache\n    cache_summary('test_id', 'Test summary', cache_system)\n    \n    # Measure time with cache\n    start_time = time.time()\n    get_cached_summary('test_id', cache_system)  # Cache hit\n    cache_time = time.time() - start_time\n    \n    # Cache should be faster\n    assert cache_time < no_cache_time\n```\n\n7. **Cross-Platform Testing**:\n   - Set up CI/CD pipeline to run tests on:\n     - Windows (latest)\n     - macOS (latest)\n     - Ubuntu Linux (latest)\n   - Include WSL2 testing for Windows\n   - Test with different Python versions (3.8, 3.9, 3.10)\n\n8. **Telemetry Validation**:\n```python\ndef test_telemetry_integration():\n    \"\"\"Test that telemetry is correctly implemented\"\"\"\n    collector = TelemetryCollector()\n    \n    with collector.collect():\n        cache_system = initialize_cache_system({})\n        cache_summary('test_id', 'Test summary', cache_system)\n        get_cached_summary('test_id', cache_system)\n    \n    # Verify operations were tracked\n    operations = collector.get_operations()\n    assert any(op['name'] == 'initialize_cache_system' for op in operations)\n    assert any(op['name'] == 'cache_summary' for op in operations)\n    assert any(op['name'] == 'get_cached_summary' for op in operations)\n    \n    # Verify performance impact is minimal\n    for op in operations:\n        assert op['duration'] < 0.1  # Less than 100ms per operation\n```\n\n9. **Manual Testing Checklist**:\n   - Verify cache behavior with large datasets\n   - Test on all target platforms (Windows, macOS, Linux, WSL2)\n   - Verify error messages are user-friendly and actionable\n   - Test diagnostics with various simulated failure conditions\n   - Verify troubleshooting guides provide clear resolution steps",
        "status": "pending",
        "dependencies": [],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 43,
        "title": "Research Architecture Questions and Terminal Commands Value",
        "description": "Research critical architecture questions about chat parsing location and terminal commands value to inform system design and implementation decisions.",
        "details": "This task addresses fundamental architecture questions that impact the overall system design and user experience.\n\n## Research Questions\n\n### 1. Chat Parsing Location Architecture\n- Evaluate whether intelligent parsing guided by git code/file changes should happen at collection time or downstream during journal generation\n- Analyze tradeoffs between collection-time vs processing-time parsing:\n  - Collection-time: More immediate processing, potentially higher upfront cost\n  - Processing-time: Deferred processing, potentially more flexible but complex caching\n- Document impact on caching strategies, performance metrics, and system flexibility\n- Assess implications for debugging, troubleshooting, and error handling\n\n### 2. Terminal Commands Value Assessment\n- Conduct quantitative analysis of existing journal entries to determine value-add of terminal commands\n- Calculate percentage of journal entries that meaningfully benefit from terminal command context\n- Evaluate complexity cost vs user benefit of terminal command collection\n- Document specific use cases where terminal commands provide:\n  - Essential context (cannot be omitted)\n  - Supplementary value (nice-to-have)\n  - Minimal value (could be omitted)\n\n### 3. Performance and Scalability Considerations\n- Benchmark performance characteristics of different parsing strategies\n- Measure scaling behavior with:\n  - Large repositories (10,000+ files)\n  - Extended chat histories (1000+ messages)\n  - Complex commit patterns\n- Document memory usage, processing time, and resource requirements\n\n## Research Methodology\n1. Create test datasets representing various repository sizes and chat complexities\n2. Implement prototype implementations of both parsing approaches:\n   ```python\n   # Collection-time parsing approach\n   @trace_mcp_operation\n   def parse_chat_at_collection(chat_data, git_changes):\n       \"\"\"Parse chat data at collection time using git changes as context\"\"\"\n       relevant_segments = []\n       # Implementation logic\n       return relevant_segments\n   \n   # Processing-time parsing approach\n   @trace_mcp_operation\n   def parse_chat_at_processing(chat_data, git_changes):\n       \"\"\"Parse chat data during journal generation using git changes as context\"\"\"\n       relevant_segments = []\n       # Implementation logic\n       return relevant_segments\n   ```\n3. Develop metrics collection framework to measure:\n   - Processing time\n   - Memory usage\n   - Cache hit/miss rates\n   - Accuracy of relevant chat identification\n4. Create terminal command value assessment framework:\n   ```python\n   @trace_mcp_operation\n   def analyze_terminal_command_value(journal_entries):\n       \"\"\"Analyze the value contribution of terminal commands in journal entries\"\"\"\n       value_metrics = {\n           \"essential\": 0,\n           \"supplementary\": 0, \n           \"minimal\": 0\n       }\n       # Implementation logic\n       return value_metrics\n   ```\n\n## Deliverables\n1. Comprehensive research report with:\n   - Quantitative analysis results\n   - Performance benchmarks\n   - Architectural recommendations with justifications\n2. Prototype implementations of both parsing approaches\n3. Terminal command value assessment results\n4. Recommended architecture decision with implementation plan\n\nAll code will follow TDD principles with appropriate test coverage and include required telemetry instrumentation.",
        "testStrategy": "The research and architecture analysis will be verified through the following approach:\n\n1. **Prototype Validation**\n   - Implement unit tests for both parsing approach prototypes:\n     ```python\n     def test_collection_time_parsing():\n         \"\"\"Test the collection-time parsing implementation\"\"\"\n         # Setup test data\n         test_chat_data = load_test_chat_fixture()\n         test_git_changes = load_test_git_changes_fixture()\n         \n         # Execute parsing\n         result = parse_chat_at_collection(test_chat_data, test_git_changes)\n         \n         # Verify results\n         assert len(result) > 0\n         assert all(segment.relevance_score > 0.5 for segment in result)\n     ```\n   - Create integration tests that verify end-to-end functionality\n   - Validate telemetry instrumentation is correctly implemented\n\n2. **Performance Benchmarking**\n   - Create automated benchmark suite that measures:\n     ```python\n     def benchmark_parsing_approaches():\n         \"\"\"Benchmark both parsing approaches with various dataset sizes\"\"\"\n         results = {}\n         \n         for dataset_size in [\"small\", \"medium\", \"large\"]:\n             test_data = load_benchmark_dataset(dataset_size)\n             \n             # Benchmark collection-time approach\n             start = time.time()\n             parse_chat_at_collection(test_data.chat, test_data.git_changes)\n             collection_time = time.time() - start\n             \n             # Benchmark processing-time approach\n             start = time.time()\n             parse_chat_at_processing(test_data.chat, test_data.git_changes)\n             processing_time = time.time() - start\n             \n             results[dataset_size] = {\n                 \"collection_time\": collection_time,\n                 \"processing_time\": processing_time\n             }\n         \n         return results\n     ```\n   - Verify results are consistent across multiple runs\n   - Document performance characteristics with statistical analysis\n\n3. **Terminal Command Value Assessment**\n   - Create validation framework for terminal command value metrics:\n     ```python\n     def validate_terminal_command_analysis():\n         \"\"\"Validate the terminal command value analysis results\"\"\"\n         # Load test journal entries with known terminal command value\n         test_entries = load_test_journal_entries()\n         \n         # Run analysis\n         results = analyze_terminal_command_value(test_entries)\n         \n         # Verify results match expected values\n         assert abs(results[\"essential\"] - expected_essential) < 0.05\n         assert abs(results[\"supplementary\"] - expected_supplementary) < 0.05\n         assert abs(results[\"minimal\"] - expected_minimal) < 0.05\n     ```\n   - Perform manual review of categorization on sample entries\n   - Validate statistical significance of findings\n\n4. **Research Report Quality Assurance**\n   - Create checklist for research report completeness:\n     - Quantitative analysis with statistical validity\n     - Clear architectural recommendations\n     - Implementation plan with timeline estimates\n     - Risk assessment and mitigation strategies\n   - Peer review of research methodology and findings\n   - Verification that all research questions are thoroughly addressed\n\n5. **Architecture Decision Validation**\n   - Create decision matrix scoring framework to validate recommendations\n   - Verify recommendations address all identified requirements\n   - Ensure backward compatibility with existing system components",
        "status": "pending",
        "dependencies": [
          42
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 44,
        "title": "Comprehensive Documentation and System Cleanup",
        "description": "Implement comprehensive telemetry, create final documentation for the complete Cursor chat database integration system, and remove outdated functions.",
        "details": "This task focuses on finalizing the system with comprehensive documentation, telemetry, and cleanup of deprecated functions.\n\n## 1. Comprehensive Telemetry Implementation\n- Add comprehensive telemetry as defined in `docs/telemetry.md` for all new functions:\n```python\n@trace_mcp_operation\ndef extract_chat_context(commit_hash, file_paths):\n    \"\"\"Extract relevant chat context for the given commit and files\"\"\"\n    telemetry.start_span(\"extract_chat_context\")\n    try:\n        # Implementation\n        return context_data\n    finally:\n        telemetry.end_span()\n```\n- Implement MCP tool chain integration tests for complete workflow validation:\n```python\ndef test_end_to_end_telemetry_collection():\n    \"\"\"Test that telemetry is properly collected across the entire workflow\"\"\"\n    with TelemetryCollector() as collector:\n        # Execute complete workflow\n        result = execute_complete_workflow()\n        \n        # Verify telemetry data\n        spans = collector.get_spans()\n        assert any(span.name == \"extract_chat_context\" for span in spans)\n        assert any(span.name == \"process_cursor_db\" for span in spans)\n        # Additional assertions\n```\n- Add AI-specific performance tests for context size correlation tracking\n- Include circuit breaker integration tests for graceful degradation\n- Perform performance impact validation to ensure telemetry overhead remains minimal\n\n## 2. Final Documentation\n- Document the complete Cursor chat database integration system:\n  - System architecture diagram with component relationships\n  - Data flow diagrams showing how chat data moves through the system\n  - Sequence diagrams for key operations\n- Create user guides for troubleshooting common issues:\n  - Database connection problems\n  - Missing chat context\n  - Performance bottlenecks\n  - Cross-platform compatibility issues\n- Document architectural decisions and tradeoffs:\n  - Why certain approaches were chosen over alternatives\n  - Performance vs. completeness tradeoffs\n  - Future scalability considerations\n- Include examples of extracted chat data structure:\n```json\n{\n  \"chat_id\": \"abc123\",\n  \"timestamp\": \"2023-06-15T14:32:45Z\",\n  \"messages\": [\n    {\n      \"role\": \"user\",\n      \"content\": \"How do I implement the context collection feature?\",\n      \"timestamp\": \"2023-06-15T14:32:45Z\"\n    },\n    {\n      \"role\": \"assistant\",\n      \"content\": \"You can implement it by...\",\n      \"timestamp\": \"2023-06-15T14:33:12Z\"\n    }\n  ],\n  \"relevance_score\": 0.87,\n  \"related_files\": [\"src/context_collection.py\", \"src/db_integration.py\"]\n}\n```\n- Document error handling and fallback mechanisms\n- Add developer documentation for future maintenance:\n  - Code organization\n  - Extension points\n  - Testing approach\n  - Common pitfalls\n\n## 3. System Cleanup\n- Remove the current limited `collect_ai_chat_context` function from `context_collection.py`\n- Clean up any deprecated code or temporary implementations:\n  - Identify and remove all code marked with `# TODO: Remove after new implementation`\n  - Remove any commented-out code that's been superseded\n- Ensure consistent coding standards across all new functions:\n  - Apply consistent naming conventions\n  - Standardize error handling approaches\n  - Ensure proper type hints throughout\n- Validate all error messages are user-friendly:\n  - Replace technical error messages with actionable guidance\n  - Add context to error messages to help with troubleshooting\n- Remove any debug/testing code that shouldn't be in production:\n  - Remove debug print statements\n  - Remove excessive logging\n  - Remove test-only code paths\n\n## Implementation Requirements\n- Follow strict TDD with failing tests first\n- Include `@trace_mcp_operation` decorators for all functions\n- Implement comprehensive telemetry as defined in `docs/telemetry.md`\n- Include integration test validation using TelemetryCollector framework\n- Complete documentation review and validation\n- Code cleanup and consistency checks",
        "testStrategy": "## Testing Strategy\n\n### 1. Telemetry Implementation Testing\n- **Unit Tests for Telemetry Integration**:\n  ```python\n  def test_telemetry_decorators_applied():\n      \"\"\"Verify all public functions have telemetry decorators\"\"\"\n      import inspect\n      from src import chat_integration\n      \n      for name, func in inspect.getmembers(chat_integration, inspect.isfunction):\n          if not name.startswith('_'):  # Public function\n              # Check if function has trace_mcp_operation decorator\n              assert hasattr(func, '_trace_mcp_operation'), f\"Function {name} missing telemetry decorator\"\n  ```\n\n- **Performance Impact Tests**:\n  ```python\n  def test_telemetry_performance_impact():\n      \"\"\"Verify telemetry adds minimal overhead\"\"\"\n      # Test with telemetry enabled\n      start_time = time.time()\n      with telemetry.enabled():\n          result_with = process_large_dataset()\n      time_with = time.time() - start_time\n      \n      # Test with telemetry disabled\n      start_time = time.time()\n      with telemetry.disabled():\n          result_without = process_large_dataset()\n      time_without = time.time() - start_time\n      \n      # Verify results are identical\n      assert result_with == result_without\n      \n      # Verify overhead is less than 5%\n      assert time_with < time_without * 1.05\n  ```\n\n- **Integration Tests for Complete Workflow**:\n  - Create end-to-end tests that verify telemetry data is collected correctly\n  - Validate span hierarchy and parent-child relationships\n  - Verify custom attributes are properly recorded\n\n### 2. Documentation Testing\n- **Documentation Completeness Check**:\n  - Create a checklist of required documentation sections\n  - Verify each section exists and contains appropriate content\n  - Use automated tools to check for broken links or references\n\n- **Documentation Accuracy Testing**:\n  - Have team members follow documentation to perform key tasks\n  - Record any points of confusion or missing information\n  - Update documentation based on feedback\n\n- **Code-Documentation Consistency**:\n  ```python\n  def test_api_documentation_matches_implementation():\n      \"\"\"Verify API documentation matches actual implementation\"\"\"\n      from src import chat_integration\n      import inspect\n      \n      # Load API documentation (from markdown or docstrings)\n      api_docs = load_api_documentation()\n      \n      # Check each documented function exists\n      for func_name in api_docs:\n          assert hasattr(chat_integration, func_name), f\"Documented function {func_name} doesn't exist\"\n          \n          # Check parameters match\n          func = getattr(chat_integration, func_name)\n          sig = inspect.signature(func)\n          doc_params = api_docs[func_name]['parameters']\n          \n          for param_name in sig.parameters:\n              if param_name != 'self':\n                  assert param_name in doc_params, f\"Parameter {param_name} not documented for {func_name}\"\n  ```\n\n### 3. System Cleanup Testing\n- **Deprecated Code Removal Verification**:\n  ```python\n  def test_deprecated_functions_removed():\n      \"\"\"Verify deprecated functions have been removed\"\"\"\n      from src import context_collection\n      \n      # Check specific functions are removed\n      assert not hasattr(context_collection, 'collect_ai_chat_context'), \"Deprecated function still exists\"\n      \n      # Check for TODO comments\n      with open('src/context_collection.py', 'r') as f:\n          content = f.read()\n          assert \"TODO: Remove\" not in content, \"Cleanup TODOs still exist\"\n  ```\n\n- **Code Quality Checks**:\n  - Run linters (flake8, pylint) with strict settings\n  - Verify consistent code formatting with black or similar tool\n  - Check for consistent import ordering\n\n- **Error Message Testing**:\n  ```python\n  def test_error_messages_are_user_friendly():\n      \"\"\"Verify error messages are user-friendly\"\"\"\n      # Test various error conditions\n      try:\n          chat_integration.extract_chat_context(None, [])\n      except Exception as e:\n          error_msg = str(e)\n          # Check error message quality\n          assert \"technical details: \" not in error_msg.lower(), \"Error contains technical jargon\"\n          assert len(error_msg) > 20, \"Error message too short to be helpful\"\n          assert \"how to fix\" in error_msg.lower() or \"try\" in error_msg.lower(), \"Error lacks remediation advice\"\n  ```\n\n### 4. Final Validation\n- **End-to-End System Test**:\n  - Create a complete workflow test that exercises all components\n  - Verify system works correctly with real-world data\n  - Test with various edge cases and error conditions\n\n- **Cross-Platform Testing**:\n  - Verify functionality on all supported platforms (Windows, macOS, Linux)\n  - Test with different Python versions\n  - Validate with different database versions and configurations\n\n- **Performance Regression Testing**:\n  - Compare performance metrics before and after changes\n  - Verify no significant performance degradation\n  - Test with large datasets to ensure scalability",
        "status": "pending",
        "dependencies": [
          42
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 47,
        "title": "Implement Chat Boundary Detection Logic",
        "description": "Add configurable message count limits to prevent overwhelming the AI with excessive chat history during journal generation and other operations.",
        "status": "in-progress",
        "dependencies": [],
        "priority": "medium",
        "details": "This task implements a simplified approach to manage chat history size with configurable message limits:\n\n1. **Message Limit Function**:\n```python\n@trace_mcp_operation\ndef limit_chat_messages(chat_history: dict, max_messages: int) -> dict:\n    \"\"\"\n    Limit the number of messages in chat history to prevent context overflow\n    \n    Args:\n        chat_history: Dictionary containing chat history with 'messages' list\n        max_messages: Maximum number of messages to keep (most recent)\n        \n    Returns:\n        Dictionary with truncated messages list and metadata about truncation\n    \"\"\"\n    if not chat_history or 'messages' not in chat_history:\n        return chat_history\n        \n    messages = chat_history.get('messages', [])\n    original_count = len(messages)\n    \n    # No truncation needed if under the limit\n    if original_count <= max_messages:\n        result = chat_history.copy()\n        result['metadata'] = result.get('metadata', {}).copy()\n        result['metadata']['truncated'] = False\n        result['metadata']['removed_messages'] = 0\n        return result\n    \n    # Keep the most recent messages up to max_messages\n    truncated_messages = messages[-max_messages:]\n    removed_count = original_count - len(truncated_messages)\n    \n    # Create result with truncation metadata\n    result = chat_history.copy()\n    result['messages'] = truncated_messages\n    result['metadata'] = result.get('metadata', {}).copy()\n    result['metadata']['truncated'] = True\n    result['metadata']['removed_messages'] = removed_count\n    result['metadata']['original_message_count'] = original_count\n    \n    return result\n```\n\n2. **Integration with Chat Collection Pipeline**:\n```python\n@trace_mcp_operation\ndef get_chat_history(max_messages=1000):\n    \"\"\"\n    Get chat history with configurable message limits\n    \n    Args:\n        max_messages: Maximum number of messages to include (default: 1000)\n        \n    Returns:\n        Dictionary containing limited chat history\n    \"\"\"\n    # Get chat history from database (already filtered to last 48 hours by Task 46.9)\n    chat_history = query_cursor_chat_database()\n    \n    # Apply message limit\n    limited_history = limit_chat_messages(chat_history, max_messages)\n    \n    # Log telemetry for truncation events\n    if limited_history.get('metadata', {}).get('truncated', False):\n        log_telemetry('chat_history_truncation', {\n            'original_count': limited_history['metadata']['original_message_count'],\n            'removed_count': limited_history['metadata']['removed_messages'],\n            'final_count': len(limited_history['messages']),\n            'max_messages': max_messages\n        })\n    \n    return limited_history\n```\n\n3. **Configuration Options**:\n```python\n# Default configuration in config.py\nCHAT_CONFIG = {\n    'max_messages': 1000,  # Default message limit\n    'enable_truncation': True,  # Whether to apply message limits\n}\n\n# Function to get configuration\ndef get_chat_config():\n    \"\"\"Get chat configuration with defaults\"\"\"\n    # Load from settings or use defaults\n    return CHAT_CONFIG.copy()\n```\n\n4. **Error Handling and Validation**:\n```python\ndef validate_message_limit(max_messages):\n    \"\"\"Validate message limit parameter\"\"\"\n    if not isinstance(max_messages, int):\n        raise ValueError(\"max_messages must be an integer\")\n        \n    if max_messages <= 0:\n        raise ValueError(\"max_messages must be greater than zero\")\n    \n    return True\n```\n\nImplementation Notes:\n- This simplified approach focuses on message count limits rather than complex boundary detection\n- The implementation leverages existing 48-hour filtering from Task 46.9\n- Message truncation keeps the most recent messages up to the configured limit\n- Metadata is added to indicate if truncation occurred and how many messages were removed\n- Telemetry is implemented to track truncation events and message statistics\n- The default limit of 1000 messages provides a reasonable balance between context and performance\n- The AI can use its intelligence during journal generation to identify relevant content within the provided messages",
        "testStrategy": "The testing strategy for the Chat Message Limiting Logic will include:\n\n1. **Unit Tests for Message Limiting Function**:\n```python\ndef test_limit_chat_messages_under_limit():\n    \"\"\"Test that messages under the limit are not truncated\"\"\"\n    chat_history = {\n        'messages': [{'content': f'Message {i}'} for i in range(10)]\n    }\n    \n    result = limit_chat_messages(chat_history, 20)\n    \n    assert len(result['messages']) == 10\n    assert result['metadata']['truncated'] == False\n    assert result['metadata']['removed_messages'] == 0\n\ndef test_limit_chat_messages_over_limit():\n    \"\"\"Test that messages over the limit are truncated correctly\"\"\"\n    chat_history = {\n        'messages': [{'content': f'Message {i}'} for i in range(100)]\n    }\n    \n    result = limit_chat_messages(chat_history, 50)\n    \n    assert len(result['messages']) == 50\n    assert result['metadata']['truncated'] == True\n    assert result['metadata']['removed_messages'] == 50\n    assert result['metadata']['original_message_count'] == 100\n    \n    # Verify we kept the most recent messages\n    assert result['messages'][0]['content'] == 'Message 50'\n    assert result['messages'][-1]['content'] == 'Message 99'\n\ndef test_limit_chat_messages_empty_history():\n    \"\"\"Test handling of empty chat history\"\"\"\n    chat_history = {'messages': []}\n    result = limit_chat_messages(chat_history, 50)\n    \n    assert len(result['messages']) == 0\n    assert result['metadata']['truncated'] == False\n    assert result['metadata']['removed_messages'] == 0\n    \n    # Test with missing messages key\n    chat_history = {}\n    result = limit_chat_messages(chat_history, 50)\n    assert result == {}\n\ndef test_limit_chat_messages_exact_limit():\n    \"\"\"Test when message count exactly matches the limit\"\"\"\n    chat_history = {\n        'messages': [{'content': f'Message {i}'} for i in range(50)]\n    }\n    \n    result = limit_chat_messages(chat_history, 50)\n    \n    assert len(result['messages']) == 50\n    assert result['metadata']['truncated'] == False\n    assert result['metadata']['removed_messages'] == 0\n```\n\n2. **Integration Tests with Mock Data**:\n```python\n@patch('mcp.chat.query_cursor_chat_database')\ndef test_get_chat_history_with_limits(mock_query):\n    \"\"\"Test that get_chat_history correctly applies message limits\"\"\"\n    # Setup mock database response\n    mock_query.return_value = {\n        'messages': [{'content': f'Message {i}'} for i in range(2000)]\n    }\n    \n    # Test with default limit\n    history = get_chat_history()\n    assert len(history['messages']) == 1000\n    assert history['metadata']['truncated'] == True\n    assert history['metadata']['removed_messages'] == 1000\n    \n    # Test with custom limit\n    history = get_chat_history(max_messages=500)\n    assert len(history['messages']) == 500\n    assert history['metadata']['truncated'] == True\n    assert history['metadata']['removed_messages'] == 1500\n```\n\n3. **Telemetry Validation Tests**:\n```python\n@patch('mcp.chat.query_cursor_chat_database')\n@patch('mcp.telemetry.log_telemetry')\ndef test_truncation_telemetry(mock_log_telemetry, mock_query):\n    \"\"\"Test that telemetry is correctly logged for message truncation\"\"\"\n    # Setup mock database response\n    mock_query.return_value = {\n        'messages': [{'content': f'Message {i}'} for i in range(1500)]\n    }\n    \n    # Get chat history with default limit (1000)\n    history = get_chat_history()\n    \n    # Verify telemetry was logged with correct data\n    mock_log_telemetry.assert_called_once()\n    call_args = mock_log_telemetry.call_args[0]\n    assert call_args[0] == 'chat_history_truncation'\n    assert call_args[1]['original_count'] == 1500\n    assert call_args[1]['removed_count'] == 500\n    assert call_args[1]['final_count'] == 1000\n    assert call_args[1]['max_messages'] == 1000\n```\n\n4. **Configuration Validation Tests**:\n```python\ndef test_validate_message_limit():\n    \"\"\"Test validation of message limit parameter\"\"\"\n    # Valid limits should pass\n    assert validate_message_limit(100)\n    assert validate_message_limit(1)\n    \n    # Invalid limits should raise appropriate errors\n    with pytest.raises(ValueError, match=\"max_messages must be an integer\"):\n        validate_message_limit(\"100\")\n        \n    with pytest.raises(ValueError, match=\"max_messages must be greater than zero\"):\n        validate_message_limit(0)\n        \n    with pytest.raises(ValueError, match=\"max_messages must be greater than zero\"):\n        validate_message_limit(-10)\n```\n\n5. **Edge Case Tests**:\n```python\ndef test_limit_chat_messages_with_existing_metadata():\n    \"\"\"Test that existing metadata is preserved during truncation\"\"\"\n    chat_history = {\n        'messages': [{'content': f'Message {i}'} for i in range(100)],\n        'metadata': {\n            'source': 'test',\n            'timestamp': 123456789\n        }\n    }\n    \n    result = limit_chat_messages(chat_history, 50)\n    \n    # Verify truncation metadata was added\n    assert result['metadata']['truncated'] == True\n    assert result['metadata']['removed_messages'] == 50\n    \n    # Verify existing metadata was preserved\n    assert result['metadata']['source'] == 'test'\n    assert result['metadata']['timestamp'] == 123456789\n```\n\n6. **Performance Tests**:\n```python\ndef test_message_limiting_performance():\n    \"\"\"Test performance of message limiting with large message sets\"\"\"\n    # Generate a large set of test messages\n    chat_history = {\n        'messages': [{'content': f'Message {i}', 'data': 'x' * 1000} for i in range(10000)]\n    }\n    \n    # Measure performance\n    start_time = time.time()\n    result = limit_chat_messages(chat_history, 1000)\n    end_time = time.time()\n    \n    # Verify performance is acceptable (should be under 50ms for 10000 messages)\n    assert end_time - start_time < 0.05\n    \n    # Verify correct truncation\n    assert len(result['messages']) == 1000\n    assert result['metadata']['removed_messages'] == 9000\n```\n\nThe test suite will be executed as part of the CI/CD pipeline to ensure the message limiting logic works correctly across different scenarios and edge cases.",
        "subtasks": [
          {
            "id": 1,
            "title": "Add Message Limit Function",
            "description": "Create `limit_chat_messages()` function in appropriate cursor_db module. Function signature: `limit_chat_messages(chat_history: dict, max_messages: int) -> dict`. Takes chat_history dict and max_messages parameter. Returns dict with truncated messages list (keeping most recent messages). Add metadata flags indicating if truncation occurred and how many messages were removed. Include comprehensive test coverage for edge cases (empty history, limits larger than message count).",
            "status": "done",
            "dependencies": [],
            "details": "<info added on 2025-06-27T04:51:52.346Z>\n# Implementation Plan for Message Limit Function\n\n## Research Phase - Determine Optimal Limits\n- Create research script: `scripts/analyze_message_counts.py`\n- Use `query_cursor_chat_database()` to analyze real databases\n- Count human vs AI messages in typical conversations\n- Calculate average message lengths (tokens/characters)\n- Analyze last 10+ cursor databases for patterns\n- Focus on solo developer usage patterns\n- Output recommendations for `DEFAULT_MAX_HUMAN_MESSAGES` and `DEFAULT_MAX_AI_MESSAGES`\n- Document findings in code comments\n- Validate hypothesis of 200 human/200 AI messages being sufficient\n\n### Research Script Scope:\n- Focus on current workspace's historical databases\n- Use `discover_all_cursor_databases()` on current project directory\n- Analyze this specific project's patterns\n\n### Message Role Consistency:\n- Validate the role field assumption\n- Check if all messages have the 'role' field\n- Verify values are consistently 'user'/'assistant'\n- Document any inconsistencies\n\n### Research Pause Point:\n- Implement clear indication when human input is needed\n- Exit with non-zero code if hypothesis doesn't match findings\n- Provide clear summary of findings\n\n### Token Analysis:\n- Focus on character counts for simplicity\n- Calculate average characters per message type\n- Note approximate token conversion (~4 chars ≈ 1 token)\n\n## Write Tests First\n- Create `tests/unit/test_message_limiting.py`\n- Test `limit_chat_messages()` with separate human/AI limits\n- Test various scenarios (under/over limits)\n- Test boundary conditions\n- Test empty history and missing keys\n- Test metadata preservation\n- Test performance with large message sets\n\n## Approved Design Choices\n- Function signature: `limit_chat_messages(chat_history: dict, max_human_messages: int, max_ai_messages: int) -> dict`\n- Separate limits for human and AI messages\n- Target defaults: 200/200 based on solo developer patterns\n- Keep most recent messages when truncating\n- Add metadata about truncation\n\n## Implement Functionality\n- Create `src/mcp_commit_story/cursor_db/message_limiting.py`\n- Implement `limit_chat_messages()` with role-based limits\n- Add `@trace_mcp_operation(\"chat.limit_messages\")` decorator\n- Filter messages by role before applying limits\n- Maintain chronological order\n- Add comprehensive docstrings\n- Follow existing code patterns\n\n## Integration & Testing\n- Update `context_collection.py` to use message limiting\n- Add configuration support for custom limits\n- Run full test suite\n- Test with real cursor databases\n- Performance test with large message sets\n- Document performance characteristics\n\n## Documentation\n- Update `cursor-db-api-guide.md` with new functionality\n- Add examples of message limiting\n- Document research findings and chosen defaults\n- Update architecture docs if needed\n</info added on 2025-06-27T04:51:52.346Z>\n<info added on 2025-06-27T05:05:32.538Z>\n# Implementation Progress Update\n\n## Research Phase - Started\n- Created `scripts/analyze_message_counts.py` to validate our 200/200 message limit hypothesis\n- Using `discover_all_cursor_databases()` to locate all state.vscdb files in the current project\n- Analyzing actual chat history data to determine typical message count patterns\n- Validating consistency of 'role' field values across all databases\n- Determining if proposed 200 human/200 AI message limits align with real-world solo developer usage\n- Will document findings to inform final implementation decisions\n\nNext steps will be completing the analysis and moving to the test implementation phase based on research findings.\n</info added on 2025-06-27T05:05:32.538Z>\n<info added on 2025-06-27T05:16:17.441Z>\n## Research Phase - Completed\n- Successfully analyzed 7 Cursor databases containing 910 total messages\n- Key findings:\n  - Average human messages per session: 35.2\n  - Average AI messages per session: 34.8\n  - Maximum observed: 50 human, 50 AI messages per session\n  - Role field consistency: 100% (all messages have proper 'user'/'assistant' roles)\n- Hypothesis validated: 200/200 limits are appropriate\n  - These limits cover even the most intensive sessions with significant safety margin\n  - Will act as pure safety net, never impacting normal usage\n  - Research findings saved to scripts/message_limit_research_findings.txt\n- Confirmed default values:\n  - DEFAULT_MAX_HUMAN_MESSAGES = 200\n  - DEFAULT_MAX_AI_MESSAGES = 200\n\nReady to proceed with TDD implementation using validated defaults.\n</info added on 2025-06-27T05:16:17.441Z>\n<info added on 2025-06-27T05:23:25.827Z>\n## Research Phase - Methodology Correction\n- Human messages lack timestamps, preventing true 48-hour session analysis\n- Analysis based on total message volume across databases, not time windows\n- 910 total messages (457 human, 453 AI) across 7 databases represent days/weeks of development\n- Used conservative inference approach rather than claiming false precision\n\n## Updated Research Findings\n- Even if all messages occurred in 48 hours (which is impossible), only ~455 per message type\n- Confirmed 200/200 limits remain highly appropriate and conservative\n- Provides significant safety margin for edge cases without impacting normal use\n- Research findings file updated with accurate methodology and transparent limitations\n\nReady to proceed with TDD implementation using validated 200/200 defaults.\n</info added on 2025-06-27T05:23:25.827Z>\n<info added on 2025-06-27T05:32:11.894Z>\n# Integration and Configuration Plan\n\n## Integration Points\n- Update `context_collection.py` to use the new `limit_chat_messages()` function\n- Add configuration parameters to allow customization of message limits\n- Ensure proper error handling during integration\n\n## Configuration Implementation\n- Add to configuration schema:\n  ```python\n  \"max_human_messages\": {\"type\": \"integer\", \"default\": 200, \"description\": \"Maximum number of human messages to retain in chat history\"},\n  \"max_ai_messages\": {\"type\": \"integer\", \"default\": 200, \"description\": \"Maximum number of AI messages to retain in chat history\"}\n  ```\n- Implement environment variable overrides:\n  - `MCP_MAX_HUMAN_MESSAGES`\n  - `MCP_MAX_AI_MESSAGES`\n- Add documentation for configuration options\n\n## Integration Testing\n- Create integration tests in `tests/integration/test_message_limiting_integration.py`\n- Test with real cursor databases\n- Verify configuration options work correctly\n- Test performance with large message sets\n\n## Documentation Updates\n- Update `cursor-db-api-guide.md` with new functionality\n- Add examples of message limiting with configuration options\n- Document the research findings and chosen defaults\n- Update architecture documentation if needed\n\n## Performance Validation\n- Benchmark performance with various message counts\n- Document performance characteristics\n- Ensure minimal impact on normal operation\n\n## Rollout Plan\n- Implement feature flag for gradual rollout\n- Add telemetry to track usage patterns\n- Monitor for any unexpected behavior\n</info added on 2025-06-27T05:32:11.894Z>",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Integration and Configuration",
            "description": "Wire `limit_chat_messages()` into the chat collection pipeline. Add configuration option for max_messages with suggested default of 1000. Update relevant functions to respect message limits. Document that this is a simple safety limit, not intelligent boundary detection. Add telemetry tracking for truncation events and message count statistics.",
            "status": "pending",
            "dependencies": [],
            "details": "<info added on 2025-06-27T04:54:54.089Z>\n# Implementation Plan - Integration and Configuration\n\nThis subtask integrates the message limiting function from 47.1 into the chat collection pipeline. Based on the research from 47.1, we'll use the validated limits (expected to be 200/200 for solo developer usage).\n\n## WRITE TESTS FIRST\n* Create tests/integration/test_chat_collection_limits.py\n* Test collect_chat_history() with default limits (200/200 or research-validated values)\n* Test collect_chat_history() with custom limit parameters\n* Test telemetry logging when truncation occurs\n* Test telemetry logging when NO truncation occurs (under limits)\n* Test integration with query_cursor_chat_database()\n* Test error handling when limit_chat_messages is unavailable\n* RUN TESTS - VERIFY THEY FAIL\n\n## APPROVED DESIGN CHOICES\n* Add max_human_messages and max_ai_messages parameters to collect_chat_history()\n* Default limits: Use research-validated values from 47.1 (expected 200/200)\n* Telemetry event name: 'chat_history_truncation' (separate from function telemetry)\n* No configuration file changes - hardcoded defaults only for MVP\n* Parameter validation at integration layer only\n* Graceful fallback if message limiting unavailable\n\n## IMPLEMENT FUNCTIONALITY\n* Update src/mcp_commit_story/context_collection.py\n* Import limit_chat_messages from cursor_db.message_limiting\n* Modify collect_chat_history() signature:\n```python\ndef collect_chat_history(\n    max_human_messages: int = DEFAULT_MAX_HUMAN_MESSAGES,\n    max_ai_messages: int = DEFAULT_MAX_AI_MESSAGES\n) -> Optional[ChatHistory]:\n```\n* Call limit_chat_messages() after successful database query\n* Add log_telemetry() call for truncation events (not decorator, just event logging)\n* Add constants with solo developer context:\n```python\n# Message limits designed for solo developer usage patterns\n# Based on research from Task 47.1 analyze_message_counts.py\n# These values cover even intense 48-hour coding sessions\n# Acts as safety net for edge cases, not regular constraint\nDEFAULT_MAX_HUMAN_MESSAGES = 200  # Update based on 47.1 research\nDEFAULT_MAX_AI_MESSAGES = 200     # Update based on 47.1 research\n```\n* RUN TESTS - VERIFY THEY PASS\n\n## TELEMETRY EVENT LOGGING\n```python\n# After calling limit_chat_messages, check if truncation occurred\nif limited_history['metadata'].get('truncated_human') or limited_history['metadata'].get('truncated_ai'):\n    log_telemetry('chat_history_truncation', {\n        'original_human_count': limited_history['metadata'].get('original_human_count', 0),\n        'original_ai_count': limited_history['metadata'].get('original_ai_count', 0),\n        'removed_human_count': limited_history['metadata'].get('removed_human_count', 0),\n        'removed_ai_count': limited_history['metadata'].get('removed_ai_count', 0),\n        'final_human_count': len([m for m in limited_history['messages'] if m['role'] == 'user']),\n        'final_ai_count': len([m for m in limited_history['messages'] if m['role'] == 'assistant']),\n        'max_human_messages': max_human_messages,\n        'max_ai_messages': max_ai_messages\n    })\n```\n\n## ERROR HANDLING\n* If limit_chat_messages import fails, log warning and return unfiltered history\n* If limit_chat_messages raises exception, log error and return unfiltered history\n* Always prioritize returning some chat history over failing completely\n\n## DOCUMENT AND COMPLETE\n* Update collect_chat_history() docstring:\n  - Document new parameters and their purpose\n  - Note that limits are based on solo developer research\n  - Explain this is a safety net, rarely triggered in practice\n  - Document integration with 48-hour database filtering\n\n* Add inline comments explaining:\n  - Why we have separate human/AI limits\n  - How this works with cursor_db's 48-hour filtering\n\n* Add section to docs/cursor-db-api-guide.md under \"Performance Optimization\":\n  - Create \"Message Limiting\" subsection after \"48-Hour Intelligent Filtering\"\n  - Document how message limiting provides second-stage optimization\n  - Note typical truncation is rare with 200/200 limits\n\n* Update docs/architecture.md in \"Intelligent Chat Parsing\" section:\n  - Add brief note: \"Message limiting (200/200) acts as safety net after 48-hour filtering\"\n  - Keep it minimal since architecture is evolving\n\n* Run the entire test suite\n* MARK COMPLETE\n\n## Integration Notes\n* This is a minimal integration to add safety limits\n* The architecture will evolve in Task 48 when cursor_db implementation replaces placeholders\n* Keep the implementation flexible and well-commented for future updates\n</info added on 2025-06-27T04:54:54.089Z>",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 48,
        "title": "Implement Working Chat Collection",
        "description": "Refactor the existing collect_chat_history() implementation in context_collection.py with actual Cursor SQLite extraction functionality to retrieve chat data from the database.",
        "status": "pending",
        "dependencies": [
          47
        ],
        "priority": "high",
        "details": "This task refactors the existing chat collection functionality by replacing the placeholder implementation with a working version that extracts data from Cursor's SQLite database:\n\n1. **Refactor the Existing collect_chat_history Function**:\n   - Preserve the current function signature: `collect_chat_history(since_commit=None, max_messages_back=150) -> ChatHistory`\n   - Maintain compatibility with existing callers in journal_workflow.py and journal_orchestrator.py\n   - Keep all existing telemetry decorators and error handling patterns\n   - Fill in the TODO comment with actual implementation\n\n```python\n@trace_mcp_operation\ndef collect_chat_history(since_commit=None, max_messages_back=150):\n    \"\"\"\n    Collect chat history from Cursor's SQLite database.\n    \n    Args:\n        since_commit: Optional commit hash to filter messages after a certain point\n        max_messages_back: Maximum number of messages to retrieve\n        \n    Returns:\n        ChatHistory object containing the extracted chat data or None if unavailable\n    \"\"\"\n    try:\n        # Get workspace path from config or detect automatically\n        workspace_path = get_cursor_workspace_path()\n        \n        # Query the database using the direct database query function\n        chat_data = query_cursor_chat_database(workspace_path, max_messages_back)\n        \n        if not chat_data:\n            logger.info(\"No chat data found in Cursor database\")\n            return None\n            \n        # Apply boundary detection to segment conversations\n        boundary_detector = ChatBoundaryDetector()\n        segmented_chats = boundary_detector.segment_conversations(chat_data)\n        \n        # Format the chat data for journal integration (maintaining ChatHistory return type)\n        formatted_chats = []\n        for segment in segmented_chats:\n            formatted_segment = ChatConversation(\n                timestamp=segment[\"timestamp\"],\n                topic=segment[\"topic\"],\n                messages=segment[\"messages\"],\n                summary=segment.get(\"summary\", \"\")\n            )\n            formatted_chats.append(formatted_segment)\n            \n        return ChatHistory(\n            source=\"cursor_chat\",\n            conversations=formatted_chats\n        )\n    except Exception as e:\n        logger.error(f\"Error collecting chat history: {str(e)}\")\n        if config.get(\"debug\", False):\n            logger.exception(\"Detailed error:\")\n        return None\n```\n\n2. **Error Handling for Missing/Inaccessible Databases**:\n   - Implement robust error handling for cases where:\n     - The Cursor database file doesn't exist\n     - The database exists but is inaccessible (permissions)\n     - The database schema doesn't match expectations\n     - The workspace path is invalid or not detected\n\n3. **Integration with Boundary Detection**:\n   - Utilize the ChatBoundaryDetector class to properly segment conversations\n   - Ensure proper handling of conversation context and topic changes\n   - Apply the max_messages_back parameter to limit conversation history\n\n4. **Workspace Detection Logic**:\n   - Implement fallback logic for workspace detection:\n     - Use configured workspace path if available\n     - Fall back to automatic detection using patterns from cursor-chat-database-research.md\n     - Handle cross-platform differences (Windows, macOS, Linux)\n\n5. **Data Transformation**:\n   - Transform raw SQLite data into the expected ChatHistory structure\n   - Ensure compatibility with existing code that consumes the ChatHistory type\n   - Maintain the existing return type structure to avoid breaking changes",
        "testStrategy": "To verify the correct refactoring of the chat collection functionality:\n\n1. **Verify Existing Tests**:\n   - Confirm that all existing tests continue to pass with the refactored implementation\n   - Ensure no regressions are introduced in the journal workflow\n   - Verify that the function maintains its expected behavior from the caller's perspective\n\n2. **Unit Tests**:\n   - Create test cases for the refactored collect_chat_history function:\n     - Test with valid workspace path and existing database\n     - Test with invalid workspace path\n     - Test with valid path but missing database\n     - Test with corrupted database\n     - Test with empty database (no chats)\n     - Test with various max_messages_back values\n\n3. **Integration Tests**:\n   - Test the integration with the ChatBoundaryDetector:\n     - Verify that conversations are properly segmented\n     - Verify that topic changes are correctly identified\n     - Test with different boundary detection configurations\n\n4. **Cross-Platform Testing**:\n   - Test on Windows, macOS, and Linux to ensure workspace detection works correctly\n   - Verify database path resolution on each platform\n   - Test with different user permission scenarios\n\n5. **Manual Verification**:\n   - Run the function against a known Cursor chat database\n   - Compare the output with expected chat history\n   - Verify that all messages are correctly extracted\n   - Check that timestamps and user information are preserved\n\n6. **Error Handling Verification**:\n   - Deliberately introduce errors to test error handling:\n     - Remove database file permissions\n     - Modify database schema\n     - Simulate database corruption\n     - Test with network drives or unusual file paths\n\n7. **Performance Testing**:\n   - Test with large chat histories to ensure performance\n   - Measure execution time and memory usage\n   - Verify that the function handles large datasets efficiently\n   - Test the impact of different max_messages_back values",
        "subtasks": [
          {
            "id": 1,
            "title": "Basic cursor_db Integration",
            "description": "Update collect_chat_history() in context_collection.py to use cursor_db package with hardcoded 200/200 message limits",
            "details": "Replace the placeholder implementation with cursor_db integration while preserving function signature and existing telemetry\n<info added on 2025-06-27T05:54:18.443Z>\n# Task 48.1 Implementation Plan - Basic cursor_db Integration\n\n## WRITE TESTS FIRST\n\nCreate `tests/unit/test_chat_collection_cursor_integration.py` with comprehensive test coverage:\n- Test that `collect_chat_history()` properly calls `query_cursor_chat_database()`\n- Test handling of `since_commit` parameter (document that it's not supported yet)\n- Test `max_messages_back` parameter interaction with message limiting from Task 47\n- Test successful cursor_db integration returns `ChatHistory` object\n- Test graceful handling when cursor_db returns empty/no data\n- Test that message limiting from Task 47 is applied with hardcoded 200/200 defaults\n- Test error scenarios (cursor_db exceptions, import failures)\n- Test conversion from cursor_db format (separate prompts/generations) to ChatHistory\n\n**RUN TESTS - VERIFY THEY FAIL**\n\n## APPROVED DESIGN CHOICES\n\n- Update existing `collect_chat_history()` in `context_collection.py` to use cursor_db\n- Call `query_cursor_chat_database()` from the cursor_db package\n- Map cursor_db response format to expected `ChatHistory` return type\n- `since_commit` parameter: Document as \"reserved for future use\" (no TODO comments)\n- `max_messages_back`: Keep for compatibility but actual limits come from Task 47 hardcoded defaults\n- Preserve `@trace_mcp_operation` decorator and existing function signature\n- Use hardcoded 200/200 limits from Task 47.1 research validation\n\n## IMPLEMENT FUNCTIONALITY\n\nUpdate `src/mcp_commit_story/context_collection.py`:\n```python\nfrom mcp_commit_story.cursor_db import query_cursor_chat_database\nfrom mcp_commit_story.cursor_db.message_limiting import (\n    limit_chat_messages, \n    DEFAULT_MAX_HUMAN_MESSAGES,  # 200\n    DEFAULT_MAX_AI_MESSAGES      # 200\n)\n```\n\nImplementation steps:\n- Replace existing implementation with cursor_db integration\n- Convert cursor_db's separate prompts/generations to alternating ChatHistory format\n- Apply message limiting: `limit_chat_messages(chat_history, DEFAULT_MAX_HUMAN_MESSAGES, DEFAULT_MAX_AI_MESSAGES)`\n- Map cursor_db's response to expected `ChatHistory` structure\n- Handle edge cases: empty data, import errors, cursor not installed - return empty results gracefully\n- Ensure telemetry span tracks integration success/failure\n\n**RUN TESTS - VERIFY THEY PASS**\n\n## TELEMETRY ATTRIBUTES TO ADD\n\n- `cursor_db_available`: boolean indicating if import succeeded\n- `prompts_from_cursor`: count of user prompts from cursor_db\n- `generations_from_cursor`: count of AI generations from cursor_db\n- `workspace_detected`: whether cursor_db found a workspace\n- `conversion_success`: whether format conversion succeeded\n- `message_limiting_applied`: whether 200/200 limits were applied\n\n## DOCUMENT AND COMPLETE\n\n- Update `collect_chat_history()` docstring noting cursor_db integration\n- Document that `since_commit` is reserved for future enhancement\n- Add comments explaining conversion from cursor_db (separate tables) to ChatHistory (alternating messages)\n- Note this bridges old architecture with new cursor_db package\n- Add to engineering spec: \"Git commit boundary integration: The since_commit parameter is currently reserved for future use. Future enhancement could filter messages based on git commit timestamps to provide more precise context boundaries.\"\n- Run the entire test suite\n- **MARK COMPLETE**\n</info added on 2025-06-27T05:54:18.443Z>",
            "status": "pending",
            "dependencies": [],
            "parentTaskId": 48
          },
          {
            "id": 2,
            "title": "Enhance with Boundary Detection",
            "description": "Add conversation boundary detection to segment chats and create ChatConversation objects",
            "details": "Implement time-based conversation segmentation and create new data structures for conversation boundaries\n<info added on 2025-06-27T05:54:54.691Z>\n# Task 48.2 Implementation Plan - Enhance with Boundary Detection\n\n## WRITE TESTS FIRST\n\nCreate `tests/unit/test_chat_boundary_detection.py` with comprehensive test coverage:\n- Test boundary detection with continuous conversation (no gaps)\n- Test detection of conversation breaks (30+ minute gaps)\n- Test handling of messages without timestamps (human messages)\n- Test conversion to `ChatConversation` objects  \n- Test preservation of cursor_db metadata through segmentation\n- Test `ChatHistory` enhancement to support multiple conversations\n- Test edge cases: single message, empty chat, malformed timestamps\n- Test configuration of gap threshold (default 30 minutes)\n\n**RUN TESTS - VERIFY THEY FAIL**\n\n## DESIGN DECISIONS\n\n### Data Structure Enhancements:\n**Add to `context_types.py`:**\n```python\n@dataclass\nclass ChatConversation:\n    \"\"\"Represents a single conversation segment with temporal boundaries.\"\"\"\n    start_time: datetime\n    end_time: datetime\n    messages: List[Dict[str, Any]]\n    topic_summary: Optional[str] = None\n    message_count: int = 0\n    \n@dataclass \nclass ChatHistory:\n    \"\"\"Enhanced to support multiple conversation segments.\"\"\"\n    source: str\n    total_messages: int\n    conversations: List[ChatConversation]  # New field\n    workspace_info: Optional[Dict[str, Any]] = None\n    collection_metadata: Optional[Dict[str, Any]] = None\n```\n\n### Boundary Detection Logic:\n- **Gap Threshold**: 30+ minutes between AI generations = new conversation\n- **Fallback for Human Messages**: Group with nearest AI message by position\n- **Single Message Handling**: Create conversation with start_time = end_time\n- **Topic Summarization**: Optional future enhancement (not in MVP)\n\n## IMPLEMENT FUNCTIONALITY\n\n### 1. **Create Boundary Detection Class**\nIn `src/mcp_commit_story/chat_boundary_detector.py`:\n```python\nclass ChatBoundaryDetector:\n    def __init__(self, gap_threshold_minutes: int = 30):\n        self.gap_threshold = timedelta(minutes=gap_threshold_minutes)\n    \n    def segment_conversations(self, messages: List[Dict]) -> List[ChatConversation]:\n        # Implementation for AI timestamp-based segmentation\n        pass\n```\n\n### 2. **Enhance collect_chat_history()**\nIn `context_collection.py` (builds on 48.1):\n- Apply boundary detection to cursor_db output\n- Convert segmented conversations to `ChatConversation` objects\n- Return enhanced `ChatHistory` with conversations list\n- Maintain backward compatibility for callers expecting flat message list\n\n### 3. **Update context_types.py**\n- Add `ChatConversation` dataclass\n- Enhance `ChatHistory` to support conversation segments\n- Ensure backward compatibility with existing message access patterns\n\n**RUN TESTS - VERIFY THEY PASS**\n\n## TELEMETRY ATTRIBUTES TO ADD\n\n- `conversations_detected`: number of conversation segments found\n- `avg_conversation_length`: average messages per conversation\n- `longest_gap_minutes`: largest time gap detected between messages\n- `boundary_detection_success`: whether segmentation completed successfully\n- `fallback_grouping_used`: whether fallback logic was needed for timestamp-less messages\n\n## LIMITATIONS TO DOCUMENT\n\n- **Human Message Timestamps**: Not available in cursor_db, grouped by proximity to AI messages\n- **Topic Detection**: Not implemented in MVP (manual analysis would be needed)\n- **Cross-Session Boundaries**: Only detects gaps within single database sessions\n- **Timezone Handling**: Uses timestamps as-is from cursor_db (typically UTC)\n\n## DOCUMENT AND COMPLETE\n\n- Update engineering spec with conversation boundary feature\n- Document the 30-minute gap threshold rationale\n- Add examples showing `ChatHistory` with multiple conversations\n- Note limitations around human message timestamp availability\n- Document backward compatibility approach for existing callers\n- Run the entire test suite including integration tests\n- **MARK COMPLETE**\n\n## FUTURE ENHANCEMENT NOTES\n\nDocument in engineering spec under \"Future Enhancements\":\n- **Topic Summarization**: AI-powered conversation topic detection\n- **Git Integration**: Boundary detection based on commit timestamps\n- **Configurable Thresholds**: User-customizable gap detection settings\n- **Cross-Database Sessions**: Boundary detection across multiple workspace databases\n</info added on 2025-06-27T05:54:54.691Z>",
            "status": "pending",
            "dependencies": [
              "48.1"
            ],
            "parentTaskId": 48
          }
        ]
      },
      {
        "id": 49,
        "title": "Implement Git-Driven Chat Parsing",
        "description": "Add a function that intelligently filters chat history based on git diff content to find only relevant discussions related to code changes, and modify the existing collect_chat_history() to optionally use this filter.",
        "status": "pending",
        "dependencies": [
          48
        ],
        "priority": "high",
        "details": "This task extends the existing chat collection system with a sophisticated filtering capability that uses git diff information to identify relevant chat segments:\n\n1. **Extract Keywords from Git Diffs**:\n```python\ndef extract_keywords_from_diff(diff_content):\n    \"\"\"\n    Extract meaningful keywords, function names, and concepts from git diff content.\n    \n    Args:\n        diff_content (str): The raw git diff content\n        \n    Returns:\n        list: Extracted keywords, function names, and concepts\n    \"\"\"\n    keywords = []\n    \n    # Extract function/method names (lines starting with + or - and containing def or function)\n    function_pattern = r'[+-].*(?:def|function)\\s+(\\w+)'\n    functions = re.findall(function_pattern, diff_content)\n    keywords.extend(functions)\n    \n    # Extract class names\n    class_pattern = r'[+-].*class\\s+(\\w+)'\n    classes = re.findall(class_pattern, diff_content)\n    keywords.extend(classes)\n    \n    # Extract variable names with meaningful context\n    var_pattern = r'[+-].*(\\w+)\\s*='\n    variables = re.findall(var_pattern, diff_content)\n    keywords.extend([v for v in variables if len(v) > 3])  # Filter out short variable names\n    \n    # Extract file paths and names\n    file_pattern = r'(?:---|\\+\\+\\+)\\s+(?:a/|b/)?(.+)'\n    files = re.findall(file_pattern, diff_content)\n    keywords.extend([os.path.basename(f) for f in files])\n    \n    # Remove duplicates and common programming keywords\n    common_words = {'self', 'None', 'True', 'False', 'return', 'import', 'from', 'as'}\n    keywords = [k for k in keywords if k not in common_words]\n    \n    return list(set(keywords))\n```\n\n2. **Score Chat Relevance**:\n```python\ndef score_chat_relevance(chat_message, keywords, threshold=0.3):\n    \"\"\"\n    Score the relevance of a chat message based on extracted keywords.\n    \n    Args:\n        chat_message (dict): The chat message to score\n        keywords (list): Keywords extracted from git diff\n        threshold (float): Minimum score to consider relevant\n        \n    Returns:\n        float: Relevance score between 0 and 1\n    \"\"\"\n    if not keywords or not chat_message.get('content'):\n        return 0.0\n    \n    content = chat_message['content'].lower()\n    \n    # Basic keyword matching\n    direct_matches = sum(1 for kw in keywords if kw.lower() in content)\n    \n    # Use NLP techniques for semantic matching\n    # This could be enhanced with embeddings or more sophisticated NLP\n    words = re.findall(r'\\w+', content)\n    word_matches = sum(1 for kw in keywords for w in words \n                      if (kw.lower() in w.lower() or w.lower() in kw.lower()) \n                      and len(w) > 3)\n    \n    # Calculate final score\n    total_keywords = len(keywords)\n    if total_keywords == 0:\n        return 0.0\n        \n    score = (direct_matches * 0.7 + word_matches * 0.3) / total_keywords\n    \n    return score if score >= threshold else 0.0\n```\n\n3. **Add New Filter Function to context_collection.py**:\n```python\n@trace_mcp_operation\ndef filter_chat_by_git_relevance(chat_history, diff_content, min_score=0.3, context_window=2):\n    \"\"\"\n    Filter chat history based on git diff content to find relevant discussions.\n    \n    Args:\n        chat_history (list): Complete chat history from collect_chat_history()\n        diff_content (str): Git diff content\n        min_score (float): Minimum relevance score threshold\n        context_window (int): Number of messages to include before/after relevant ones\n        \n    Returns:\n        list: Filtered chat segments relevant to the code changes\n    \"\"\"\n    if not chat_history or not diff_content:\n        return chat_history  # Return original if no filtering possible\n    \n    # Extract keywords from diff\n    keywords = extract_keywords_from_diff(diff_content)\n    \n    # Score each message\n    scored_messages = []\n    for i, message in enumerate(chat_history):\n        score = score_chat_relevance(message, keywords, min_score)\n        scored_messages.append((i, message, score))\n    \n    # Identify relevant message clusters\n    relevant_indices = set()\n    for i, message, score in scored_messages:\n        if score >= min_score:\n            # Add the message and its context window\n            for j in range(max(0, i - context_window), min(len(chat_history), i + context_window + 1)):\n                relevant_indices.add(j)\n    \n    # Create continuous segments\n    if not relevant_indices:\n        return []  # No relevant messages found\n        \n    relevant_indices = sorted(relevant_indices)\n    segments = []\n    current_segment = []\n    \n    for i in range(len(chat_history)):\n        if i in relevant_indices:\n            current_segment.append(chat_history[i])\n        elif current_segment:\n            segments.append(current_segment)\n            current_segment = []\n    \n    if current_segment:\n        segments.append(current_segment)\n    \n    return segments\n```\n\n4. **Modify Existing collect_chat_history Function**:\n```python\n@trace_mcp_operation\ndef collect_chat_history(commit_hash=None, config=None, filter_by_git=False, file_path=None):\n    \"\"\"\n    Collect chat history, optionally filtered by git relevance.\n    \n    Args:\n        commit_hash (str): Optional commit hash for context\n        config (dict): Configuration options\n        filter_by_git (bool): Whether to filter chat by git relevance\n        file_path (str): Optional specific file to focus on for filtering\n        \n    Returns:\n        list: Chat history, potentially filtered by git relevance\n    \"\"\"\n    # Original chat collection logic remains unchanged\n    chat_history = [...existing implementation...]  \n    \n    # Apply git-based filtering if requested\n    if filter_by_git:\n        # Get git diff content using existing collect_git_context patterns\n        if commit_hash:\n            diff_content = get_git_diff(commit_hash, file_path)\n        else:\n            diff_content = get_uncommitted_changes(file_path)\n        \n        # Apply filtering with configurable parameters\n        min_score = config.get('min_relevance_score', 0.3) if config else 0.3\n        context_window = config.get('context_window', 2) if config else 2\n        \n        return filter_chat_by_git_relevance(chat_history, diff_content, min_score, context_window)\n    \n    # Return unfiltered chat history if filtering not requested\n    return chat_history\n```\n\nThe implementation should:\n- Preserve backward compatibility by making filtering optional\n- Integrate with existing git context collection patterns\n- Handle edge cases such as empty git diffs or chat history\n- Support configurable relevance thresholds and context windows\n- Include proper error handling for git command failures\n- Be optimized for performance with large chat histories",
        "testStrategy": "To verify the correct implementation of the Git-Driven Chat Parsing functionality:\n\n1. **Unit Tests for Keyword Extraction**:\n   - Create test cases with sample git diffs containing various code constructs (functions, classes, variables)\n   - Verify that the `extract_keywords_from_diff` function correctly identifies and extracts:\n     - Function names (both additions and deletions)\n     - Class names\n     - Meaningful variable names\n     - File paths and names\n   - Test with empty diffs, large diffs, and diffs with special characters\n\n2. **Unit Tests for Relevance Scoring**:\n   - Create test cases with predefined keywords and various chat messages\n   - Verify that `score_chat_relevance` correctly scores messages:\n     - High scores for messages containing exact keyword matches\n     - Moderate scores for messages with partial or semantic matches\n     - Zero scores for completely irrelevant messages\n   - Test edge cases: empty keywords, empty messages, very long messages\n\n3. **Integration Tests for Filtering Logic**:\n   - Create mock chat history and git diff content\n   - Verify that `filter_chat_by_git_relevance` correctly:\n     - Identifies relevant messages based on scoring\n     - Includes appropriate context window messages\n     - Creates continuous segments when relevant messages are close\n     - Returns empty list for irrelevant chat history\n     - Returns original chat history when diff content is empty\n\n4. **Backward Compatibility Tests**:\n   - Verify that `collect_chat_history` works exactly as before when `filter_by_git=False`\n   - Ensure no regression in existing functionality\n   - Test with various combinations of parameters to ensure all paths work correctly\n\n5. **End-to-End Tests**:\n   - Test the modified `collect_chat_history` function with:\n     - `filter_by_git=True` and various other parameters\n     - Real commit hashes from the project repository\n     - Different configuration options (min_score, context_window)\n     - Various file paths to filter specific changes\n   - Verify that the returned chat segments are actually relevant to the code changes\n\n6. **Performance Testing**:\n   - Measure execution time with large chat histories (1000+ messages)\n   - Compare performance with and without filtering\n   - Ensure memory usage remains reasonable with large inputs\n   - Verify that the implementation scales linearly with input size\n\n7. **Manual Verification**:\n   - Perform manual review of filtered results for a set of real commits\n   - Compare the filtered chat segments with the complete history to verify relevance\n   - Adjust relevance thresholds based on manual verification results\n\n8. **Error Handling Tests**:\n   - Test with invalid commit hashes\n   - Test with non-existent file paths\n   - Verify proper error messages and graceful failure modes\n   - Ensure the system degrades gracefully when filtering cannot be applied\n\n9. **Configuration Tests**:\n   - Verify that different configuration values properly affect the filtering behavior\n   - Test extreme configuration values (very high/low thresholds, large context windows)\n\nDocument all test results and any adjustments made to the implementation based on test findings.",
        "subtasks": []
      },
      {
        "id": 50,
        "title": "Create Standalone Journal Generator",
        "description": "Refactor the existing git_hook_worker.py to call journal_workflow.generate_journal_entry() directly instead of creating signals, enabling journal generation to run from git hooks without requiring signals or MCP server.",
        "status": "pending",
        "dependencies": [
          48,
          49
        ],
        "priority": "high",
        "details": "This task refactors the existing git hook worker to enable standalone journal generation that operates independently of the MCP server architecture:\n\n1. **Refactor git_hook_worker.py to Call Journal Generation Directly**:\n```python\n# Current implementation (to be refactored):\ndef process_git_hook(hook_type):\n    \"\"\"Process a git hook event by creating appropriate signals\"\"\"\n    try:\n        git_context = collect_git_context()\n        # Create signal for journal generation\n        create_tool_signal('journal_generation', {\n            'git_context': git_context,\n            'hook_type': hook_type\n        })\n        return True\n    except Exception as e:\n        log_error(f\"Git hook processing failed: {str(e)}\")\n        return False\n\n# Refactored implementation:\ndef process_git_hook(hook_type):\n    \"\"\"Process a git hook event by directly generating journal entries\"\"\"\n    try:\n        git_context = collect_git_context()\n        \n        # Import journal workflow and call directly instead of creating a signal\n        from journal_workflow import generate_journal_entry\n        \n        # Call journal generation directly with git context\n        success = generate_journal_entry(git_context=git_context, hook_type=hook_type)\n        \n        # Log telemetry for the direct call\n        log_telemetry('git_hook_journal_generation', {\n            'success': success,\n            'hook_type': hook_type,\n            'commit_hash': git_context.get('commit_hash', 'unknown')\n        })\n        \n        return success\n    except Exception as e:\n        log_error(f\"Git hook processing failed: {str(e)}\")\n        return False\n```\n\n2. **Update Import Statements**:\n```python\n# Add these imports at the top of git_hook_worker.py\nfrom journal_workflow import generate_journal_entry\nfrom context_collection import collect_git_context\nfrom logging_utils import log_error, log_telemetry\n```\n\n3. **Ensure Error Handling is Preserved**:\n```python\n# Make sure all existing error handling is maintained\ndef safe_journal_generation(git_context, hook_type):\n    \"\"\"Safely generate journal entries with proper error handling\"\"\"\n    try:\n        from journal_workflow import generate_journal_entry\n        return generate_journal_entry(git_context=git_context, hook_type=hook_type)\n    except ImportError as e:\n        log_error(f\"Failed to import journal_workflow: {str(e)}\")\n        return False\n    except Exception as e:\n        log_error(f\"Journal generation failed: {str(e)}\")\n        return False\n```\n\n4. **Update CLI Command Handler**:\n```python\ndef handle_cli_commands():\n    \"\"\"Handle CLI commands for git hook management\"\"\"\n    parser = argparse.ArgumentParser(description='Git hook management and journal generation')\n    parser.add_argument('--install-hooks', action='store_true', help='Install git hooks for automatic journal generation')\n    parser.add_argument('--generate-now', action='store_true', help='Generate journal entry immediately')\n    \n    args = parser.parse_args()\n    \n    if args.install_hooks:\n        success = install_git_hooks()\n        print(f\"Git hooks installation {'successful' if success else 'failed'}\")\n    \n    if args.generate_now:\n        git_context = collect_git_context()\n        from journal_workflow import generate_journal_entry\n        success = generate_journal_entry(git_context=git_context, hook_type='manual')\n        print(f\"Journal generation {'successful' if success else 'failed'}\")\n        \n    if not args.install_hooks and not args.generate_now:\n        parser.print_help()\n```\n\n5. **Update Post-Commit Hook Template**:\n```python\ndef get_hook_template(hook_type):\n    \"\"\"Get the template for a specific git hook type\"\"\"\n    if hook_type == 'post-commit':\n        return \"\"\"#!/bin/sh\n# Auto-generated journal hook\npython -m git_hook_worker --hook-type=post-commit\n\"\"\"\n    # Other hook templates...\n    return \"\"\"\"\"\"\n\n# Update the hook handler to support direct execution\ndef main():\n    \"\"\"Main entry point for git hook worker\"\"\"\n    parser = argparse.ArgumentParser(description='Git hook worker')\n    parser.add_argument('--hook-type', type=str, help='Type of git hook being processed')\n    \n    args = parser.parse_args()\n    \n    if args.hook_type:\n        process_git_hook(args.hook_type)\n    else:\n        handle_cli_commands()\n```\n\n6. **Ensure Compatibility with Existing Journal Workflow**:\n```python\n# Add this function to check if we can use the direct approach\ndef can_use_direct_journal_generation():\n    \"\"\"Check if we can use direct journal generation\"\"\"\n    try:\n        from journal_workflow import generate_journal_entry\n        return True\n    except ImportError:\n        return False\n\n# Fallback mechanism if needed\ndef process_git_hook_with_fallback(hook_type):\n    \"\"\"Process git hook with fallback to signal if direct approach fails\"\"\"\n    try:\n        git_context = collect_git_context()\n        \n        # Try direct approach first\n        if can_use_direct_journal_generation():\n            from journal_workflow import generate_journal_entry\n            success = generate_journal_entry(git_context=git_context, hook_type=hook_type)\n            if success:\n                return True\n        \n        # Fallback to signal approach if direct approach fails or isn't available\n        from signal_utils import create_tool_signal\n        create_tool_signal('journal_generation', {\n            'git_context': git_context,\n            'hook_type': hook_type\n        })\n        return True\n    except Exception as e:\n        log_error(f\"Git hook processing failed: {str(e)}\")\n        return False\n```\n\n7. **Update Tests for Direct Journal Generation**:\n```python\ndef test_direct_journal_generation():\n    \"\"\"Test direct journal generation from git hook\"\"\"\n    # Mock git context collection\n    with patch('context_collection.collect_git_context') as mock_collect, \\\n         patch('journal_workflow.generate_journal_entry') as mock_generate:\n        \n        # Configure mocks\n        mock_collect.return_value = {'commit_hash': 'test123'}\n        mock_generate.return_value = True\n        \n        # Call the function\n        result = process_git_hook('post-commit')\n        \n        # Verify correct function calls\n        mock_collect.assert_called_once()\n        mock_generate.assert_called_once_with(\n            git_context={'commit_hash': 'test123'}, \n            hook_type='post-commit'\n        )\n        \n        assert result is True\n```",
        "testStrategy": "To verify the refactored git hook worker is functioning correctly:\n\n1. **Unit Testing Direct Journal Generation**:\n   - Create unit tests for the refactored process_git_hook function:\n     ```python\n     def test_process_git_hook_direct_call():\n         # Mock dependencies\n         with patch('git_hook_worker.collect_git_context') as mock_git, \\\n              patch('journal_workflow.generate_journal_entry') as mock_generate, \\\n              patch('git_hook_worker.log_telemetry') as mock_telemetry:\n             \n             # Configure mocks\n             mock_git.return_value = {'commit_hash': 'abc123'}\n             mock_generate.return_value = True\n             \n             # Call function\n             result = process_git_hook('post-commit')\n             \n             # Verify direct call to journal generation\n             mock_git.assert_called_once()\n             mock_generate.assert_called_once_with(\n                 git_context={'commit_hash': 'abc123'},\n                 hook_type='post-commit'\n             )\n             mock_telemetry.assert_called_once()\n             \n             assert result is True\n     ```\n\n2. **Test Error Handling**:\n   - Verify that errors are properly handled:\n     ```python\n     def test_process_git_hook_error_handling():\n         # Test with journal generation raising an exception\n         with patch('git_hook_worker.collect_git_context') as mock_git, \\\n              patch('journal_workflow.generate_journal_entry', side_effect=Exception(\"Test error\")), \\\n              patch('git_hook_worker.log_error') as mock_log:\n             \n             mock_git.return_value = {'commit_hash': 'abc123'}\n             \n             result = process_git_hook('post-commit')\n             \n             # Verify error was logged but not raised\n             mock_log.assert_called_once()\n             assert \"Test error\" in mock_log.call_args[0][0]\n             assert result is False\n     ```\n\n3. **Test CLI Integration**:\n   - Test the command-line interface:\n     ```python\n     def test_cli_generate_now():\n         # Mock dependencies\n         with patch('git_hook_worker.collect_git_context') as mock_git, \\\n              patch('journal_workflow.generate_journal_entry') as mock_generate, \\\n              patch('sys.argv', ['git_hook_worker.py', '--generate-now']), \\\n              patch('builtins.print') as mock_print:\n             \n             mock_git.return_value = {'commit_hash': 'abc123'}\n             mock_generate.return_value = True\n             \n             # Call main function\n             main()\n             \n             # Verify journal generation was called\n             mock_git.assert_called_once()\n             mock_generate.assert_called_once()\n             mock_print.assert_called_with(\"Journal generation successful\")\n     ```\n\n4. **Test Hook Execution**:\n   - Test the hook execution path:\n     ```python\n     def test_hook_execution():\n         # Mock dependencies\n         with patch('git_hook_worker.process_git_hook') as mock_process, \\\n              patch('sys.argv', ['git_hook_worker.py', '--hook-type=post-commit']):\n             \n             mock_process.return_value = True\n             \n             # Call main function\n             main()\n             \n             # Verify process_git_hook was called with correct hook type\n             mock_process.assert_called_once_with('post-commit')\n     ```\n\n5. **Integration Test with Actual Journal Workflow**:\n   - Create an integration test that verifies the actual journal generation:\n     ```python\n     def test_integration_with_journal_workflow():\n         # Create a test git context\n         test_git_context = {\n             'commit_hash': 'test123',\n             'branch': 'test-branch',\n             'commit_message': 'Test commit',\n             'diff': 'test diff content',\n             'author': 'Test User',\n             'timestamp': '1609459200'\n         }\n         \n         # Mock git context collection but use real journal generation\n         with patch('git_hook_worker.collect_git_context', return_value=test_git_context):\n             # Create a temporary journal directory\n             with tempfile.TemporaryDirectory() as temp_dir:\n                 # Configure journal output to use temp directory\n                 with patch('journal.get_journal_directory', return_value=temp_dir):\n                     # Run the hook processing\n                     result = process_git_hook('post-commit')\n                     \n                     # Verify journal file was created\n                     today = datetime.now().strftime('%Y-%m-%d')\n                     journal_path = os.path.join(temp_dir, f\"{today}.md\")\n                     \n                     assert os.path.exists(journal_path)\n                     with open(journal_path, 'r') as f:\n                         content = f.read()\n                         assert 'Test commit' in content\n                     \n                     assert result is True\n     ```\n\n6. **Test Compatibility Check**:\n   - Test the compatibility check function:\n     ```python\n     def test_can_use_direct_journal_generation():\n         # Test when journal_workflow is available\n         with patch.dict('sys.modules', {'journal_workflow': MagicMock()}):\n             assert can_use_direct_journal_generation() is True\n         \n         # Test when journal_workflow is not available\n         with patch.dict('sys.modules', {}, clear=True):\n             with patch('builtins.__import__', side_effect=ImportError):\n                 assert can_use_direct_journal_generation() is False\n     ```\n\n7. **Test Fallback Mechanism**:\n   - Test the fallback to signal approach:\n     ```python\n     def test_process_git_hook_with_fallback():\n         # Mock dependencies\n         with patch('git_hook_worker.collect_git_context') as mock_git, \\\n              patch('git_hook_worker.can_use_direct_journal_generation', return_value=False), \\\n              patch('signal_utils.create_tool_signal') as mock_signal:\n             \n             mock_git.return_value = {'commit_hash': 'abc123'}\n             \n             # Call function\n             result = process_git_hook_with_fallback('post-commit')\n             \n             # Verify fallback to signal\n             mock_git.assert_called_once()\n             mock_signal.assert_called_once_with('journal_generation', {\n                 'git_context': {'commit_hash': 'abc123'},\n                 'hook_type': 'post-commit'\n             })\n             \n             assert result is True\n     ```\n\n8. **Manual Testing with Git Commits**:\n   1. Install the refactored git hooks using the CLI tool\n   2. Make a small code change and commit it\n   3. Verify that a journal entry was generated in the journal directory\n   4. Check that the journal entry contains relevant information about the commit\n   5. Verify no signals were created in the process\n\n9. **Performance Testing**:\n   - Measure execution time to ensure it's suitable for a git hook:\n     ```python\n     def test_performance():\n         start_time = time.time()\n         process_git_hook('post-commit')\n         execution_time = time.time() - start_time\n         \n         # Should complete in a reasonable time for a git hook\n         assert execution_time < 5.0  # 5 seconds max\n     ```\n\n10. **Verify No Signal Dependencies**:\n    - Create a test that ensures no signals are created:\n      ```python\n      def test_no_signals_created():\n          # Mock dependencies including signal creation\n          with patch('git_hook_worker.collect_git_context') as mock_git, \\\n               patch('journal_workflow.generate_journal_entry') as mock_generate, \\\n               patch('signal_utils.create_tool_signal') as mock_signal:\n              \n              mock_git.return_value = {'commit_hash': 'abc123'}\n              mock_generate.return_value = True\n              \n              # Call function\n              process_git_hook('post-commit')\n              \n              # Verify signal creation was not called\n              mock_signal.assert_not_called()\n      ```",
        "subtasks": [
          {
            "id": 1,
            "title": "Refactor git_hook_worker.py to Call Journal Generation Directly",
            "description": "Update the process_git_hook function to directly call journal_workflow.generate_journal_entry() instead of creating signals.",
            "dependencies": [],
            "details": "Modify the existing process_git_hook function to import and call journal_workflow.generate_journal_entry() directly with the collected git context. Remove any signal creation logic (create_tool_signal calls) while preserving all existing error handling and telemetry logging. Ensure the function maintains the same return value behavior (True for success, False for failure).",
            "status": "pending",
            "testStrategy": "Create unit tests that verify the direct call to journal generation is made correctly and that no signals are created. Test with both successful and failing scenarios to ensure error handling is preserved."
          },
          {
            "id": 2,
            "title": "Update Import Statements and Dependencies",
            "description": "Add necessary imports for journal_workflow and ensure all dependencies are properly managed.",
            "dependencies": [],
            "details": "Add import statements for journal_workflow.generate_journal_entry at the top of git_hook_worker.py. Review and update any other imports that may be needed. Implement a can_use_direct_journal_generation() function to check if the direct approach is available, which can be used for graceful fallback if needed.",
            "status": "pending",
            "testStrategy": "Test the import statements with various module availability scenarios. Verify that the can_use_direct_journal_generation function correctly identifies when the journal workflow is available."
          },
          {
            "id": 3,
            "title": "Update CLI Command Handler for Direct Journal Generation",
            "description": "Modify the CLI command handler to use direct journal generation when the --generate-now flag is used.",
            "dependencies": [],
            "details": "Update the handle_cli_commands function to directly call journal_workflow.generate_journal_entry when the --generate-now flag is provided, instead of creating a signal. Preserve all existing CLI options and help text. Ensure proper error handling and user feedback through console output.",
            "status": "pending",
            "testStrategy": "Test the CLI with various argument combinations, including --generate-now. Verify that the direct journal generation is called correctly and that appropriate feedback is provided to the user."
          },
          {
            "id": 4,
            "title": "Implement Fallback Mechanism for Compatibility",
            "description": "Create a fallback mechanism that can use signal-based journal generation if direct generation fails.",
            "dependencies": [],
            "details": "Implement a process_git_hook_with_fallback function that first attempts to use direct journal generation, but falls back to the signal approach if the direct method fails or isn't available. This ensures backward compatibility and graceful degradation if the journal workflow module can't be imported or encounters errors.",
            "status": "pending",
            "testStrategy": "Test the fallback mechanism with scenarios where direct generation succeeds and where it fails. Verify that the signal approach is used as a fallback only when needed."
          },
          {
            "id": 5,
            "title": "Update Tests for Direct Journal Generation",
            "description": "Create new tests and update existing tests to verify the direct journal generation approach.",
            "dependencies": [],
            "details": "Develop comprehensive tests for the refactored git_hook_worker.py, including unit tests for direct journal generation, error handling, CLI integration, and the fallback mechanism. Ensure all tests pass with the refactored implementation.",
            "status": "pending",
            "testStrategy": "Create a comprehensive test suite that covers all aspects of the refactored implementation, including direct calls, error handling, CLI integration, and the fallback mechanism."
          },
          {
            "id": 6,
            "title": "Verify Integration with Existing Journal Workflow",
            "description": "Ensure the refactored git hook worker integrates correctly with the existing journal workflow.",
            "dependencies": [],
            "details": "Perform integration testing to verify that the refactored git hook worker correctly calls the journal workflow and generates journal entries. Test with actual git commits to ensure the end-to-end flow works as expected. Verify that no signals are created in the process.",
            "status": "pending",
            "testStrategy": "Perform integration tests with actual git commits and verify that journal entries are generated correctly. Check that no signals are created during the process."
          }
        ]
      },
      {
        "id": 51,
        "title": "Implement Journal/Capture-Context MCP Tool",
        "description": "Create an MCP tool that allows users to manually capture context that will be included in future journal entries, enabling developers to add relevant information that might not be captured automatically.",
        "details": "Implement the journal/capture-context MCP tool with the following components:\n\n1. **MCP Server Handler**:\n```python\n@trace_mcp_operation\ndef handle_journal_capture_context(params, config):\n    \"\"\"\n    Handle requests to capture manual context for journal entries.\n    \n    Args:\n        params (dict): Parameters including:\n            - text (str): The context text to capture\n            - tags (list, optional): List of tags to associate with the context\n        config (dict): Configuration dictionary\n        \n    Returns:\n        dict: Response with status and captured context details\n    \"\"\"\n    try:\n        # Extract parameters\n        text = params.get(\"text\")\n        if not text:\n            return {\"status\": \"error\", \"message\": \"No context text provided\"}\n            \n        tags = params.get(\"tags\", [\"manual-context\"])\n        if \"manual-context\" not in tags:\n            tags.append(\"manual-context\")\n            \n        # Format the captured context\n        timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        formatted_context = f\"\\n\\n## Manual Context Capture ({timestamp})\\n\\n{text}\\n\\n\"\n        formatted_context += f\"Tags: {', '.join(tags)}\\n\"\n        \n        # Determine today's journal file path\n        journal_path = Path(config[\"journal\"][\"path\"])\n        today = datetime.now().strftime(\"%Y-%m-%d\")\n        journal_file = journal_path / f\"{today}.md\"\n        \n        # Create directory if it doesn't exist\n        journal_path.mkdir(parents=True, exist_ok=True)\n        \n        # Append context to today's journal file\n        with open(journal_file, \"a+\") as f:\n            f.write(formatted_context)\n            \n        return {\n            \"status\": \"success\",\n            \"message\": \"Context captured successfully\",\n            \"file\": str(journal_file),\n            \"timestamp\": timestamp,\n            \"tags\": tags\n        }\n    except Exception as e:\n        return {\"status\": \"error\", \"message\": f\"Failed to capture context: {str(e)}\"}\n```\n\n2. **Register the Handler in MCP Server**:\nAdd the new handler to the server's tool registry in `src/mcp_commit_story/server.py`:\n```python\ndef register_tools():\n    # ... existing tool registrations ...\n    \n    # Register the capture-context tool\n    register_tool(\n        \"journal/capture-context\",\n        \"Capture manual context for journal entries\",\n        handle_journal_capture_context,\n        [\n            {\"name\": \"text\", \"type\": \"string\", \"description\": \"Context text to capture\"},\n            {\"name\": \"tags\", \"type\": \"array\", \"description\": \"Optional tags for the context\", \"required\": False}\n        ]\n    )\n```\n\n3. **CLI Command Implementation**:\nAdd a CLI command for capturing context in `src/mcp_commit_story/cli.py`:\n```python\n@cli.command()\n@click.argument(\"text\")\n@click.option(\"--tags\", \"-t\", multiple=True, help=\"Tags to associate with the context\")\ndef capture(text, tags):\n    \"\"\"Capture manual context for journal entries.\"\"\"\n    response = send_mcp_request(\"journal/capture-context\", {\n        \"text\": text,\n        \"tags\": list(tags) if tags else [\"manual-context\"]\n    })\n    \n    if response.get(\"status\") == \"success\":\n        click.echo(f\"Context captured successfully in {response.get('file')}\")\n    else:\n        click.echo(f\"Error: {response.get('message')}\", err=True)\n```\n\n4. **Update Standalone Journal Generator**:\nModify the standalone journal generator (from Task 50) to include captured context when generating entries:\n```python\ndef collect_recent_manual_context(days=1):\n    \"\"\"\n    Collect manual context captured in recent journal entries.\n    \n    Args:\n        days (int): Number of days to look back for context\n        \n    Returns:\n        str: Concatenated manual context entries\n    \"\"\"\n    journal_path = Path(config[\"journal\"][\"path\"])\n    context_entries = []\n    \n    # Get dates for the lookback period\n    today = datetime.now().date()\n    date_range = [today - timedelta(days=i) for i in range(days)]\n    \n    # Check each date's journal file for manual context\n    for date in date_range:\n        date_str = date.strftime(\"%Y-%m-%d\")\n        journal_file = journal_path / f\"{date_str}.md\"\n        \n        if journal_file.exists():\n            with open(journal_file, \"r\") as f:\n                content = f.read()\n                \n            # Extract manual context sections using regex\n            manual_contexts = re.findall(r\"## Manual Context Capture \\(.*?\\)(.*?)(?=\\n## |\\Z)\", \n                                        content, re.DOTALL)\n            \n            if manual_contexts:\n                for context in manual_contexts:\n                    context_entries.append(context.strip())\n    \n    return \"\\n\\n\".join(context_entries)\n```\n\n5. **Integration with Journal Generation**:\nUpdate the journal generation function to include the captured context:\n```python\ndef generate_journal_entry(commit_info, config):\n    \"\"\"Generate a journal entry for a commit\"\"\"\n    # ... existing code ...\n    \n    # Add manual context if available\n    recent_context = collect_recent_manual_context()\n    if recent_context:\n        prompt_parts.append(\"\\nRecently captured manual context:\")\n        prompt_parts.append(recent_context)\n    \n    # ... continue with existing generation code ...\n```",
        "testStrategy": "To verify the correct implementation of the journal/capture-context MCP tool:\n\n1. **Unit Tests**:\n   - Create unit tests for the `handle_journal_capture_context` function:\n     ```python\n     def test_handle_journal_capture_context():\n         # Test with valid parameters\n         result = handle_journal_capture_context({\"text\": \"Test context\"}, test_config)\n         assert result[\"status\"] == \"success\"\n         assert \"file\" in result\n         \n         # Test with empty text\n         result = handle_journal_capture_context({\"text\": \"\"}, test_config)\n         assert result[\"status\"] == \"error\"\n         \n         # Test with custom tags\n         result = handle_journal_capture_context({\"text\": \"Test with tags\", \"tags\": [\"important\", \"meeting\"]}, test_config)\n         assert \"manual-context\" in result[\"tags\"]\n         assert \"important\" in result[\"tags\"]\n     ```\n\n2. **Integration Tests**:\n   - Test the MCP server handler registration:\n     ```python\n     def test_capture_context_tool_registration():\n         tools = get_registered_tools()\n         assert \"journal/capture-context\" in tools\n     ```\n   \n   - Test the CLI command:\n     ```python\n     def test_capture_cli_command():\n         runner = CliRunner()\n         result = runner.invoke(cli, [\"capture\", \"Test context from CLI\"])\n         assert \"Context captured successfully\" in result.output\n         \n         # Test with tags\n         result = runner.invoke(cli, [\"capture\", \"Test with tags\", \"-t\", \"important\", \"-t\", \"meeting\"])\n         assert \"Context captured successfully\" in result.output\n     ```\n\n3. **Manual Testing**:\n   - Execute the following test scenarios:\n     1. Capture context with the CLI command: `mcp-commit-story capture \"This is important context for today's work\"`\n     2. Verify the context is appended to today's journal file\n     3. Capture context with tags: `mcp-commit-story capture \"Meeting notes\" -t meeting -t important`\n     4. Verify the context with tags is correctly formatted in the journal file\n     5. Generate a journal entry after capturing context and verify the context is included\n     6. Test the MCP API directly: `curl -X POST http://localhost:5000/api/tool/journal/capture-context -d '{\"text\":\"API test context\"}'`\n\n4. **File System Verification**:\n   - Check that the journal directory is created if it doesn't exist\n   - Verify that context is properly appended to existing journal files\n   - Ensure the timestamp and tags are correctly formatted\n\n5. **Standalone Generator Integration Test**:\n   - Capture context using the tool\n   - Run the standalone journal generator\n   - Verify that the generated journal entry includes the captured context\n   - Test with multiple days of context to ensure the lookback period works correctly",
        "status": "pending",
        "dependencies": [
          22,
          50
        ],
        "priority": "high",
        "subtasks": []
      },
      {
        "id": 52,
        "title": "Add Machine-Readable Journal Format",
        "description": "Enhance journal entries with structured metadata for future AI parsing and analysis by implementing YAML frontmatter with standardized fields.",
        "details": "This task enhances the journal entry format to include machine-readable metadata while maintaining human readability:\n\n1. **Design YAML Frontmatter Structure**:\n   - Define a standard set of metadata fields including:\n     - `timestamp`: ISO 8601 format (YYYY-MM-DDTHH:MM:SS+TZ)\n     - `commit_hash`: Git commit SHA that triggered the journal entry\n     - `tags`: Array of relevant keywords/topics\n     - `entry_type`: Categorization (e.g., \"commit\", \"refactor\", \"bugfix\", \"feature\")\n     - `files_changed`: List of modified files\n     - `semantic_topics`: Higher-level themes detected in the change\n\n2. **Update Journal Generation Function**:\n   - Modify the standalone journal generator from Task 50 to include frontmatter:\n   ```python\n   def generate_journal_entry(git_context, chat_context=None):\n       \"\"\"Generate a journal entry with YAML frontmatter and markdown content\"\"\"\n       # Extract metadata from contexts\n       timestamp = datetime.now().isoformat()\n       commit_hash = git_context.get('commit_hash', 'unknown')\n       files_changed = git_context.get('files_changed', [])\n       \n       # Detect entry type and tags based on commit message and changes\n       entry_type = detect_entry_type(git_context)\n       tags = extract_tags(git_context, chat_context)\n       semantic_topics = analyze_semantic_content(git_context, chat_context)\n       \n       # Create YAML frontmatter\n       frontmatter = {\n           'timestamp': timestamp,\n           'commit_hash': commit_hash,\n           'entry_type': entry_type,\n           'files_changed': files_changed,\n           'tags': tags,\n           'semantic_topics': semantic_topics\n       }\n       \n       # Format as YAML\n       yaml_content = yaml.dump(frontmatter, default_flow_style=False)\n       \n       # Generate the actual journal content (using existing logic)\n       journal_content = format_journal_content(git_context, chat_context)\n       \n       # Combine with proper YAML frontmatter delimiters\n       return f\"---\\n{yaml_content}---\\n\\n{journal_content}\"\n   ```\n\n3. **Implement Helper Functions**:\n   - Create `detect_entry_type()` to categorize changes based on commit message and files\n   - Create `extract_tags()` to generate relevant keywords\n   - Create `analyze_semantic_content()` to identify higher-level themes\n\n4. **Ensure Backward Compatibility**:\n   - Add a parser function to handle both new and old format entries:\n   ```python\n   def parse_journal_entry(entry_text):\n       \"\"\"Parse a journal entry, handling both new and legacy formats\"\"\"\n       if entry_text.startswith('---'):\n           # New format with frontmatter\n           parts = entry_text.split('---', 2)\n           if len(parts) >= 3:\n               try:\n                   metadata = yaml.safe_load(parts[1])\n                   content = parts[2].strip()\n                   return {'metadata': metadata, 'content': content}\n               except yaml.YAMLError:\n                   pass\n       \n       # Legacy format or parsing error - return as content only\n       return {'metadata': {}, 'content': entry_text.strip()}\n   ```\n\n5. **Update Documentation**:\n   - Document the new journal format structure\n   - Provide examples of both reading and writing the new format\n   - Include migration notes for handling existing journal entries",
        "testStrategy": "1. **Unit Tests for YAML Frontmatter Generation**:\n   - Create test cases with various git contexts and verify the generated YAML frontmatter contains all required fields\n   - Test with edge cases like empty commit messages, binary files, etc.\n   - Verify timestamp format conforms to ISO 8601\n\n2. **Backward Compatibility Tests**:\n   - Test the parser with both new format entries and legacy entries\n   - Verify that legacy entries are correctly handled without metadata\n   - Ensure malformed YAML is gracefully handled\n\n3. **Integration Tests**:\n   - Test the complete journal generation pipeline with the new format\n   - Verify that git hooks correctly generate entries with proper frontmatter\n   - Check that entries are correctly stored in the journal file\n\n4. **Validation Tests**:\n   - Create a validation script that checks all generated entries against the schema\n   - Verify that all required fields are present and correctly formatted\n   - Test with a variety of commit types to ensure proper entry_type detection\n\n5. **AI Parsing Test**:\n   - Create a simple script that uses the structured data for analysis\n   - Verify that metadata can be easily extracted and processed\n   - Test aggregation of entries by tags, types, and semantic topics\n\n6. **Manual Review**:\n   - Visually inspect generated journal entries to ensure they remain human-readable\n   - Verify that markdown formatting in the content section is preserved\n   - Check that the YAML frontmatter is properly delimited and doesn't interfere with content rendering",
        "status": "pending",
        "dependencies": [
          50
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 53,
        "title": "Refactor Daily Summary Generation",
        "description": "Refactor existing daily summary generation from MCP-based to background, non-MCP generation using the same standalone approach as journal entries.",
        "status": "pending",
        "dependencies": [
          50
        ],
        "priority": "high",
        "details": "This task refactors the existing daily summary functionality in `src/mcp_commit_story/daily_summary.py` to use the new background generation pattern instead of MCP tools:\n\n1. **Refactor Existing Daily Summary Generator Function**:\n```python\ndef generate_daily_summary_standalone(date=None, git_context=None):\n    \"\"\"\n    Generate daily summaries without requiring MCP server or signals.\n    \n    Args:\n        date: Optional date to generate summary for (defaults to today)\n        git_context: Optional git context dictionary (if None, will be collected)\n        \n    Returns:\n        dict: Generated summary data\n    \"\"\"\n    # Set default date to today if not provided\n    if date is None:\n        date = datetime.now().strftime(\"%Y-%m-%d\")\n        \n    # Collect git context if not provided\n    if git_context is None:\n        git_context = collect_git_context()\n    \n    # Reuse existing function to collect journal entries for the specified date\n    journal_entries = collect_recent_journal_entries(date)\n    \n    # Reuse existing prompt preparation logic\n    prompt = prepare_daily_summary_prompt(date, journal_entries, git_context)\n    \n    # Generate summary using AI (similar pattern to journal generator)\n    summary_content = generate_ai_content(\n        prompt=prompt,\n        model_config=get_model_config(\"daily_summary\")\n    )\n    \n    # Reuse existing formatting and saving functions\n    formatted_summary = format_daily_summary(summary_content, date)\n    save_daily_summary(formatted_summary, date)\n    \n    return formatted_summary\n```\n\n2. **Adapt Existing Helper Functions**:\n- Identify all helper functions in `daily_summary.py` that currently depend on MCP\n- Refactor these functions to work without MCP dependencies while preserving their functionality\n- Ensure the refactored functions maintain the same interfaces and return values\n\n3. **Reuse Existing Git Hook Integration**:\n- Locate the existing summary trigger logic in git hooks\n- Update the hooks to call the standalone version instead of the MCP-based version\n- Preserve all existing trigger conditions and logic\n\n4. **Update Configuration**:\n```python\ndef get_model_config(generation_type):\n    \"\"\"Get the appropriate model configuration based on generation type\"\"\"\n    # Return different configurations for journal vs. summary generation\n    # Reuse existing configuration values where appropriate\n```\n\n5. **Remove MCP Dependencies**:\n- Identify and remove all MCP-dependent code in the current daily summary generation\n- Ensure all necessary context is collected directly without MCP signals\n- Maintain the same output format and quality as the MCP-based version\n\n6. **Preserve Existing Scheduling Logic**:\n- Locate and reuse the existing logic that determines when summaries should be generated\n- Ensure this logic continues to work correctly with the standalone approach\n\nThe implementation should follow the same patterns established in the standalone journal generator (Task 50), adapting them specifically for daily summaries. This refactoring completes the transition of all AI generation functionality to background processes that don't require the MCP server while preserving all existing daily summary capabilities.",
        "testStrategy": "To verify the correct implementation of the daily summary refactoring:\n\n1. **Unit Testing**:\n   - Create unit tests for each refactored helper function to ensure they work without MCP\n   - Test the main generate_daily_summary_standalone function with mocked dependencies\n   - Verify proper error handling for missing journal entries or AI generation failures\n\n2. **Integration Testing**:\n   - Test the complete flow from git hook trigger to summary generation\n   - Verify that summaries are generated correctly with the same quality as the MCP version\n   - Test with various date inputs and journal entry scenarios\n\n3. **Comparison Testing**:\n   - Generate summaries using both the old MCP-based approach and the new standalone approach\n   - Compare outputs to ensure consistency and quality\n   - Document any differences and ensure they're acceptable or improvements\n\n4. **Git Hook Testing**:\n   - Verify that existing git hooks correctly trigger the refactored summary generation\n   - Test with different git operations (commit, push, etc.)\n   - Ensure hooks don't significantly impact git operation performance\n\n5. **Offline Testing**:\n   - Verify summaries can be generated without MCP server running\n   - Test in environments with limited connectivity\n   - Ensure all necessary context is collected directly\n\n6. **Manual Review**:\n   - Manually review several generated summaries for quality and completeness\n   - Compare with previous MCP-generated summaries\n   - Ensure the refactored implementation produces equivalent or better results\n\n7. **Edge Case Testing**:\n   - Test with empty journal entries\n   - Test with very large numbers of journal entries\n   - Test with unusual date ranges or formats\n\n8. **Regression Testing**:\n   - Verify that all existing functionality continues to work as expected\n   - Ensure no regressions in summary quality or format\n   - Test all edge cases that were previously handled correctly\n\n9. **Performance Testing**:\n   - Measure and compare generation time with the MCP-based approach\n   - Ensure the background process doesn't consume excessive resources\n   - Verify that git operations remain responsive during summary generation",
        "subtasks": []
      },
      {
        "id": 54,
        "title": "Clean Up Obsolete Signal-Based Architecture Code",
        "description": "Remove all obsolete code, files, and references from the old signal-based architecture after MVP functionality is complete with the new direct git hook → standalone generator architecture.",
        "details": "This cleanup task should systematically remove all components of the abandoned signal-based approach:\n\n1. **Signal File Handling**:\n   - Remove all signal file creation, reading, and processing functions\n   - Delete any signal file templates and related resources\n   - Remove signal file path configurations and constants\n\n2. **MCP Tool Signaling Mechanisms**:\n   - Remove MCP signal handling code in the server\n   - Delete signal-based MCP tools and their registration functions\n   - Clean up any signal-based command processing logic\n\n3. **File Watcher Patterns**:\n   - Remove file watchers specifically designed for signal files\n   - Clean up any event handlers tied to signal file detection\n   - Delete configuration related to signal file monitoring\n\n4. **Code Cleanup**:\n   - Update imports to remove references to signal modules\n   - Remove unused functions and classes related to signaling\n   - Clean up test files that were testing signal functionality\n   - Remove signal creation/processing logic throughout the codebase\n\n5. **Directory and File Cleanup**:\n   - Delete signal file directories if they're no longer needed\n   - Remove any temporary or cache directories used for signal processing\n   - Update documentation to remove references to the signal architecture\n\n6. **Configuration Cleanup**:\n   - Remove signal-related configuration options\n   - Update default configurations to reflect the new architecture\n   - Clean up environment variables related to signal processing\n\n7. **Dependency Updates**:\n   - Remove any external dependencies only used for signal processing\n   - Update requirements files to remove unnecessary packages\n\nThe goal is to have a clean codebase that only contains code supporting the new direct git hook → standalone generator architecture, improving maintainability and reducing confusion for future development.",
        "testStrategy": "To verify the successful completion of this cleanup task:\n\n1. **Static Code Analysis**:\n   - Run a grep or similar tool to search for signal-related terms across the codebase\n   - Verify no references to signal files, signal processing, or signal directories remain\n   - Check for any orphaned imports or unused functions\n\n2. **Functional Testing**:\n   - Verify all core functionality works without the signal architecture:\n     - Journal generation triggers correctly from git hooks\n     - Daily summaries generate properly using the standalone approach\n     - All data collection works through direct database access\n\n3. **Configuration Validation**:\n   - Ensure configuration files don't contain obsolete signal-related settings\n   - Verify environment variables documentation is updated\n   - Check that default configurations are appropriate for the new architecture\n\n4. **Import Analysis**:\n   - Run an import analyzer to ensure no imports reference deleted modules\n   - Verify no circular dependencies were created during cleanup\n\n5. **Test Suite Execution**:\n   - Run the full test suite to ensure no functionality was broken\n   - Verify tests don't reference signal-based functionality\n   - Ensure test coverage remains high for core functionality\n\n6. **Code Size Metrics**:\n   - Compare codebase size before and after cleanup\n   - Document the reduction in lines of code and number of files\n\n7. **Documentation Review**:\n   - Verify all documentation is updated to reflect the new architecture\n   - Ensure no references to signals remain in comments or docstrings\n\n8. **Git Hook Verification**:\n   - Test git hooks to ensure they properly trigger journal and summary generation\n   - Verify hooks don't attempt to use any signal-based functionality",
        "status": "pending",
        "dependencies": [
          53
        ],
        "priority": "medium",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-06-21T07:52:11.612Z",
      "updated": "2025-06-27T05:31:50.669Z",
      "description": "Tasks for master context"
    }
  }
}