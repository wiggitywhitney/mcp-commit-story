{
  "tasks": [
    {
      "id": 7,
      "title": "Implement CLI Interface",
      "description": "Create the command-line interface using Click to provide setup functionality for the journal. This is a necessary foundation component for the MVP and other tasks.",
      "status": "pending",
      "dependencies": [],
      "priority": "medium",
      "details": "Implement the CLI interface in `src/mcp_journal/cli.py` with the following features:\n\n1. CLI setup:\n```python\nimport click\n\n@click.group()\ndef cli():\n    \"\"\"MCP Journal - Engineering journal for Git repositories\"\"\"\n    pass\n```\n\n2. Setup command implementations:\n```python\n@cli.command()\n@click.option(\"--debug\", is_flag=True, help=\"Show debug information\")\ndef journal_init(debug):\n    \"\"\"Initialize journal in current repository\"\"\"\n    # Implementation\n\n@cli.command()\n@click.option(\"--debug\", is_flag=True, help=\"Show debug information\")\ndef install_hook(debug):\n    \"\"\"Install Git hook to connect with MCP server\"\"\"\n    # Implementation\n```\n\n3. Global options:\n```python\n@click.option(\"--config\", help=\"Override config file location\")\n@click.option(\"--dry-run\", is_flag=True, help=\"Preview operations without writing files\")\n@click.option(\"--verbose\", is_flag=True, help=\"Detailed output for debugging\")\n```\n\n4. Main entry point:\n```python\ndef main():\n    \"\"\"Main entry point for CLI\"\"\"\n    try:\n        cli()\n    except Exception as e:\n        click.echo(f\"Error: {e}\", err=True)\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n```\n\nNote: This CLI is focused primarily on setup commands (journal-init, install-hook), but is a necessary foundation for the MVP as it's a blocking dependency for tasks 11, 12, 13, and 15, and has subtasks from MVP Task 9 that require CLI functionality. Most operational tasks (journal entry creation, reflection addition, summarization, etc.) are handled by the MCP server and AI agents, not through this CLI.",
      "testStrategy": "1. Unit tests for setup CLI commands (journal-init, install-hook)\n2. Tests for command options and arguments\n3. Tests for error handling\n4. Tests for global options\n5. Integration tests for setup commands\n6. Tests for exit codes and error messages",
      "subtasks": []
    },
    {
      "id": 11,
      "title": "Implement Summary Generation",
      "description": "Create the functionality to generate daily, weekly, monthly, quarterly, and yearly summaries of journal entries, with special emphasis on manual reflections.",
      "status": "pending",
      "dependencies": [
        7
      ],
      "priority": "medium",
      "details": "Implement summary generation in the MCP server with the following features:\n\n1. Date range utilities:\n```python\ndef get_date_range(period, date=None):\n    \"\"\"Get start and end dates for a period\"\"\"\n    if date is None:\n        date = datetime.now().date()\n    elif isinstance(date, str):\n        date = datetime.strptime(date, \"%Y-%m-%d\").date()\n    \n    if period == \"day\":\n        return date, date\n    elif period == \"week\":\n        # Start of week (Monday)\n        start = date - timedelta(days=date.weekday())\n        end = start + timedelta(days=6)\n        return start, end\n    elif period == \"month\":\n        start = date.replace(day=1)\n        # Last day of month\n        next_month = date.replace(day=28) + timedelta(days=4)\n        end = next_month - timedelta(days=next_month.day)\n        return start, end\n    elif period == \"quarter\":\n        # Determine which quarter the date falls in\n        quarter = (date.month - 1) // 3 + 1\n        # Start of quarter (first day of first month in quarter)\n        start_month = (quarter - 1) * 3 + 1\n        start = date.replace(month=start_month, day=1)\n        # End of quarter (last day of last month in quarter)\n        end_month = quarter * 3\n        end_day = 31 if end_month in [3, 12] else 30 if end_month in [6, 9] else 28\n        if end_month == 2 and (date.year % 4 == 0 and (date.year % 100 != 0 or date.year % 400 == 0)):\n            end_day = 29  # Leap year\n        end = date.replace(month=end_month, day=end_day)\n        return start, end\n    elif period == \"year\":\n        start = date.replace(month=1, day=1)\n        end = date.replace(month=12, day=31)\n        return start, end\n    else:\n        raise ValueError(f\"Unknown period: {period}\")\n```\n\n2. Journal file collection:\n```python\ndef get_journal_files_in_range(start_date, end_date, config):\n    \"\"\"Get journal files in date range\"\"\"\n    files = []\n    current = start_date\n    while current <= end_date:\n        file_path = Path(config[\"journal\"][\"path\"]) / \"daily\" / f\"{current.strftime('%Y-%m-%d')}.md\"\n        if file_path.exists():\n            files.append(file_path)\n        current += timedelta(days=1)\n    return files\n```\n\n3. Summary generation with manual reflection prioritization:\n```python\ndef generate_summary(files, period, config):\n    \"\"\"Generate summary from journal files\"\"\"\n    # Extract content from files\n    entries = []\n    manual_reflections = []\n    \n    for file_path in files:\n        with open(file_path, \"r\") as f:\n            content = f.read()\n            # Extract entries and reflections\n            # Extract manual reflections from special sections\n            reflection_sections = extract_manual_reflections(content, file_path.stem)\n            if reflection_sections:\n                manual_reflections.extend(reflection_sections)\n            # Extract regular entries\n            # Implementation\n    \n    # Analyze entries for significance/complexity\n    weighted_entries = []\n    for entry in entries:\n        # Determine entry significance based on factors like:\n        # - Length/detail of the entry\n        # - Presence of technical terms or complex concepts\n        # - Keywords indicating substantial work (\"implemented\", \"designed\", \"solved\")\n        # - Absence of trivial indicators (\"minor fix\", \"typo\", \"small change\")\n        significance_score = calculate_entry_significance(entry)\n        weighted_entries.append((entry, significance_score))\n    \n    # Sort entries by significance score to prioritize important work\n    weighted_entries.sort(key=lambda x: x[1], reverse=True)\n    \n    # Generate summary sections\n    summary = []\n    \n    # Add manual reflections section first - always prioritized\n    if manual_reflections:\n        summary.append(\"# 📝 Manual Reflections\\n\")\n        summary.append(\"*These are your own reflections from the period, presented verbatim.*\\n\")\n        formatted_reflections = []\n        for date, reflection in manual_reflections:\n            formatted_reflections.append(f\"## {date}\\n\\n{reflection}\\n\")\n        summary.append(\"\\n\".join(formatted_reflections))\n    \n    # Add other sections\n    summary.append(\"# Summary\\n\")\n    # Generate overall summary with emphasis on significant entries\n    \n    summary.append(\"# Key Accomplishments\\n\")\n    # Extract accomplishments, prioritizing substantial work\n    \n    summary.append(\"# Challenges\\n\")\n    # Extract challenges, focusing on complex problems\n    \n    summary.append(\"# Technical Decisions\\n\")\n    # Extract decisions, highlighting important architectural choices\n    \n    return \"\\n\\n\".join(summary)\n\ndef extract_manual_reflections(content, date_str):\n    \"\"\"Extract manual reflections from journal content\"\"\"\n    reflections = []\n    \n    # Look for reflection sections with patterns like:\n    # ## Reflection\n    # ## Daily Reflection\n    # ## Personal Reflection\n    # etc.\n    \n    reflection_patterns = [\n        r\"#+\\s*(?:Daily\\s*)?Reflection[s]?\\s*\\n([\\s\\S]*?)(?:\\n#+\\s|$)\",\n        r\"#+\\s*(?:Personal\\s*)?Thought[s]?\\s*\\n([\\s\\S]*?)(?:\\n#+\\s|$)\",\n        r\"#+\\s*(?:Manual\\s*)?Note[s]?\\s*\\n([\\s\\S]*?)(?:\\n#+\\s|$)\"\n    ]\n    \n    for pattern in reflection_patterns:\n        matches = re.finditer(pattern, content, re.MULTILINE)\n        for match in matches:\n            reflection_text = match.group(1).strip()\n            if reflection_text:  # Only add non-empty reflections\n                reflections.append((date_str, reflection_text))\n    \n    return reflections\n\ndef calculate_entry_significance(entry):\n    \"\"\"Calculate significance score for an entry to prioritize substantial work\"\"\"\n    score = 0\n    \n    # Base score from length (longer entries often indicate more substantial work)\n    score += min(len(entry) / 100, 5)  # Cap at 5 points for length\n    \n    # Keywords indicating substantial work\n    substantial_indicators = [\n        \"implement\", \"design\", \"architecture\", \"refactor\", \"optimize\", \n        \"solve\", \"complex\", \"challenge\", \"significant\", \"major\"\n    ]\n    \n    # Keywords indicating trivial work\n    trivial_indicators = [\n        \"typo\", \"minor fix\", \"small change\", \"tweak\", \"trivial\", \n        \"cosmetic\", \"rename\", \"formatting\"\n    ]\n    \n    # Add points for substantial work indicators\n    for word in substantial_indicators:\n        if word in entry.lower():\n            score += 2\n    \n    # Subtract points for trivial work indicators\n    for word in trivial_indicators:\n        if word in entry.lower():\n            score -= 1.5\n    \n    # Analyze for technical complexity\n    # (This could be enhanced with more sophisticated NLP in the future)\n    technical_terms = [\"algorithm\", \"database\", \"architecture\", \"performance\", \"security\"]\n    for term in technical_terms:\n        if term in entry.lower():\n            score += 1\n    \n    return max(score, 0)  # Ensure score doesn't go negative\n```\n\n4. Summary file saving:\n```python\ndef save_summary(content, period, date, config):\n    \"\"\"Save summary to appropriate file\"\"\"\n    if period == \"day\":\n        file_name = f\"{date.strftime('%Y-%m-%d')}-summary.md\"\n        dir_path = Path(config[\"journal\"][\"path\"]) / \"summaries\" / \"daily\"\n    elif period == \"week\":\n        # Get week number\n        week_num = date.isocalendar()[1]\n        file_name = f\"{date.strftime('%Y-%m')}-week{week_num}.md\"\n        dir_path = Path(config[\"journal\"][\"path\"]) / \"summaries\" / \"weekly\"\n    elif period == \"month\":\n        file_name = f\"{date.strftime('%Y-%m')}.md\"\n        dir_path = Path(config[\"journal\"][\"path\"]) / \"summaries\" / \"monthly\"\n    elif period == \"quarter\":\n        # Determine which quarter the date falls in\n        quarter = (date.month - 1) // 3 + 1\n        file_name = f\"{date.strftime('%Y')}-Q{quarter}.md\"\n        dir_path = Path(config[\"journal\"][\"path\"]) / \"summaries\" / \"quarterly\"\n    elif period == \"year\":\n        file_name = f\"{date.strftime('%Y')}.md\"\n        dir_path = Path(config[\"journal\"][\"path\"]) / \"summaries\" / \"yearly\"\n    else:\n        raise ValueError(f\"Unknown period: {period}\")\n    \n    # Create file path\n    file_path = dir_path / file_name\n    \n    # Ensure directory exists using on-demand directory creation pattern\n    ensure_journal_directory(dir_path)\n    \n    # Save file\n    with open(file_path, \"w\") as f:\n        f.write(content)\n    \n    return file_path\n```\n\n5. MCP handler implementation:\n```python\n@trace_operation(\"journal_summarize\")\nasync def handle_summarize(request):\n    \"\"\"Handle journal/summarize operation\"\"\"\n    period = request.get(\"period\", \"day\")\n    date = request.get(\"date\")\n    date_range = request.get(\"range\")\n    \n    # Load config\n    config = load_config()\n    \n    # Get date range\n    if date_range:\n        # Parse range (format: \"YYYY-MM-DD:YYYY-MM-DD\")\n        start_str, end_str = date_range.split(\":\")\n        start_date = datetime.strptime(start_str, \"%Y-%m-%d\").date()\n        end_date = datetime.strptime(end_str, \"%Y-%m-%d\").date()\n    else:\n        start_date, end_date = get_date_range(period, date)\n    \n    # Get journal files\n    files = get_journal_files_in_range(start_date, end_date, config)\n    if not files:\n        return {\"status\": \"error\", \"error\": \"No journal entries found in date range\"}\n    \n    # Generate summary\n    content = generate_summary(files, period, config)\n    \n    # Save summary\n    file_path = save_summary(content, period, start_date, config)\n    \n    return {\n        \"status\": \"success\",\n        \"file_path\": str(file_path),\n        \"content\": content\n    }\n```\n\n6. Directory creation utility:\n```python\ndef ensure_journal_directory(dir_path):\n    \"\"\"Ensure the journal directory exists, creating it if necessary\"\"\"\n    if not dir_path.exists():\n        dir_path.mkdir(parents=True, exist_ok=True)\n        logger.info(f\"Created directory: {dir_path}\")\n    return dir_path\n```\n\n7. On-demand directory creation pattern:\n- All summary file-writing operations must use the on-demand directory creation pattern\n- Directories should only be created when needed, not upfront\n- All summary-writing functions (including save_summary) must call ensure_journal_directory(file_path) before writing\n- See docs/on-demand-directory-pattern.md for implementation details and test patterns\n\n8. Manual reflection prioritization:\n- Manual reflections must be prominently displayed at the beginning of summaries\n- Use visual distinction (emoji, formatting) to highlight manual reflections\n- Include date context for each reflection\n- Preserve the original wording of manual reflections\n- Implement reflection extraction from common section patterns\n- Ensure manual reflections are always prioritized over inferred content\n\nNote: This implementation focuses solely on MCP/AI agent operations for summary generation. CLI functionality is limited to setup commands (journal-init, install-hook) only. Refer to updated documentation for details.",
      "testStrategy": "1. Unit tests for date range utilities\n   - Test all periods (day, week, month, quarter, year)\n   - Test edge cases like quarter boundaries\n   - Test leap year handling for February in quarterly calculations\n2. Tests for journal file collection\n3. Tests for summary generation\n4. Tests for summary file saving\n   - Test saving for all periods (daily, weekly, monthly, quarterly, yearly)\n   - Test correct file naming for quarterly summaries (YYYY-Q1, YYYY-Q2, etc.)\n5. Tests for MCP handler implementation\n6. Tests for handling different periods (day, week, month, quarter, year)\n7. Tests for handling date ranges\n8. Integration tests for full summary generation flow\n9. Tests for entry significance calculation\n10. Tests to verify that substantial work is properly prioritized in summaries\n11. Tests to verify that trivial entries are de-emphasized in summaries\n12. Tests with mixed entry types to ensure proper weighting in the final summary\n13. Tests for on-demand directory creation:\n    - Test that summary directories are created automatically when they don't exist\n    - Test that ensure_journal_directory() is called for all summary types (daily, weekly, monthly, quarterly, yearly)\n    - Test that directory creation works with nested paths\n    - Test that no errors occur when directories already exist\n    - Test that directories are only created when needed, not upfront\n    - Verify that all summary-writing functions call ensure_journal_directory() before writing\n    - Follow test patterns described in docs/on-demand-directory-pattern.md\n14. Tests to verify that summarization is available as an MCP operation\n15. Tests to verify that the AI agent can properly interact with the summarization functionality\n16. Verify that summary generation works correctly through the MCP interface only (not CLI)\n17. Test that the AI agent can request summaries for different time periods and date ranges\n18. Tests for manual reflection extraction:\n    - Test extraction from various section formats (## Reflection, ## Daily Reflection, etc.)\n    - Test with multiple reflection sections in a single file\n    - Test with reflection sections containing various formatting (lists, code blocks, etc.)\n    - Test with empty reflection sections\n    - Test with reflection sections at different positions in the file\n19. Tests for manual reflection prioritization:\n    - Verify that manual reflections appear at the beginning of summaries\n    - Verify that manual reflections are visually distinguished\n    - Verify that date context is included for each reflection\n    - Verify that original wording is preserved\n    - Test with mixed content (manual reflections and regular entries)\n    - Test with only manual reflections\n    - Test with no manual reflections\n20. Tests for quarterly summary generation:\n    - Test correct date range calculation for each quarter\n    - Test correct file naming (YYYY-Q1, YYYY-Q2, etc.)\n    - Test with entries spanning multiple months within a quarter\n    - Test with entries at quarter boundaries",
      "subtasks": [
        {
          "id": "11.1",
          "title": "Implement entry significance calculation",
          "description": "Create the algorithm to analyze journal entries and assign significance scores based on content analysis.",
          "status": "pending"
        },
        {
          "id": "11.2",
          "title": "Modify summary generation to prioritize significant entries",
          "description": "Update the summary generation logic to give more narrative weight to entries with higher significance scores.",
          "status": "pending"
        },
        {
          "id": "11.3",
          "title": "Create test cases for entry significance calculation",
          "description": "Develop test cases with various types of entries (substantial, trivial, mixed) to verify proper significance scoring.",
          "status": "pending"
        },
        {
          "id": "11.4",
          "title": "Test summary prioritization with real-world examples",
          "description": "Test the summary generation with a set of real-world journal entries to ensure meaningful work is properly highlighted.",
          "status": "pending"
        },
        {
          "id": "11.5",
          "title": "Implement ensure_journal_directory utility",
          "description": "Create the utility function to ensure journal directories exist, creating them on-demand if necessary.",
          "status": "pending"
        },
        {
          "id": "11.6",
          "title": "Update save_summary to use ensure_journal_directory",
          "description": "Modify the save_summary function to use the ensure_journal_directory utility for all summary types.",
          "status": "pending"
        },
        {
          "id": "11.7",
          "title": "Add tests for directory creation functionality",
          "description": "Create tests to verify that summary directories are created automatically when they don't exist and that the ensure_journal_directory utility works correctly.",
          "status": "pending"
        },
        {
          "id": "11.8",
          "title": "Implement on-demand directory creation pattern",
          "description": "Update all summary file-writing operations to follow the on-demand directory creation pattern as described in docs/on-demand-directory-pattern.md.",
          "status": "pending"
        },
        {
          "id": "11.9",
          "title": "Add tests for on-demand directory creation",
          "description": "Create tests to verify that directories are only created when needed, not upfront, and that all summary-writing functions call ensure_journal_directory() before writing.",
          "status": "pending"
        },
        {
          "id": "11.10",
          "title": "Review and update all file-writing operations",
          "description": "Review all file-writing operations in the codebase to ensure they follow the on-demand directory creation pattern.",
          "status": "pending"
        },
        {
          "id": "11.11",
          "title": "Verify MCP operation for summarization",
          "description": "Ensure that summarization is properly implemented as an MCP operation and accessible to the AI agent.",
          "status": "pending"
        },
        {
          "id": "11.12",
          "title": "Test AI agent interaction with summarization",
          "description": "Create tests to verify that the AI agent can properly request and process summary generation through the MCP server.",
          "status": "pending"
        },
        {
          "id": "11.13",
          "title": "Ensure summary generation is MCP-only",
          "description": "Verify that summary generation functionality is only available through the MCP interface and not through CLI commands.",
          "status": "pending"
        },
        {
          "id": "11.14",
          "title": "Update documentation to reflect MCP-only approach",
          "description": "Update relevant documentation to clarify that summary generation is only available through the MCP/AI agent interface, not through CLI commands.",
          "status": "pending"
        },
        {
          "id": "11.15",
          "title": "Implement manual reflection extraction",
          "description": "Create functionality to extract manual reflections from journal entries using pattern matching for common section headers.",
          "status": "pending"
        },
        {
          "id": "11.16",
          "title": "Implement manual reflection prioritization in summaries",
          "description": "Update summary generation to display manual reflections prominently at the beginning with visual distinction and date context.",
          "status": "pending"
        },
        {
          "id": "11.17",
          "title": "Add tests for manual reflection extraction",
          "description": "Create tests to verify that manual reflections are correctly extracted from various section formats and positions.",
          "status": "pending"
        },
        {
          "id": "11.18",
          "title": "Add tests for manual reflection prioritization",
          "description": "Create tests to verify that manual reflections appear at the beginning of summaries with proper visual distinction and preserved wording.",
          "status": "pending"
        },
        {
          "id": "11.19",
          "title": "Implement quarterly summary support",
          "description": "Add support for generating quarterly summaries, including date range calculation and file naming conventions.",
          "status": "pending"
        },
        {
          "id": "11.20",
          "title": "Create tests for quarterly summary generation",
          "description": "Develop tests to verify correct date range calculation, file naming, and content generation for quarterly summaries.",
          "status": "pending"
        },
        {
          "id": "11.21",
          "title": "Update documentation to include quarterly summaries",
          "description": "Update relevant documentation to include information about quarterly summary generation and usage.",
          "status": "pending"
        }
      ]
    },
    {
      "id": 12,
      "title": "Implement Blog Post Generation",
      "description": "Create the functionality to convert journal entries and summaries into blog post format for storytelling.",
      "details": "Implement blog post generation in both the MCP server and CLI with the following features:\n\n1. Blog post generation:\n```python\ndef generate_blog_post(files, config):\n    \"\"\"Generate blog post from journal files\"\"\"\n    # Extract content from files\n    entries = []\n    \n    for file_path in files:\n        with open(file_path, \"r\") as f:\n            content = f.read()\n            # Extract entries\n            # Implementation\n    \n    # Generate blog post sections\n    blog_post = []\n    \n    # Add title and introduction\n    blog_post.append(\"# Project Journey: From Idea to Implementation\\n\")\n    blog_post.append(\"*An engineering story based on journal entries*\\n\")\n    \n    # Add narrative sections\n    blog_post.append(\"## The Challenge\\n\")\n    # Generate challenge narrative\n    \n    blog_post.append(\"## The Approach\\n\")\n    # Generate approach narrative\n    \n    blog_post.append(\"## Key Decisions\\n\")\n    # Extract and narrate decisions\n    \n    blog_post.append(\"## Lessons Learned\\n\")\n    # Extract and narrate lessons\n    \n    blog_post.append(\"## Conclusion\\n\")\n    # Generate conclusion\n    \n    return \"\\n\\n\".join(blog_post)\n```\n\n2. Blog post file saving:\n```python\ndef save_blog_post(content, title, config):\n    \"\"\"Save blog post to file\"\"\"\n    # Create directory if needed\n    dir_path = Path(config[\"journal\"][\"path\"]) / \"blog_posts\"\n    dir_path.mkdir(parents=True, exist_ok=True)\n    \n    # Generate file name from title\n    file_name = title.lower().replace(\" \", \"-\") + \".md\"\n    file_path = dir_path / file_name\n    \n    # Save file\n    with open(file_path, \"w\") as f:\n        f.write(content)\n    \n    return file_path\n```\n\n3. MCP handler implementation:\n```python\n@trace_operation(\"journal_blogify\")\nasync def handle_blogify(request):\n    \"\"\"Handle journal/blogify operation\"\"\"\n    files = request.get(\"files\", [])\n    title = request.get(\"title\", \"Engineering Journey\")\n    \n    if not files:\n        return {\"status\": \"error\", \"error\": \"No files provided\"}\n    \n    # Load config\n    config = load_config()\n    \n    # Convert file paths to Path objects\n    file_paths = [Path(f) for f in files]\n    \n    # Check if files exist\n    missing = [str(f) for f in file_paths if not f.exists()]\n    if missing:\n        return {\"status\": \"error\", \"error\": f\"Files not found: {', '.join(missing)}\"}\n    \n    # Generate blog post\n    content = generate_blog_post(file_paths, config)\n    \n    # Save blog post\n    file_path = save_blog_post(content, title, config)\n    \n    return {\n        \"status\": \"success\",\n        \"file_path\": str(file_path),\n        \"content\": content\n    }\n```\n\n4. CLI command implementation:\n```python\n@cli.command()\n@click.argument(\"files\", nargs=-1, type=click.Path(exists=True))\n@click.option(\"--title\", default=\"Engineering Journey\", help=\"Blog post title\")\n@click.option(\"--debug\", is_flag=True, help=\"Show debug information\")\ndef blogify(files, title, debug):\n    \"\"\"Convert journal entries to blog post\"\"\"\n    try:\n        if not files:\n            click.echo(\"No files provided\")\n            return\n        \n        # Load config\n        config = load_config()\n        \n        # Convert file paths to Path objects\n        file_paths = [Path(f) for f in files]\n        \n        # Generate blog post\n        content = generate_blog_post(file_paths, config)\n        \n        # Save blog post\n        file_path = save_blog_post(content, title, config)\n        \n        click.echo(f\"Blog post saved to {file_path}\")\n    except Exception as e:\n        if debug:\n            click.echo(f\"Error: {e}\")\n            traceback.print_exc()\n        else:\n            click.echo(f\"Error: {e}\")\n```",
      "testStrategy": "1. Unit tests for blog post generation\n2. Tests for blog post file saving\n3. Tests for MCP handler implementation\n4. Tests for CLI command implementation\n5. Tests for handling multiple input files\n6. Tests for narrative generation\n7. Integration tests for full blog post generation flow",
      "priority": "low",
      "dependencies": [
        7
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 13,
      "title": "Implement Backfill Functionality",
      "description": "Create the functionality to detect and generate journal entries for missed commits.",
      "details": "Implement backfill functionality in both the MCP server and CLI with the following features:\n\n1. Missed commit detection:\n```python\ndef get_missed_commits(repo, config):\n    \"\"\"Get commits that don't have journal entries\"\"\"\n    # Get journal directory\n    journal_path = Path(config[\"journal\"][\"path\"])\n    \n    # Get all commits\n    commits = list(repo.iter_commits())\n    \n    # Get all journal files\n    journal_files = list(journal_path.glob(\"daily/*.md\"))\n    \n    # Extract commit hashes from journal files\n    journal_commits = set()\n    for file_path in journal_files:\n        with open(file_path, \"r\") as f:\n            content = f.read()\n            # Extract commit hashes using regex\n            # Implementation\n    \n    # Find commits not in journal\n    missed_commits = []\n    for commit in commits:\n        if commit.hexsha not in journal_commits and not is_journal_only_commit(commit, config[\"journal\"][\"path\"]):\n            missed_commits.append(commit)\n    \n    return missed_commits\n```\n\n2. Backfill processing:\n```python\ndef process_backfill(commits, config, debug=False):\n    \"\"\"Process backfill for missed commits\"\"\"\n    results = []\n    \n    # Sort commits by date\n    commits.sort(key=lambda c: c.committed_date)\n    \n    for commit in commits:\n        # Generate entry\n        entry = generate_journal_entry(commit, config, debug)\n        if not entry:\n            continue\n        \n        # Mark as backfilled\n        entry.is_backfilled = True\n        \n        # Save entry\n        file_path = save_journal_entry(entry, config)\n        \n        results.append({\n            \"commit\": commit.hexsha,\n            \"file_path\": str(file_path)\n        })\n    \n    return results\n```\n\n3. MCP handler implementation:\n```python\n@trace_operation(\"journal_backfill\")\nasync def handle_backfill(request):\n    \"\"\"Handle journal/backfill operation\"\"\"\n    debug = request.get(\"debug\", False)\n    \n    # Load config\n    config = load_config()\n    \n    # Get repo\n    repo = get_repo()\n    \n    # Get missed commits\n    missed_commits = get_missed_commits(repo, config)\n    if not missed_commits:\n        return {\"status\": \"success\", \"message\": \"No missed commits found\"}\n    \n    # Process backfill\n    results = process_backfill(missed_commits, config, debug)\n    \n    return {\n        \"status\": \"success\",\n        \"count\": len(results),\n        \"entries\": results\n    }\n```\n\n4. CLI command implementation:\n```python\n@cli.command()\n@click.option(\"--debug\", is_flag=True, help=\"Show debug information\")\ndef backfill(debug):\n    \"\"\"Check for missed commits and create entries\"\"\"\n    try:\n        # Load config\n        config = load_config()\n        \n        # Get repo\n        repo = get_repo()\n        \n        # Get missed commits\n        missed_commits = get_missed_commits(repo, config)\n        if not missed_commits:\n            click.echo(\"No missed commits found\")\n            return\n        \n        # Process backfill\n        results = process_backfill(missed_commits, config, debug)\n        \n        click.echo(f\"Created {len(results)} journal entries for missed commits\")\n        for result in results:\n            click.echo(f\"  - {result['commit'][:8]}: {result['file_path']}\")\n    except Exception as e:\n        if debug:\n            click.echo(f\"Error: {e}\")\n            traceback.print_exc()\n        else:\n            click.echo(f\"Error: {e}\")\n```",
      "testStrategy": "1. Unit tests for missed commit detection\n2. Tests for backfill processing\n3. Tests for MCP handler implementation\n4. Tests for CLI command implementation\n5. Tests for handling journal-only commits\n6. Tests for chronological ordering of backfilled entries\n7. Integration tests for full backfill flow",
      "priority": "medium",
      "dependencies": [
        7
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 15,
      "title": "Create Comprehensive Tests and Documentation",
      "description": "Develop comprehensive tests for all components and create documentation for the project.",
      "status": "pending",
      "dependencies": [
        7,
        11,
        12,
        13
      ],
      "priority": "high",
      "details": "Create comprehensive tests and documentation with the following features:\n\n1. Test fixtures:\n```python\n@pytest.fixture\ndef mock_git_repo():\n    \"\"\"Create temporary git repo with test commits\"\"\"\n    # Implementation\n\n@pytest.fixture\ndef sample_journal_entries():\n    \"\"\"Load sample journal files\"\"\"\n    # Implementation\n\n@pytest.fixture\ndef mock_terminal_history():\n    \"\"\"Provide test terminal command history\"\"\"\n    # Implementation\n\n@pytest.fixture\ndef mock_chat_history():\n    \"\"\"Provide test chat history\"\"\"\n    # Implementation\n\n@pytest.fixture\ndef mock_telemetry_exporter():\n    \"\"\"Provide a test exporter that captures telemetry events\"\"\"\n    # Implementation\n```\n\n2. Unit tests:\n```python\ndef test_config_loading():\n    \"\"\"Test configuration loading\"\"\"\n    # Implementation\n\ndef test_git_utils():\n    \"\"\"Test git utilities\"\"\"\n    # Implementation\n\ndef test_journal_entry_generation():\n    \"\"\"Test journal entry generation\"\"\"\n    # Implementation\n\ndef test_telemetry():\n    \"\"\"Test telemetry integration\"\"\"\n    # Implementation\n\n# Additional unit tests for all components\n```\n\n3. Integration tests:\n```python\ndef test_cli_init():\n    \"\"\"Test CLI init command\"\"\"\n    # Implementation\n\ndef test_cli_new_entry():\n    \"\"\"Test CLI new-entry command\"\"\"\n    # Implementation\n\ndef test_mcp_server():\n    \"\"\"Test MCP server operations\"\"\"\n    # Implementation\n\n# Additional integration tests for all workflows\n```\n\n4. Documentation:\n   - README.md with project overview, installation, and usage\n   - Configuration documentation\n   - CLI command reference\n   - MCP server API reference\n   - Development guide\n   - Examples and tutorials\n\n5. Test coverage:\n   - Configure pytest-cov for coverage reporting\n   - Ensure >90% test coverage\n   - Add coverage reporting to CI pipeline\n\n6. Documentation structure:\n```\nREADME.md\ndocs/\n├── configuration.md\n├── cli.md\n├── mcp-server.md\n├── development.md\n└── examples/\n    ├── basic-usage.md\n    ├── custom-configuration.md\n    └── integration-examples.md\n```",
      "testStrategy": "1. Verify test coverage meets >90% threshold\n2. Ensure all components have unit tests\n3. Verify integration tests cover all workflows\n4. Test documentation for accuracy and completeness\n5. Verify examples work as documented\n6. Test installation and usage instructions\n7. Verify CI pipeline runs all tests\n8. Ensure telemetry system is thoroughly tested with both unit and integration tests",
      "subtasks": [
        {
          "id": "15.1",
          "title": "Implement telemetry-specific tests",
          "description": "Create comprehensive tests for the telemetry system implemented in task 4",
          "status": "pending",
          "details": "Develop unit and integration tests for the telemetry infrastructure including:\n1. Test telemetry event generation\n2. Test telemetry data collection\n3. Test telemetry exporters\n4. Test telemetry configuration options\n5. Test telemetry integration with other components"
        },
        {
          "id": "15.2",
          "title": "Document telemetry system",
          "description": "Create documentation for the telemetry system",
          "status": "pending",
          "details": "Add telemetry documentation including:\n1. Overview of telemetry capabilities\n2. Configuration options for telemetry\n3. How to extend telemetry with custom exporters\n4. Privacy considerations\n5. Add a telemetry.md file to the docs directory"
        }
      ]
    },
    {
      "id": 19,
      "title": "Document MCP Server Configuration and Integration",
      "description": "Ensure the MCP server launch/discovery/configuration requirements are documented in the PRD, README, and codebase. The MCP server must be launchable as a standalone process, expose the required journal operations, and be discoverable by compatible clients. The method for launching the MCP server is not prescribed; it may be started via CLI, Python entry point, etc.",
      "details": "",
      "testStrategy": "",
      "status": "pending",
      "dependencies": [],
      "priority": "medium",
      "subtasks": [
        {
          "id": 1,
          "title": "Provide generic client/editor config block example",
          "description": "Add a JSON example of a configuration block for connecting to the MCP server, showing command, args, and optional env vars.",
          "details": "",
          "status": "pending",
          "dependencies": [],
          "parentTaskId": 19
        },
        {
          "id": 2,
          "title": "Clarify API key/env var requirements",
          "description": "Document that API keys or environment variables are only required if the underlying SDK or provider needs them, not for all deployments.",
          "details": "",
          "status": "pending",
          "dependencies": [],
          "parentTaskId": 19
        },
        {
          "id": 3,
          "title": "Ensure separation of MCP server config from journal config",
          "description": "Make sure documentation clearly distinguishes between MCP server configuration and the journal system's .mcp-journalrc.yaml.",
          "details": "",
          "status": "pending",
          "dependencies": [],
          "parentTaskId": 19
        },
        {
          "id": 4,
          "title": "Review and update README/docs",
          "description": "Review and update the README.md and other documentation to reflect changes made in this task. Ensure documentation is clear, accurate, and up to date.",
          "details": "",
          "status": "pending",
          "dependencies": [
            "19.1",
            "19.2",
            "19.3"
          ],
          "parentTaskId": 19
        }
      ]
    },
    {
      "id": 21,
      "title": "Integrate Codecov for Test Coverage Reporting",
      "description": "Set up Codecov integration with the GitHub repository to track and report test coverage metrics, culminating in a functional coverage badge in the README.",
      "details": "This task involves establishing a connection between the repository and Codecov to enable automated test coverage reporting. Implementation steps include:\n\n1. Create a Codecov account if not already available and link it to the organization's GitHub account\n2. Add the repository to Codecov's dashboard\n3. Generate a Codecov token for secure communication between CI and Codecov\n4. Update the CI pipeline configuration (GitHub Actions, CircleCI, etc.) to:\n   - Install necessary coverage tools (e.g., pytest-cov for Python)\n   - Run tests with coverage collection enabled\n   - Upload coverage reports to Codecov using the token\n5. Add a `.codecov.yml` configuration file to the repository root to customize coverage settings (thresholds, exclusions, etc.)\n6. Uncomment or add the Codecov badge in the README.md file using the format provided by Codecov\n7. Verify the badge displays the actual coverage percentage after the first successful upload\n\nConsider setting coverage thresholds to maintain code quality and potentially configure PR comments from Codecov to highlight coverage changes in code reviews.",
      "testStrategy": "To verify successful completion of this task:\n\n1. Manually trigger a CI build and confirm the coverage report is generated and uploaded to Codecov\n2. Check the Codecov dashboard to ensure:\n   - The repository appears with correct coverage data\n   - Historical data begins tracking from the first upload\n   - Coverage reports include all relevant files (no critical omissions)\n3. Verify the Codecov badge in the README:\n   - Badge is properly displayed (not broken)\n   - Badge shows an actual percentage value (not \"unknown\" or \"N/A\")\n   - The percentage matches what's shown in the Codecov dashboard\n4. Create a test PR with code changes that would affect coverage (both positively and negatively) to confirm:\n   - Codecov reports the coverage change in the PR\n   - The badge updates accordingly after merging\n5. Document the integration process in the project documentation for future reference\n6. Have another team member verify they can access the Codecov dashboard for the repository",
      "status": "pending",
      "dependencies": [],
      "priority": "medium",
      "subtasks": []
    },
    {
      "id": 22,
      "title": "Implement Remaining MCP Server Handlers",
      "description": "Add the remaining non-MVP MCP tool handlers to complete the full feature set after their backend dependencies are implemented.",
      "status": "pending",
      "dependencies": [
        11,
        12,
        13
      ],
      "priority": "medium",
      "details": "Implement the remaining MCP server tool handlers in `src/mcp_commit_story/server.py` to complete the full feature set:\n\n1. **journal/summarize** handler:\n   - Depends on Task 11 (Summary Generation)\n   - Handle daily, weekly, monthly, yearly summary requests\n   - Return summary content and file paths\n   - Must use on-demand directory creation pattern\n\n2. **journal/blogify** handler:\n   - Depends on Task 12 (Blog Post Generation)\n   - Convert journal entries to blog post format\n   - Accept multiple file inputs\n   - Must use on-demand directory creation pattern\n\n3. **journal/backfill** handler:\n   - Depends on Task 13 (Backfill Functionality)\n   - Detect and create entries for missed commits\n   - Return list of created entries\n   - Must use on-demand directory creation pattern\n\n4. **journal/add-reflection** handler:\n   - Add reflection content to existing journal entries\n   - Accept entry path and reflection content\n   - Must use on-demand directory creation pattern\n\nAll handlers should:\n- Use existing `@handle_mcp_error` decorator\n- Follow TypedDict patterns established in Tasks 6.3-6.4\n- Include proper async/await support\n- Integrate with existing backend logic from their dependency tasks\n- Include comprehensive error handling and validation\n- Call ensure_journal_directory(file_path) before writing any files\n- Never create directories upfront - only on demand when needed\n- Implement as MCP operations only (no CLI commands required)\n- Focus exclusively on MCP/AI agent operations for file-writing handlers\n\nNote that the CLI functionality is limited to setup commands (journal-init, install-hook) only. All file-writing functionality must be implemented as MCP operations. Refer to the updated engineering spec and README.md for implementation details and test patterns.",
      "testStrategy": "1. Unit tests for each new handler\n2. Integration tests with backend logic\n3. Error handling validation\n4. End-to-end workflow testing\n5. Backward compatibility with existing handlers\n6. Verify on-demand directory creation pattern is used correctly\n7. Test that directories are only created when files are actually written\n8. Verify ensure_journal_directory() is called before file writes\n9. Verify all file-writing functionality is accessible via MCP operations only\n10. Test the journal/add-reflection handler functionality as an MCP operation",
      "subtasks": []
    },
    {
      "id": 26,
      "title": "Create Packaging Strategy and Release Process for MVP Launch",
      "description": "Develop a comprehensive packaging and distribution strategy for the MCP Commit Story MVP, including PyPI publishing, version management, installation methods, and release processes.",
      "status": "pending",
      "dependencies": [
        "27",
        28,
        29,
        30
      ],
      "priority": "high",
      "details": "This task involves creating a complete packaging strategy and implementation plan for the MCP Commit Story MVP launch:\n\n1. **Distribution Strategy**:\n   - Set up PyPI package configuration with appropriate metadata in setup.py/pyproject.toml\n   - Implement semantic versioning (MAJOR.MINOR.PATCH) with version tracking in a dedicated file\n   - Configure CI/CD pipeline for automated releases using GitHub Actions or similar\n   - Define package dependencies with appropriate version constraints\n   - Create package structure with proper namespacing\n\n2. **Installation Methods**:\n   - Implement standard pip installation: `pip install mcp-commit-story`\n   - Create development installation process: `pip install -e .` with dev dependencies\n   - Document MCP server deployment options (standalone, Docker, etc.)\n   - Write detailed configuration guides for different environments\n\n3. **Release Process**:\n   - Implement automated version tagging and changelog generation\n   - Create pre-release testing checklist and validation procedures\n   - Set up documentation update workflow tied to releases\n   - Document rollback procedures for failed releases\n   - Establish release branch strategy (e.g., release/v1.0.0)\n   - Integrate with the Release Preparation Script (Task 30)\n\n4. **User Experience Documentation**:\n   - Write comprehensive getting started guide\n   - Create integration examples for VSCode, PyCharm, and command line\n   - Develop troubleshooting guide with common issues and solutions\n   - Set up community support channels (GitHub Discussions, Discord, etc.)\n   - Document the MCP Info Command functionality (Task 29)\n\n5. **Technical Implementation**:\n   - Define package structure with clear entry points\n   - Implement dependency management with compatibility matrices\n   - Create environment testing matrix (OS, Python versions)\n   - Document performance benchmarks and minimum requirements\n   - Ensure journal entry functionality is properly packaged and accessible\n   - Verify proper integration with the File-Based Logging System (Task 28)\n\nImplementation should follow Python packaging best practices and ensure the journal entry creation functionality from Task 9, daily summary git hook trigger from Task 27, logging system from Task 28, info command from Task 29, and release preparation script from Task 30 are all properly exposed and documented in the package.",
      "testStrategy": "To verify the packaging strategy and release process:\n\n1. **Package Structure Testing**:\n   - Validate package structure using `check-manifest`\n   - Verify all necessary files are included in the distribution\n   - Test package installation in a clean virtual environment\n   - Confirm entry points work as expected after installation\n\n2. **Release Process Validation**:\n   - Perform a test release to TestPyPI\n   - Verify version bumping and changelog generation\n   - Test the release automation pipeline with a pre-release version\n   - Validate rollback procedures with a simulated failed release\n   - Test the Release Preparation Script (Task 30) integration\n\n3. **Installation Testing**:\n   - Test pip installation on different operating systems (Windows, macOS, Linux)\n   - Verify development installation for contributors\n   - Test MCP server deployment using the documented methods\n   - Validate configuration options work as described\n\n4. **Documentation Review**:\n   - Conduct user testing with the getting started guide\n   - Review integration examples for accuracy and completeness\n   - Verify troubleshooting documentation addresses common issues\n   - Test community support channels are properly set up\n   - Verify MCP Info Command (Task 29) documentation is accurate\n\n5. **Functionality Testing**:\n   - Verify journal entry creation (from Task 9) works correctly after package installation\n   - Test daily summary git hook trigger (from Task 27) functions properly\n   - Validate the File-Based Logging System (Task 28) works as expected\n   - Test the MCP Info Command (Task 29) functionality\n   - Verify the Release Preparation Script (Task 30) executes correctly\n   - Test all documented features are accessible through the package\n   - Validate performance meets the documented benchmarks\n   - Ensure compatibility with all supported Python versions and environments\n\nThe packaging strategy is considered complete when a test user can successfully install and use the package following only the provided documentation.",
      "subtasks": []
    },
    {
      "id": 27,
      "title": "Implement Daily Summary Git Hook Trigger",
      "description": "Create functionality that automatically generates a daily summary of journal entries from the previous day, triggered by a Git hook when the date changes.",
      "status": "pending",
      "dependencies": [],
      "priority": "high",
      "details": "This task involves implementing an automated daily summary generation system triggered by Git hooks:\n\n1. **Git Hook Implementation**:\n   ```bash\n   #!/bin/bash\n   # post-commit hook to check for date change\n   \n   # Get current date\n   CURRENT_DATE=$(date +%Y-%m-%d)\n   \n   # Get previous date from state file\n   STATE_FILE=\".commit-story-state\"\n   if [ -f \"$STATE_FILE\" ]; then\n     PREV_DATE=$(cat \"$STATE_FILE\")\n   else\n     PREV_DATE=\"\"\n   fi\n   \n   # Update state file with current date\n   echo \"$CURRENT_DATE\" > \"$STATE_FILE\"\n   \n   # If date changed, trigger summary generation\n   if [ \"$PREV_DATE\" != \"\" ] && [ \"$PREV_DATE\" != \"$CURRENT_DATE\" ]; then\n     # Call the summary generator for previous day\n     commit-story generate-summary --period day --date \"$PREV_DATE\"\n   fi\n   ```\n\n2. **Daily Summary Generation Function**:\n   ```python\n   def generate_daily_summary(date=None):\n       \"\"\"Generate summary for the specified date (defaults to yesterday)\"\"\"\n       if date is None:\n           # Default to yesterday\n           date = (datetime.now() - timedelta(days=1)).date()\n       elif isinstance(date, str):\n           date = datetime.strptime(date, \"%Y-%m-%d\").date()\n           \n       # Get all journal entries for the specified date\n       entries = get_journal_entries_for_date(date)\n       \n       if not entries:\n           logger.info(f\"No journal entries found for {date}\")\n           return None\n           \n       # Generate summary using existing summary generation logic\n       summary = synthesize_entries(entries, date)\n       \n       # Save summary to appropriate location\n       save_daily_summary(summary, date)\n       \n       return summary\n   ```\n\n3. **Entry Collection and Parsing**:\n   ```python\n   def get_journal_entries_for_date(date):\n       \"\"\"Retrieve all journal entries for the specified date\"\"\"\n       journal_path = get_journal_path()\n       date_str = date.strftime(\"%Y-%m-%d\")\n       \n       # Find all journal files for the date\n       entry_pattern = f\"{date_str}-*.md\"\n       entry_files = list(journal_path.glob(entry_pattern))\n       \n       entries = []\n       for file_path in entry_files:\n           with open(file_path, \"r\") as f:\n               content = f.read()\n               entries.append(parse_journal_entry(content, file_path))\n               \n       return entries\n   ```\n\n4. **Summary Synthesis**:\n   ```python\n   def synthesize_entries(entries, date):\n       \"\"\"Create a cohesive summary from multiple journal entries\"\"\"\n       # Sort entries by timestamp\n       entries.sort(key=lambda e: e.get('timestamp'))\n       \n       # Extract key information\n       commits = [e.get('commit_message') for e in entries if e.get('commit_message')]\n       reflections = [e.get('reflection') for e in entries if e.get('reflection')]\n       \n       # Prioritize manual reflections as a core requirement\n       manual_reflections = [r for r in reflections if r.get('is_manual', False)]\n       \n       # Generate summary template\n       summary = {\n           \"date\": date.strftime(\"%Y-%m-%d\"),\n           \"title\": f\"Daily Summary for {date.strftime('%B %d, %Y')}\",\n           \"manual_reflections\": manual_reflections,\n           \"commit_count\": len(commits),\n           \"commit_summary\": summarize_commits(commits),\n           \"key_achievements\": extract_key_achievements(entries),\n           \"challenges\": extract_challenges(entries),\n           \"next_steps\": suggest_next_steps(entries)\n       }\n       \n       return summary\n   ```\n\n5. **Summary Storage**:\n   ```python\n   def save_daily_summary(summary, date):\n       \"\"\"Save the generated summary to the appropriate location\"\"\"\n       journal_path = get_journal_path()\n       summaries_dir = journal_path / \"summaries\" / \"daily\"\n       \n       # Create directories if they don't exist\n       summaries_dir.mkdir(parents=True, exist_ok=True)\n       \n       # Create filename\n       filename = f\"{date.strftime('%Y-%m-%d')}-daily-summary.md\"\n       file_path = summaries_dir / filename\n       \n       # Format summary as markdown\n       content = format_summary_as_markdown(summary)\n       \n       # Save to file\n       with open(file_path, \"w\") as f:\n           f.write(content)\n           \n       logger.info(f\"Daily summary saved to {file_path}\")\n       return file_path\n   ```\n\n6. **Format Summary with Prioritized Manual Reflections**:\n   ```python\n   def format_summary_as_markdown(summary):\n       \"\"\"Format the summary as a markdown document with prioritized manual reflections\"\"\"\n       md_content = []\n       \n       # Add title\n       md_content.append(f\"# {summary['title']}\\n\")\n       \n       # Prominently display manual reflections at the beginning with visual distinction\n       if summary.get('manual_reflections'):\n           md_content.append(\"## 💭 Manual Reflections\\n\")\n           md_content.append(\"<div class='manual-reflections'>\\n\")\n           \n           for reflection in summary['manual_reflections']:\n               md_content.append(f\"### {reflection.get('title', 'Reflection')}\\n\")\n               md_content.append(f\"{reflection.get('content', '')}\\n\\n\")\n           \n           md_content.append(\"</div>\\n\")\n       \n       # Add commit summary\n       md_content.append(f\"## Commit Activity\\n\")\n       md_content.append(f\"Total commits: {summary['commit_count']}\\n\\n\")\n       md_content.append(f\"{summary['commit_summary']}\\n\")\n       \n       # Add key achievements\n       md_content.append(f\"## Key Achievements\\n\")\n       for achievement in summary['key_achievements']:\n           md_content.append(f\"- {achievement}\\n\")\n       md_content.append(\"\\n\")\n       \n       # Add challenges\n       if summary.get('challenges'):\n           md_content.append(f\"## Challenges\\n\")\n           for challenge in summary['challenges']:\n               md_content.append(f\"- {challenge}\\n\")\n           md_content.append(\"\\n\")\n       \n       # Add next steps\n       md_content.append(f\"## Next Steps\\n\")\n       for step in summary['next_steps']:\n           md_content.append(f\"- {step}\\n\")\n       \n       return ''.join(md_content)\n   ```\n\n7. **CLI Integration**:\n   ```python\n   @click.command()\n   @click.option(\"--date\", help=\"Date to generate summary for (YYYY-MM-DD)\")\n   def generate_summary_command(date=None):\n       \"\"\"Generate a daily summary for the specified date\"\"\"\n       summary = generate_daily_summary(date)\n       if summary:\n           click.echo(f\"Summary generated for {summary['date']}\")\n       else:\n           click.echo(\"No entries found for the specified date\")\n   ```\n\n8. **Git Hook Installation**:\n   ```python\n   def install_git_hooks():\n       \"\"\"Install the necessary git hooks for commit-story\"\"\"\n       repo_path = get_git_repo_path()\n       hooks_dir = repo_path / \".git\" / \"hooks\"\n       \n       # Create post-commit hook\n       post_commit_path = hooks_dir / \"post-commit\"\n       \n       # Write hook content\n       with open(post_commit_path, \"w\") as f:\n           f.write(POST_COMMIT_HOOK_CONTENT)\n           \n       # Make hook executable\n       os.chmod(post_commit_path, 0o755)\n       \n       logger.info(f\"Installed post-commit hook at {post_commit_path}\")\n   ```\n\n9. **Configuration Integration**:\n   - Add configuration options for daily summary generation\n   - Allow customization of summary format and content\n   - Provide options to disable automatic triggering\n   - Include options for manual reflection styling and positioning\n\n10. **Error Handling and Logging**:\n   - Implement robust error handling for the Git hook\n   - Ensure failures don't disrupt normal Git operations\n   - Log summary generation attempts and results",
      "testStrategy": "To verify the correct implementation of the daily summary Git hook trigger:\n\n1. **Unit Tests for Summary Generation**:\n   ```python\n   def test_daily_summary_generation():\n       # Create mock journal entries for a specific date\n       mock_date = datetime.strptime(\"2023-05-15\", \"%Y-%m-%d\").date()\n       mock_entries = create_mock_journal_entries(mock_date, count=3)\n       \n       # Test summary generation\n       summary = synthesize_entries(mock_entries, mock_date)\n       \n       # Verify summary structure\n       assert summary[\"date\"] == \"2023-05-15\"\n       assert summary[\"commit_count\"] == 3\n       assert \"commit_summary\" in summary\n       assert \"key_achievements\" in summary\n   \n   def test_entry_collection():\n       # Create mock journal files for a specific date\n       mock_date = datetime.strptime(\"2023-05-16\", \"%Y-%m-%d\").date()\n       create_mock_journal_files(mock_date, count=4)\n       \n       # Test entry collection\n       entries = get_journal_entries_for_date(mock_date)\n       \n       # Verify entries were collected correctly\n       assert len(entries) == 4\n       for entry in entries:\n           assert \"timestamp\" in entry\n           assert \"content\" in entry\n   ```\n\n2. **Integration Test for Git Hook**:\n   ```python\n   def test_git_hook_trigger():\n       # Set up a test repository\n       repo_dir = setup_test_repo()\n       \n       # Install the git hook\n       install_git_hooks()\n       \n       # Create mock state file with yesterday's date\n       yesterday = (datetime.now() - timedelta(days=1)).date().strftime(\"%Y-%m-%d\")\n       with open(os.path.join(repo_dir, \".commit-story-state\"), \"w\") as f:\n           f.write(yesterday)\n       \n       # Create mock journal entries for yesterday\n       create_mock_journal_files(yesterday, count=2)\n       \n       # Make a commit to trigger the hook\n       make_test_commit(repo_dir, \"Test commit\")\n       \n       # Check if summary was generated\n       summary_path = get_expected_summary_path(yesterday)\n       assert os.path.exists(summary_path)\n       \n       # Verify summary content\n       with open(summary_path, \"r\") as f:\n           content = f.read()\n           assert yesterday in content\n           assert \"Daily Summary\" in content\n   ```\n\n3. **Test Manual Reflection Prioritization**:\n   ```python\n   def test_manual_reflection_prioritization():\n       # Create mock journal entries including manual reflections\n       mock_date = datetime.strptime(\"2023-05-18\", \"%Y-%m-%d\").date()\n       mock_entries = create_mock_journal_entries(mock_date, count=3)\n       \n       # Add manual reflections to one entry\n       mock_entries[1][\"reflection\"] = {\n           \"is_manual\": True,\n           \"title\": \"Test Reflection\",\n           \"content\": \"This is a manual reflection.\"\n       }\n       \n       # Generate summary\n       summary = synthesize_entries(mock_entries, mock_date)\n       \n       # Verify manual reflections are included and prioritized\n       assert \"manual_reflections\" in summary\n       assert len(summary[\"manual_reflections\"]) == 1\n       assert summary[\"manual_reflections\"][0][\"title\"] == \"Test Reflection\"\n       \n       # Test markdown formatting\n       markdown = format_summary_as_markdown(summary)\n       \n       # Verify manual reflections appear at the beginning with visual distinction\n       assert \"## 💭 Manual Reflections\" in markdown\n       assert \"<div class='manual-reflections'>\" in markdown\n       assert \"### Test Reflection\" in markdown\n       \n       # Verify manual reflections appear before other sections\n       manual_reflection_pos = markdown.find(\"## 💭 Manual Reflections\")\n       commit_activity_pos = markdown.find(\"## Commit Activity\")\n       assert manual_reflection_pos < commit_activity_pos\n   ```\n\n4. **Manual Testing Procedure**:\n   1. Install the application with the Git hook feature\n   2. Create several journal entries for \"yesterday\" (can be simulated by changing system date)\n   3. Include at least one manual reflection in the entries\n   4. Change the system date to \"today\"\n   5. Make a Git commit\n   6. Verify that a daily summary was generated for \"yesterday\"\n   7. Check that manual reflections are prominently displayed at the beginning\n   8. Verify the visual distinction of manual reflections\n   9. Check the summary content for accuracy and completeness\n\n5. **Edge Case Testing**:\n   ```python\n   def test_empty_day_handling():\n       # Test with a date that has no entries\n       empty_date = datetime.strptime(\"2000-01-01\", \"%Y-%m-%d\").date()\n       summary = generate_daily_summary(empty_date)\n       assert summary is None\n   \n   def test_malformed_entries():\n       # Create malformed journal entries\n       mock_date = datetime.strptime(\"2023-05-17\", \"%Y-%m-%d\").date()\n       create_malformed_journal_files(mock_date)\n       \n       # Test that the system handles malformed entries gracefully\n       try:\n           summary = generate_daily_summary(mock_date)\n           # Should either return a partial summary or None\n           if summary:\n               assert \"date\" in summary\n       except Exception as e:\n           assert False, f\"Should handle malformed entries without exception: {e}\"\n   ```\n\n6. **Performance Testing**:\n   - Test with a large number of journal entries (50+) for a single day\n   - Measure execution time and memory usage\n   - Ensure performance remains acceptable\n\n7. **Configuration Testing**:\n   - Test with different configuration settings\n   - Verify that customization options work as expected\n   - Test disabling the automatic trigger\n   - Test different styling options for manual reflections\n\n8. **Verification Checklist**:\n   - [ ] Git hook is properly installed during setup\n   - [ ] Hook correctly detects date changes\n   - [ ] Summary is generated for the correct date (previous day)\n   - [ ] Summary includes all journal entries from the target date\n   - [ ] Manual reflections are prioritized and displayed prominently at the beginning\n   - [ ] Manual reflections have visual distinction in the output\n   - [ ] Summary is saved to the expected location\n   - [ ] Error handling prevents Git operation disruption",
      "subtasks": [
        {
          "id": 1,
          "title": "Implement Date Change Detection and State Management",
          "description": "Create the core logic to detect when the date changes between commits and manage the state file that tracks the last processed date.",
          "details": "Create `get_current_date()` and `get_last_processed_date()` functions. Implement `update_state_file()` to write current date to `.commit-story-state`. Add `has_date_changed()` logic to compare current vs. last processed date. Handle edge cases: missing state file, corrupted state file, first run. Add proper error handling and logging. Location: `src/mcp_commit_story/daily_summary.py` (new module)\n<info added on 2025-06-04T17:59:13.844Z>\n### TDD Steps:\n1. **WRITE TESTS FIRST**\n   - Create `tests/unit/test_daily_summary.py`\n   - Test `get_current_date()` function returns today's date in YYYY-MM-DD format\n   - Test `get_last_processed_date(state_file_path)` function:\n     - Success case: reads valid date from existing state file\n     - Error case: handles missing state file (returns None)\n     - Error case: handles corrupted state file with invalid date format\n     - Error case: handles permission errors when reading state file\n   - Test `update_state_file(date, state_file_path)` function:\n     - Success case: writes date to state file in correct format\n     - Success case: creates state file if it doesn't exist\n     - Error case: handles permission errors when writing\n     - Error case: handles invalid date input\n   - Test `has_date_changed(current_date, last_date)` function:\n     - Returns True when dates differ\n     - Returns False when dates are same\n     - Returns True when last_date is None (first run)\n     - Handles edge cases with date string formats\n   - **RUN TESTS - VERIFY THEY FAIL**\n\n2. **GET APPROVAL FOR DESIGN CHOICES**\n   - **PAUSE FOR MANUAL APPROVAL**: State file location - should it be `.commit-story-state` in repo root, or in a subdirectory like `.mcp-commit-story/`?\n   - **PAUSE FOR MANUAL APPROVAL**: Date format standardization - use ISO format YYYY-MM-DD or allow different formats?\n   - **PAUSE FOR MANUAL APPROVAL**: Error handling strategy - fail silently vs. log warnings vs. raise exceptions for state file issues?\n   - **PAUSE FOR MANUAL APPROVAL**: Concurrent access handling - use file locking or simple read/write (git hooks typically run sequentially)?\n\n3. **IMPLEMENT FUNCTIONALITY**\n   - Create `src/mcp_commit_story/daily_summary.py` module\n   - Implement `get_current_date() -> str` that returns datetime.now().strftime(\"%Y-%m-%d\")\n   - Implement `get_last_processed_date(state_file_path: str) -> Optional[str]` with proper error handling\n   - Implement `update_state_file(date: str, state_file_path: str) -> bool` with directory creation if needed\n   - Implement `has_date_changed(current_date: str, last_date: Optional[str]) -> bool` with validation\n   - Add proper logging using existing structured logging patterns\n   - Add input validation for date format consistency\n   - Handle all error cases identified in tests (file permissions, invalid formats, missing files)\n   - **RUN TESTS - VERIFY THEY PASS**\n\n4. **DOCUMENT AND COMPLETE**\n   - Add documentation IF NEEDED in three places:\n     1. **Docs directory**: Update implementation-guide.md with date change detection workflow and state file management\n     2. **PRD**: Update if this adds user-facing behavior (likely minimal since this is internal infrastructure)\n     3. **Engineering Spec**: Add technical details about state file format, location, and date change detection algorithm, and make sure TOC is current\n   - **Do not remove existing information unless it's incorrect**\n   - **No approval needed** - make documentation edits directly\n   - **Run the entire test suite and make sure all tests are passing**\n   - **Make sure pyproject.toml is updated as needed** (no new dependencies expected for this subtask)\n   - Double check all subtask requirements are met: date detection, state management, error handling, logging\n   - **MARK COMPLETE**\n</info added on 2025-06-04T17:59:13.844Z>",
          "status": "pending",
          "dependencies": [],
          "parentTaskId": 27
        },
        {
          "id": 2,
          "title": "Create Daily Summary MCP Tool",
          "description": "Add an MCP tool to trigger daily summary generation, which will also be called by the git hook through the MCP server.",
          "details": "Add `handle_generate_daily_summary()` function to `server.py`. Create appropriate request/response types in `journal_workflow_types.py`. Support both manual date specification and \"yesterday\" default. Include proper error handling and telemetry integration. The git hook will need to interact with the MCP server rather than a CLI command. Location: `src/mcp_commit_story/server.py` and `src/mcp_commit_story/journal_workflow_types.py`\n<info added on 2025-06-04T17:59:36.884Z>\n### TDD Implementation Plan\n\n1. **WRITE TESTS FIRST**\n   - Create `tests/unit/test_daily_summary_mcp.py`\n   - Test `handle_generate_daily_summary()` MCP handler function\n   - Test cases: \n     - Success case: generate summary for specific date with existing journal entries\n     - Success case: generate summary for \"yesterday\" default when no date provided\n     - Error case: invalid date format in request\n     - Error case: no journal entries found for specified date\n     - Error case: file system errors during summary generation\n   - Test request/response type validation for `GenerateDailySummaryRequest` and `GenerateDailySummaryResponse`\n   - Test MCP tool registration in server.py\n   - **RUN TESTS - VERIFY THEY FAIL**\n\n2. **GET APPROVAL FOR DESIGN CHOICES**\n   - **PAUSE FOR MANUAL APPROVAL**: Request schema design - should date be required or optional with yesterday default?\n   - **PAUSE FOR MANUAL APPROVAL**: Response format - return summary content, file path, or both?\n   - **PAUSE FOR MANUAL APPROVAL**: Error response strategy - detailed error messages vs. generic messages for security?\n   - **PAUSE FOR MANUAL APPROVAL**: Integration with existing daily summary from Task 18 vs. creating new implementation?\n\n3. **IMPLEMENT FUNCTIONALITY**\n   - Add `GenerateDailySummaryRequest` and `GenerateDailySummaryResponse` types to `src/mcp_commit_story/journal_workflow_types.py`\n   - Implement `handle_generate_daily_summary()` function in `src/mcp_commit_story/server.py`\n   - Register the new MCP tool with appropriate decorator and metadata\n   - Integrate with date change detection functions from subtask 27.1\n   - Add proper error handling and telemetry integration following existing patterns\n   - Support both explicit date and \"yesterday\" default behavior\n   - Return appropriate success/error responses with consistent formatting\n   - **RUN TESTS - VERIFY THEY PASS**\n\n4. **DOCUMENT AND COMPLETE**\n   - Add documentation IF NEEDED in three places:\n     1. **Docs directory**: Update MCP API documentation with new daily summary tool\n     2. **PRD**: Update with daily summary generation capability available via MCP\n     3. **Engineering Spec**: Add MCP tool implementation details and request/response schemas, and make sure TOC is current\n   - **Do not remove existing information unless it's incorrect**\n   - **No approval needed** - make documentation edits directly\n   - **Run the entire test suite and make sure all tests are passing**\n   - **Make sure pyproject.toml is updated as needed**\n   - Double check all subtask requirements are met: MCP tool registration, request/response handling, error cases, telemetry\n</info added on 2025-06-04T17:59:36.884Z>",
          "status": "pending",
          "dependencies": [
            "27.1"
          ],
          "parentTaskId": 27
        },
        {
          "id": 3,
          "title": "Enhance Git Hook with Daily Summary Triggering",
          "description": "Modify the existing git hook generation to include date change detection and daily summary triggering logic.",
          "details": "Update `generate_hook_content()` in `git_utils.py` to include date change detection. Add bash script logic to read/write `.commit-story-state` file. Call the MCP server to trigger daily summary generation when date changes. Ensure hook remains lightweight and doesn't break git operations on failure. Handle concurrent access to state file (multiple rapid commits). Location: `src/mcp_commit_story/git_utils.py`\n<info added on 2025-06-04T18:01:36.075Z>\n### TDD Implementation Plan\n\n1. **WRITE TESTS FIRST**\n   - Create `tests/unit/test_git_hook_daily_summary.py`\n   - Test `generate_hook_content()` updated function:\n     - Verify generated hook script includes date change detection logic\n     - Verify hook script includes state file read/write operations\n     - Verify hook script calls MCP server for daily summary generation\n     - Test hook script handles missing state file on first run\n     - Test hook script handles state file read/write errors gracefully\n   - Create `tests/integration/test_git_hook_integration.py`\n   - Test full git hook execution in temporary repository:\n     - Test hook execution with same-day commits (no summary triggered)\n     - Test hook execution with date change (summary triggered)\n     - Test hook execution with missing/corrupted state file\n     - Test hook failure doesn't break git commit operation\n   - **RUN TESTS - VERIFY THEY FAIL**\n\n2. **GET APPROVAL FOR DESIGN CHOICES**\n   - **PAUSE FOR MANUAL APPROVAL**: Hook communication method - should git hook call MCP server directly, use CLI wrapper, or use HTTP endpoint?\n   - **PAUSE FOR MANUAL APPROVAL**: Error handling in hook - silent failure vs. warning messages vs. hook failure on summary generation errors?\n   - **PAUSE FOR MANUAL APPROVAL**: State file location relative to git repo - repo root vs. .git directory vs. configurable path?\n   - **PAUSE FOR MANUAL APPROVAL**: Concurrent commit handling - file locking vs. atomic operations vs. allow race conditions?\n\n3. **IMPLEMENT FUNCTIONALITY**\n   - Update `generate_hook_content()` function in `src/mcp_commit_story/git_utils.py`\n   - Add bash script logic for date change detection using the state file pattern\n   - Add bash script logic to read current date and compare with state file content\n   - Add MCP server communication logic (likely through Python subprocess call)\n   - Ensure hook script remains lightweight and handles errors gracefully\n   - Test hook script generation produces valid, executable bash\n   - Add proper logging and error handling that doesn't disrupt git operations\n   - Handle edge cases: first run, corrupted state file, permission errors\n   - **RUN TESTS - VERIFY THEY PASS**\n\n4. **DOCUMENT AND COMPLETE**\n   - Add documentation IF NEEDED in three places:\n     1. **Docs directory**: Update git-hooks.md or implementation-guide.md with daily summary hook behavior\n     2. **PRD**: Update with automatic daily summary generation feature\n     3. **Engineering Spec**: Add git hook enhancement details and bash script logic, and make sure TOC is current\n   - **Do not remove existing information unless it's incorrect**\n   - **No approval needed** - make documentation edits directly\n   - **Run the entire test suite and make sure all tests are passing**\n   - **Make sure pyproject.toml is updated as needed**\n   - Double check all subtask requirements are met: hook enhancement, date detection, MCP integration, error handling\n</info added on 2025-06-04T18:01:36.075Z>",
          "status": "pending",
          "dependencies": [
            "27.1",
            "27.2"
          ],
          "parentTaskId": 27
        },
        {
          "id": 4,
          "title": "Implement Daily Summary Generation Function",
          "description": "Create the core daily summary generation function that collects journal entries for a specific date and generates a comprehensive summary.",
          "details": "Create `generate_daily_summary(date)` function in new `daily_summary.py` module. Implement `get_journal_entries_for_date(date)` to collect all entries for a specific date. Add `synthesize_entries(entries, date)` to create cohesive summary. Implement `save_daily_summary(summary, date)` with proper directory creation. Follow on-demand directory creation pattern for `summaries/daily/`. Integrate manual reflection prioritization from existing summary logic. Location: `src/mcp_commit_story/daily_summary.py`\n<info added on 2025-06-04T18:02:11.248Z>\n### TDD Implementation Plan\n\n#### 1. WRITE TESTS FIRST\n- Create `tests/unit/test_daily_summary_generation.py`\n- Test `generate_daily_summary(date: str, config: dict) -> Optional[dict]` function:\n  - Success case: generate summary for date with multiple journal entries\n  - Success case: generate summary for date with single journal entry\n  - Success case: handle manual reflections prioritization correctly\n  - Edge case: return None for date with no journal entries\n  - Error case: handle invalid date format\n  - Error case: handle file system errors when reading journal files\n- Test `get_journal_entries_for_date(date: str, config: dict) -> List[dict]` function:\n  - Success case: collect all journal files for specific date\n  - Edge case: return empty list for date with no entries\n  - Test correct file pattern matching (YYYY-MM-DD-*.md)\n- Test `synthesize_entries(entries: List[dict], date: str) -> dict` function:\n  - Test summary structure includes required fields\n  - Test manual reflection extraction and prioritization\n  - Test commit count and activity summarization\n- Test `save_daily_summary(summary: dict, date: str, config: dict) -> str` function:\n  - Test file saving to correct location (summaries/daily/)\n  - Test on-demand directory creation\n  - Test markdown formatting of summary content\n- RUN TESTS - VERIFY THEY FAIL\n\n#### 2. GET APPROVAL FOR DESIGN CHOICES\n- PAUSE FOR MANUAL APPROVAL: Summary content structure - which fields to include (commits, reflections, achievements, challenges)?\n- PAUSE FOR MANUAL APPROVAL: Manual reflection extraction strategy - parse from specific sections vs. detect by patterns?\n- PAUSE FOR MANUAL APPROVAL: File naming convention - date-only vs. date-with-timestamp vs. configurable format?\n- PAUSE FOR MANUAL APPROVAL: Integration approach - reuse existing Task 18 daily summary logic vs. create new implementation?\n\n#### 3. IMPLEMENT FUNCTIONALITY\n- Implement `generate_daily_summary(date, config)` as main orchestration function in `src/mcp_commit_story/daily_summary.py`\n- Implement `get_journal_entries_for_date(date, config)` with proper file pattern matching\n- Implement `synthesize_entries(entries, date)` with manual reflection prioritization logic\n- Implement `save_daily_summary(summary, date, config)` following on-demand directory creation pattern\n- Add `format_summary_as_markdown(summary)` for consistent markdown output\n- Integrate with existing journal parsing utilities where appropriate\n- Ensure proper error handling and logging throughout\n- Follow existing code patterns and conventions from journal.py\n- RUN TESTS - VERIFY THEY PASS\n\n#### 4. DOCUMENT AND COMPLETE\n- Add documentation IF NEEDED in three places:\n  1. Docs directory: Update implementation-guide.md with daily summary generation workflow and file structure\n  2. PRD: Update with daily summary generation feature description and user benefits\n  3. Engineering Spec: Add daily summary generation algorithm details and file organization, and make sure TOC is current\n- Do not remove existing information unless it's incorrect\n- No approval needed - make documentation edits directly\n- Run the entire test suite and make sure all tests are passing\n- Make sure pyproject.toml is updated as needed\n- Double check all subtask requirements are met: summary generation, entry collection, synthesis, file saving, directory creation\n- MARK COMPLETE\n</info added on 2025-06-04T18:02:11.248Z>",
          "status": "pending",
          "dependencies": [
            "27.1",
            "27.2"
          ],
          "parentTaskId": 27
        },
        {
          "id": 5,
          "title": "Integration Testing and Documentation",
          "description": "Create comprehensive end-to-end tests and update all documentation for the daily summary git hook feature.",
          "details": "Create end-to-end test simulating full workflow: commits → date change → summary generation. Test across multiple days with various journal entry patterns. Update docs/implementation-guide.md with daily summary workflow. Update PRD and engineering spec with completed functionality. Add troubleshooting guide for common hook issues. Test manual vs. automatic triggering scenarios.\n<info added on 2025-06-04T18:02:38.668Z>\n### TDD Steps:\n1. **WRITE TESTS FIRST**\n   - Create `tests/integration/test_daily_summary_end_to_end.py`\n   - Test complete workflow in temporary git repository:\n     - Setup: create repo, install hook, create journal entries for \"yesterday\"\n     - Action: make commit with today's date (simulating date change)\n     - Verify: daily summary file created for yesterday with correct content\n     - Verify: state file updated with today's date\n   - Test multi-day scenario:\n     - Create journal entries across multiple days\n     - Simulate commits with date changes\n     - Verify summary generation for each day transition\n   - Test edge cases:\n     - First commit ever (no previous state)\n     - Multiple commits on same day (no duplicate summaries)\n     - Commits on days with no journal entries\n     - Hook execution with file system errors\n   - Test manual vs automatic triggering:\n     - Test MCP tool direct invocation\n     - Test git hook automatic triggering\n     - Verify both produce identical results\n   - **RUN TESTS - VERIFY THEY FAIL**\n\n2. **GET APPROVAL FOR DESIGN CHOICES**\n   - **PAUSE FOR MANUAL APPROVAL**: Integration test scope - how comprehensive should the end-to-end testing be?\n   - **PAUSE FOR MANUAL APPROVAL**: Documentation update scope - which docs need updates vs. which are already sufficient?\n   - **PAUSE FOR MANUAL APPROVAL**: Troubleshooting guide detail level - basic vs. comprehensive diagnostic information?\n\n3. **IMPLEMENT FUNCTIONALITY**\n   - Implement comprehensive integration tests covering full workflow\n   - Create test fixtures for multi-day journal entry scenarios\n   - Add test utilities for git repository setup and hook installation\n   - Verify integration between all subtasks 27.1-27.4 works correctly\n   - Test error recovery and graceful degradation scenarios\n   - Ensure tests can run in CI environment without external dependencies\n   - **RUN TESTS - VERIFY THEY PASS**\n\n4. **DOCUMENT AND COMPLETE**\n   - Add documentation IF NEEDED in three places:\n     1. **Docs directory**: \n        - Update implementation-guide.md with complete daily summary workflow\n        - Add troubleshooting-guide.md section for git hook issues\n        - Update user documentation with automatic daily summary feature\n     2. **PRD**: \n        - Update with completed daily summary automation feature\n        - Add user benefits and workflow description\n        - Update MVP completion status for Task 27\n     3. **Engineering Spec**: \n        - Update with complete system architecture including daily summary workflow\n        - Add integration testing documentation\n        - Document troubleshooting procedures and common issues\n        - Make sure TOC is current with all new sections\n   - **Do not remove existing information unless it's incorrect**\n   - **No approval needed** - make documentation edits directly\n   - **Run the entire test suite and make sure all tests are passing**\n   - **Make sure pyproject.toml is updated as needed**\n   - Double check all subtask requirements are met: end-to-end testing, multi-day scenarios, manual/automatic testing, comprehensive documentation\n   - **MARK COMPLETE**\n</info added on 2025-06-04T18:02:38.668Z>",
          "status": "pending",
          "dependencies": [
            "27.3",
            "27.4"
          ],
          "parentTaskId": 27
        }
      ]
    },
    {
      "id": 28,
      "title": "Implement File-Based Logging System",
      "description": "Replace Python standard logging with file-based JSON logging to prevent stdout interference with MCP clients.",
      "details": "Create a dedicated logging module in `src/mcp_commit_story/logging.py` with the following features:\n\n1. JSON-formatted file logging:\n```python\nimport json\nimport logging\nimport os\nimport sys\nimport tempfile\nfrom datetime import datetime\nfrom pathlib import Path\n\nclass JsonFileHandler(logging.FileHandler):\n    \"\"\"Custom handler that writes JSON-formatted logs to a file\"\"\"\n    \n    def __init__(self, filename):\n        super().__init__(filename)\n        \n    def emit(self, record):\n        log_entry = {\n            'timestamp': datetime.fromtimestamp(record.created).isoformat(),\n            'level': record.levelname,\n            'message': record.getMessage(),\n            'module': record.module,\n            'function': record.funcName,\n            'line': record.lineno\n        }\n        \n        # Add exception info if available\n        if record.exc_info:\n            log_entry['exception'] = self.formatException(record.exc_info)\n            \n        self.stream.write(json.dumps(log_entry) + '\\n')\n        self.flush()\n```\n\n2. Logger configuration function:\n```python\ndef configure_logging():\n    \"\"\"Configure the logging system based on environment variables\"\"\"\n    # Get log file path from environment variable or use default\n    log_file = os.environ.get(\n        'MCP_COMMIT_STORY_LOG_FILE',\n        os.path.expanduser('~/Library/Logs/mcp-commit-story.log')\n    )\n    \n    # Create parent directory if it doesn't exist\n    log_path = Path(log_file)\n    try:\n        log_path.parent.mkdir(parents=True, exist_ok=True)\n    except (PermissionError, OSError):\n        # Fallback to temp directory\n        temp_dir = tempfile.gettempdir()\n        log_file = os.path.join(temp_dir, 'mcp-commit-story.log')\n        \n    # Get log level from environment variable or use default\n    log_level_name = os.environ.get('MCP_COMMIT_STORY_LOG_LEVEL', 'INFO')\n    log_level = getattr(logging, log_level_name.upper(), logging.INFO)\n    \n    # Configure root logger\n    logger = logging.getLogger()\n    logger.setLevel(log_level)\n    \n    # Remove any existing handlers\n    for handler in logger.handlers[:]:\n        logger.removeHandler(handler)\n    \n    # Add JSON file handler\n    file_handler = JsonFileHandler(log_file)\n    logger.addHandler(file_handler)\n    \n    # Optionally add console handler for debugging\n    if os.environ.get('MCP_COMMIT_STORY_CONSOLE_LOGGING', '').lower() == 'true':\n        console_handler = logging.StreamHandler(sys.stdout)\n        console_handler.setLevel(log_level)\n        logger.addHandler(console_handler)\n    \n    # Redirect uncaught exceptions to log\n    sys.excepthook = lambda *exc_info: logger.error(\n        \"Uncaught exception\", exc_info=exc_info\n    )\n    \n    return logger\n```\n\n3. Utility functions for common logging patterns:\n```python\ndef get_logger(name):\n    \"\"\"Get a logger with the given name\"\"\"\n    return logging.getLogger(name)\n\ndef log_function_call(logger, func_name, args=None, kwargs=None):\n    \"\"\"Log a function call with arguments\"\"\"\n    args_str = str(args) if args else ''\n    kwargs_str = str(kwargs) if kwargs else ''\n    logger.debug(f\"Called {func_name}({args_str}{', ' if args and kwargs else ''}{kwargs_str})\")\n```\n\n4. Ensure logs are flushed on exit:\n```python\nimport atexit\n\ndef flush_logs():\n    \"\"\"Flush all logs on exit\"\"\"\n    for handler in logging.getLogger().handlers:\n        handler.flush()\n\natexit.register(flush_logs)\n```\n\n5. Update all existing code to use the new logging system:\n   - Import the new logging module at the top of each file\n   - Replace all print statements with appropriate log calls\n   - Replace any existing logging configuration with calls to configure_logging()\n   - Add log statements for important events and error conditions\n\n6. Add logging initialization to application entry points:\n```python\n# In __main__.py or similar entry point\nfrom mcp_commit_story.logging import configure_logging\n\ndef main():\n    logger = configure_logging()\n    logger.info(\"MCP Commit Story starting\")\n    # Rest of application code\n    \nif __name__ == \"__main__\":\n    main()\n```\n\n7. Ensure no stdout/stderr output during normal operations:\n   - Review all code for print statements and replace with logging\n   - Capture and log any subprocess output instead of letting it go to stdout\n   - Use appropriate log levels (DEBUG, INFO, WARNING, ERROR) based on message importance",
      "testStrategy": "1. Test log file creation:\n   - Create unit tests that verify log files are created in the expected location\n   - Test with both default and custom MCP_COMMIT_STORY_LOG_FILE paths\n   - Verify JSON format is correct and contains all required fields\n\n2. Test directory creation and fallback:\n   - Create a test that attempts to log to a non-writable directory\n   - Verify the fallback to temp directory works correctly\n   - Ensure no exceptions are raised during fallback\n\n3. Test environment variable configuration:\n   - Create tests with different MCP_COMMIT_STORY_LOG_LEVEL values\n   - Verify only appropriate messages are logged based on level\n   - Test MCP_COMMIT_STORY_CONSOLE_LOGGING=true and verify console output\n   - Test MCP_COMMIT_STORY_CONSOLE_LOGGING=false and verify no console output\n\n4. Test log content:\n   - Verify timestamps are in ISO format\n   - Ensure module, function, and line information is correct\n   - Test exception logging and verify stack traces are captured\n\n5. Test stdout/stderr isolation:\n   - Create integration tests that run MCP operations\n   - Capture stdout/stderr during test execution\n   - Verify no output is produced during normal operation\n   - Check that logs are written to the file instead\n\n6. Test log flushing:\n   - Create a test that simulates process exit\n   - Verify logs are properly flushed to disk\n   - Check that no log messages are lost\n\n7. Test with MCP server:\n   - Run the MCP server with the new logging system\n   - Verify it operates without stdout interference\n   - Check that all server operations are properly logged to file",
      "status": "pending",
      "dependencies": [
        7,
        19,
        22
      ],
      "priority": "high",
      "subtasks": []
    },
    {
      "id": 29,
      "title": "Implement MCP Info Command for Diagnostics",
      "description": "Add a new 'info' tool to the MCP server that provides diagnostic information to help users troubleshoot issues, including version, telemetry status, configuration details, and dependency availability.",
      "details": "Implement the MCP info command in `src/mcp_commit_story/server.py` with the following features:\n\n1. Create a new tool handler using the `@server.tool()` decorator:\n```python\n@server.tool()\nasync def info(request):\n    \"\"\"Return diagnostic information about the MCP server.\"\"\"\n    try:\n        # Get version from pyproject.toml\n        version = get_version_from_pyproject()\n        \n        # Get telemetry status\n        telemetry_status = get_telemetry_status()\n        \n        # Get active configuration path\n        config_path = get_active_config_path()\n        \n        # Get log file location from the logging system\n        log_file = get_log_file_location()\n        \n        # Check dependency availability\n        dependencies = {\n            \"git\": check_git_availability(),\n            \"opentelemetry\": check_opentelemetry_availability()\n        }\n        \n        # Validate configuration\n        config_validation = validate_configuration()\n        \n        return {\n            \"version\": version,\n            \"telemetry_status\": telemetry_status,\n            \"config_path\": str(config_path),\n            \"log_file\": str(log_file),\n            \"dependencies\": dependencies,\n            \"config_validation\": config_validation\n        }\n    except Exception as e:\n        logger.error(f\"Error in info command: {str(e)}\")\n        return {\"error\": str(e)}\n```\n\n2. Implement helper functions for retrieving diagnostic information:\n\n```python\ndef get_version_from_pyproject():\n    \"\"\"Extract version from pyproject.toml.\"\"\"\n    try:\n        import tomli\n        from pathlib import Path\n        \n        # Find the pyproject.toml file (traverse up from current file if needed)\n        current_dir = Path(__file__).parent\n        pyproject_path = None\n        \n        # Look up to 3 levels up for pyproject.toml\n        for i in range(4):\n            check_path = current_dir / (\"../\" * i) / \"pyproject.toml\"\n            if check_path.resolve().exists():\n                pyproject_path = check_path.resolve()\n                break\n        \n        if not pyproject_path:\n            return \"unknown\"\n        \n        with open(pyproject_path, \"rb\") as f:\n            pyproject_data = tomli.load(f)\n            \n        return pyproject_data.get(\"project\", {}).get(\"version\", \"unknown\")\n    except Exception as e:\n        logger.error(f\"Error getting version: {str(e)}\")\n        return \"unknown\"\n\ndef get_telemetry_status():\n    \"\"\"Get the current telemetry status.\"\"\"\n    # Check if telemetry is enabled in configuration\n    config = get_config()\n    return {\n        \"enabled\": config.get(\"telemetry\", {}).get(\"enabled\", False),\n        \"endpoint\": config.get(\"telemetry\", {}).get(\"endpoint\", \"\")\n    }\n\ndef get_active_config_path():\n    \"\"\"Get the path to the active configuration file.\"\"\"\n    # Return the path to the currently loaded config file\n    return get_config_path()\n\ndef get_log_file_location():\n    \"\"\"Get the path to the current log file.\"\"\"\n    # This should use the logging system implemented in Task 28\n    from mcp_commit_story.logging import get_log_file_path\n    return get_log_file_path()\n\ndef check_git_availability():\n    \"\"\"Check if git is available and return version info.\"\"\"\n    try:\n        import subprocess\n        result = subprocess.run([\"git\", \"--version\"], capture_output=True, text=True, check=True)\n        return {\n            \"available\": True,\n            \"version\": result.stdout.strip()\n        }\n    except Exception:\n        return {\n            \"available\": False,\n            \"version\": None\n        }\n\ndef check_opentelemetry_availability():\n    \"\"\"Check if OpenTelemetry is available.\"\"\"\n    try:\n        import opentelemetry\n        return {\n            \"available\": True,\n            \"version\": getattr(opentelemetry, \"__version__\", \"unknown\")\n        }\n    except ImportError:\n        return {\n            \"available\": False,\n            \"version\": None\n        }\n\ndef validate_configuration():\n    \"\"\"Validate the current configuration.\"\"\"\n    # Perform validation checks on the current configuration\n    config = get_config()\n    validation_results = {}\n    \n    # Check for required configuration sections\n    required_sections = [\"journal\", \"git\", \"server\"]\n    for section in required_sections:\n        validation_results[f\"{section}_section\"] = section in config\n    \n    # Check for required paths\n    if \"journal\" in config:\n        journal_path = Path(config[\"journal\"].get(\"path\", \"\"))\n        validation_results[\"journal_path_exists\"] = journal_path.exists()\n    \n    return validation_results\n```\n\n3. Update the MCP server documentation to include the new info command:\n```python\n# In the server documentation string\n\"\"\"\nMCP Server Tools:\n...\n- info: Returns diagnostic information about the MCP server\n\"\"\"\n```\n\n4. Ensure the info command is properly registered with the MCP server and accessible through the standard MCP protocol.",
      "testStrategy": "1. Unit tests for the info command:\n```python\ndef test_info_command():\n    \"\"\"Test that the info command returns all required fields.\"\"\"\n    # Setup mock server\n    server = MockMCPServer()\n    \n    # Call info command\n    response = server.call_tool(\"info\", {})\n    \n    # Verify all required fields are present\n    assert \"version\" in response\n    assert \"telemetry_status\" in response\n    assert \"config_path\" in response\n    assert \"log_file\" in response\n    assert \"dependencies\" in response\n    assert \"config_validation\" in response\n    \n    # Verify dependencies contains required checks\n    assert \"git\" in response[\"dependencies\"]\n    assert \"opentelemetry\" in response[\"dependencies\"]\n\ndef test_version_from_pyproject():\n    \"\"\"Test that version is dynamically read from pyproject.toml.\"\"\"\n    # Create a temporary pyproject.toml with a known version\n    with tempfile.TemporaryDirectory() as tmpdir:\n        temp_path = Path(tmpdir) / \"pyproject.toml\"\n        with open(temp_path, \"w\") as f:\n            f.write('[project]\\nversion = \"1.2.3\"\\n')\n        \n        # Mock the file resolution to return our temporary file\n        with patch(\"pathlib.Path.resolve\", return_value=temp_path):\n            with patch(\"pathlib.Path.exists\", return_value=True):\n                version = get_version_from_pyproject()\n                assert version == \"1.2.3\"\n\ndef test_info_with_various_configs():\n    \"\"\"Test info command with various configuration states.\"\"\"\n    # Test with missing configuration\n    with patch(\"mcp_commit_story.server.get_config\", return_value={}):\n        response = server.call_tool(\"info\", {})\n        assert response[\"config_validation\"][\"journal_section\"] is False\n    \n    # Test with valid configuration\n    valid_config = {\n        \"journal\": {\"path\": \"/tmp/journal\"},\n        \"git\": {\"repo_path\": \"/tmp/repo\"},\n        \"server\": {\"port\": 8000}\n    }\n    with patch(\"mcp_commit_story.server.get_config\", return_value=valid_config):\n        with patch(\"pathlib.Path.exists\", return_value=True):\n            response = server.call_tool(\"info\", {})\n            assert response[\"config_validation\"][\"journal_section\"] is True\n            assert response[\"config_validation\"][\"journal_path_exists\"] is True\n\ndef test_info_through_mcp_protocol():\n    \"\"\"Test that info command works through the MCP protocol.\"\"\"\n    # Start a real MCP server\n    server_process = start_test_server()\n    try:\n        # Connect to the server using the MCP client\n        client = MCPClient(\"localhost\", 8000)\n        \n        # Call the info command\n        response = client.call(\"info\", {})\n        \n        # Verify response\n        assert \"version\" in response\n        assert \"telemetry_status\" in response\n        assert \"config_path\" in response\n        assert \"log_file\" in response\n    finally:\n        # Clean up\n        server_process.terminate()\n```\n\n2. Integration tests:\n   - Test the info command through the MCP protocol from a real client\n   - Verify that all diagnostic information is correctly reported\n   - Test with different configuration states (missing config, invalid paths, etc.)\n   - Verify that the log file location matches the actual log file being used\n\n3. Manual testing:\n   - Call the info command from the CLI client\n   - Verify that all information is displayed correctly\n   - Intentionally break dependencies (e.g., rename git executable) and verify the command correctly reports their unavailability\n   - Test with telemetry enabled and disabled to ensure correct reporting",
      "status": "pending",
      "dependencies": [
        28
      ],
      "priority": "high",
      "subtasks": []
    },
    {
      "id": 30,
      "title": "Create Release Preparation Script",
      "description": "Develop an automated release validation script that performs a series of quality checks before publishing to ensure the package meets all requirements.",
      "details": "Implement a comprehensive release preparation script (`scripts/prepare_release.py`) that performs sequential validation checks before publishing:\n\n1. **Git Status Checks**:\n   ```python\n   def check_git_status():\n       \"\"\"Verify git repository is in a clean state for release\"\"\"\n       # Check current branch is main/master\n       branch = subprocess.check_output([\"git\", \"branch\", \"--show-current\"]).decode().strip()\n       if branch not in [\"main\", \"master\"]:\n           return False, f\"Not on main/master branch (current: {branch})\"\n       \n       # Check for uncommitted changes\n       status = subprocess.check_output([\"git\", \"status\", \"--porcelain\"]).decode().strip()\n       if status:\n           return False, \"Uncommitted changes detected\"\n       \n       # Check if local is in sync with remote\n       subprocess.check_call([\"git\", \"fetch\", \"origin\"])\n       local = subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"]).decode().strip()\n       remote = subprocess.check_output([\"git\", \"rev-parse\", f\"origin/{branch}\"]).decode().strip()\n       if local != remote:\n           return False, f\"Local {branch} is not in sync with origin/{branch}\"\n       \n       return True, \"Git status checks passed\"\n   ```\n\n2. **Version Checks**:\n   ```python\n   def check_version():\n       \"\"\"Verify version is consistent and not already published\"\"\"\n       # Get version from pyproject.toml\n       with open(\"pyproject.toml\", \"r\") as f:\n           pyproject = toml.load(f)\n       version = pyproject[\"project\"][\"version\"]\n       \n       # Check version format (semantic versioning)\n       if not re.match(r\"^\\d+\\.\\d+\\.\\d+$\", version):\n           return False, f\"Version {version} does not follow semantic versioning\"\n       \n       # Check if version already exists on PyPI\n       try:\n           response = requests.get(f\"https://pypi.org/pypi/mcp-commit-story/{version}/json\")\n           if response.status_code == 200:\n               return False, f\"Version {version} already exists on PyPI\"\n       except Exception as e:\n           pass  # Connection error is not a failure\n       \n       # Check version consistency across files\n       # (Add checks for other files that might contain version info)\n       \n       return True, f\"Version checks passed: {version}\"\n   ```\n\n3. **Code Quality Checks**:\n   ```python\n   def check_code_quality():\n       \"\"\"Run tests, linting, and security checks\"\"\"\n       # Run tests\n       try:\n           subprocess.check_call([\"pytest\", \"-xvs\"])\n       except subprocess.CalledProcessError:\n           return False, \"Tests failed\"\n       \n       # Run linting\n       try:\n           subprocess.check_call([\"flake8\"])\n       except subprocess.CalledProcessError:\n           return False, \"Linting failed\"\n       \n       # Run security audit\n       try:\n           subprocess.check_call([\"bandit\", \"-r\", \"src\"])\n       except subprocess.CalledProcessError:\n           return False, \"Security audit failed\"\n       \n       return True, \"Code quality checks passed\"\n   ```\n\n4. **Package Validation**:\n   ```python\n   def validate_package():\n       \"\"\"Build and validate the package\"\"\"\n       # Clean previous builds\n       if os.path.exists(\"dist\"):\n           shutil.rmtree(\"dist\")\n       \n       # Build package\n       try:\n           subprocess.check_call([\"python\", \"-m\", \"build\"])\n       except subprocess.CalledProcessError:\n           return False, \"Package build failed\"\n       \n       # Check package size\n       wheel_file = glob.glob(\"dist/*.whl\")[0]\n       size_mb = os.path.getsize(wheel_file) / (1024 * 1024)\n       if size_mb > 10:  # Example threshold\n           return False, f\"Package too large: {size_mb:.2f}MB (max 10MB)\"\n       \n       # Validate package structure\n       try:\n           subprocess.check_call([\"twine\", \"check\", \"dist/*\"])\n       except subprocess.CalledProcessError:\n           return False, \"Package validation failed\"\n       \n       return True, \"Package validation passed\"\n   ```\n\n5. **Main Script Structure**:\n   ```python\n   def main():\n       \"\"\"Run all release preparation checks\"\"\"\n       checks = [\n           (\"Git Status\", check_git_status),\n           (\"Version\", check_version),\n           (\"Code Quality\", check_code_quality),\n           (\"Package Validation\", validate_package)\n       ]\n       \n       for name, check_func in checks:\n           print(f\"Running {name} checks...\")\n           success, message = check_func()\n           if not success:\n               print(f\"❌ {name} check failed: {message}\")\n               sys.exit(1)\n           print(f\"✅ {message}\")\n       \n       print(\"✅ All checks passed! Ready for release.\")\n   \n   if __name__ == \"__main__\":\n       main()\n   ```\n\n6. **Add PyProject.toml Script Entry**:\n   Update `pyproject.toml` to include:\n   ```toml\n   [project.scripts]\n   prepare-release = \"scripts.prepare_release:main\"\n   ```\n\nThe script should be designed to fail fast, stopping at the first check that fails with a clear error message. Each check should be modular and return both a success status and a message explaining the result.",
      "testStrategy": "To verify the release preparation script works correctly:\n\n1. **Test Failure Scenarios**:\n   - Create a git repository with uncommitted changes and verify the script fails with the appropriate error message\n   - Create a version that already exists on PyPI and verify the script detects this\n   - Introduce a failing test and verify the script catches it\n   - Create an invalid package structure and verify the script detects it\n\n2. **Test Error Handling**:\n   - Verify the script provides clear, actionable error messages\n   - Confirm the script exits with non-zero status code on failure\n   - Ensure the script stops at the first failure without continuing\n\n3. **Test Success Path**:\n   - Set up a clean environment that meets all requirements\n   - Run the script and verify it completes successfully\n   - Confirm all checks are executed in the correct order\n\n4. **Integration Testing**:\n   - Test the script in a CI environment to ensure it works in automated contexts\n   - Verify the script can be run via the PyProject.toml entry point\n\n5. **Specific Test Cases**:\n   ```bash\n   # Test git status check failure\n   echo \"test\" > temp.txt\n   ./scripts/prepare_release.py  # Should fail with uncommitted changes message\n   git add temp.txt\n   git commit -m \"temp commit\"\n   ./scripts/prepare_release.py  # Should fail with branch sync message\n   \n   # Test version check\n   # (modify version to match existing PyPI version)\n   ./scripts/prepare_release.py  # Should fail with version exists message\n   \n   # Test successful run\n   git checkout main\n   git pull\n   # (ensure clean state and valid version)\n   ./scripts/prepare_release.py  # Should succeed\n   ```\n\nDocument all test scenarios and expected outcomes to ensure comprehensive coverage of the script's functionality.",
      "status": "pending",
      "dependencies": [
        15,
        26
      ],
      "priority": "high",
      "subtasks": []
    },
    {
      "id": 31,
      "title": "Refactor Large Modules for Improved Maintainability",
      "description": "Split large files into smaller, more focused modules to improve maintainability while preserving backward compatibility, following MCP best practices of keeping files under 500 lines of code.",
      "details": "This task involves refactoring large modules in the codebase to improve maintainability while ensuring backward compatibility:\n\n1. **Telemetry Module Refactoring**:\n   - Split the current telemetry.py (1800+ lines) into:\n     - `telemetry/core.py`: Core functionality and base classes\n     - `telemetry/decorators.py`: All telemetry-related decorators\n     - `telemetry/metrics.py`: Metric collection and processing\n     - `telemetry/config.py`: Configuration handling for telemetry\n   - Create appropriate `__init__.py` to re-export all public APIs\n\n2. **Journal Module Refactoring**:\n   - Split journal.py into:\n     - `journal/core.py`: Core journal functionality\n     - `journal/generators.py`: Entry generation logic\n     - `journal/serializers.py`: Serialization/deserialization logic\n   - Create appropriate `__init__.py` to re-export all public APIs\n\n3. **Backward Compatibility**:\n   - Ensure all public APIs are maintained\n   - Use `__init__.py` files to re-export all public functions and classes\n   - Example pattern:\n     ```python\n     # In telemetry/__init__.py\n     from .core import TelemetryManager, initialize_telemetry\n     from .decorators import track_usage, measure_performance\n     from .metrics import MetricCollector, send_metrics\n     from .config import TelemetryConfig, load_config\n\n     # Re-export everything to maintain the original API\n     __all__ = [\n         'TelemetryManager', 'initialize_telemetry',\n         'track_usage', 'measure_performance',\n         'MetricCollector', 'send_metrics',\n         'TelemetryConfig', 'load_config'\n     ]\n     ```\n\n4. **Update Import References**:\n   - Scan the entire codebase for imports from the original modules\n   - Update all import statements to reference the new module structure\n   - Use tools like `grep` or IDE search functionality to find all references\n\n5. **Code Organization Guidelines**:\n   - Follow single responsibility principle for each module\n   - Keep related functionality together\n   - Aim for <500 lines of code per file\n   - Add appropriate docstrings to clarify module purpose\n\n6. **Documentation Updates**:\n   - Update any documentation that references the original module structure\n   - Add module-level docstrings explaining the purpose of each new file",
      "testStrategy": "The refactoring will be verified through the following testing approach:\n\n1. **Baseline Test Execution**:\n   - Run the full test suite before starting refactoring to establish a baseline\n   - Document any existing test failures for reference\n\n2. **Incremental Testing**:\n   - Test each module after refactoring, before moving to the next\n   - Run the specific tests related to each module after refactoring\n\n3. **Full Test Suite Verification**:\n   - Run the complete test suite after all refactoring is complete\n   - Ensure all tests pass with the same results as the baseline\n\n4. **Import Compatibility Testing**:\n   - Create specific tests to verify that all public APIs are still accessible\n   - Test both direct imports and from-imports:\n     ```python\n     # Test direct imports still work\n     import telemetry\n     telemetry.initialize_telemetry()\n     \n     # Test specific imports work\n     from telemetry import initialize_telemetry\n     initialize_telemetry()\n     ```\n\n5. **Integration Testing**:\n   - Verify that components using these modules continue to function correctly\n   - Test the full application workflow to ensure no regressions\n\n6. **Manual Verification**:\n   - Manually verify that all modules are under 500 lines of code\n   - Review import statements across the codebase to ensure they've been updated\n\n7. **Documentation Testing**:\n   - Verify that documentation builds correctly with the new module structure\n   - Test any code examples in documentation to ensure they still work",
      "status": "pending",
      "dependencies": [
        26
      ],
      "priority": "medium",
      "subtasks": []
    },
    {
      "id": 32,
      "title": "Implement Parameter Parsing Leniency for MCP Handlers",
      "description": "Create a flexible parameter parsing system for MCP handlers that accepts common variations in parameter names while maintaining schema integrity.",
      "details": "This task involves implementing a parameter normalization layer to make MCP parameter parsing more flexible:\n\n1. **Parameter Normalization Layer**:\n   - Create a middleware or wrapper function that normalizes incoming parameters before they reach handler functions\n   - Implement in the core MCP request processing pipeline\n   - Design a consistent approach that works across all handlers\n\n2. **Parameter Aliasing Configuration**:\n   - Create a configuration system for parameter aliases with mappings like:\n     ```python\n     PARAMETER_ALIASES = {\n       \"path\": [\"project_path\", \"filepath\", \"file_path\"],\n       \"text\": [\"reflection\", \"content\", \"message\"],\n       \"commit_id\": [\"commit\", \"sha\", \"hash\"],\n       # Add other common variations\n     }\n     ```\n   - Ensure the configuration is extensible and documented\n\n3. **Normalization Logic**:\n   - Implement a function that transforms incoming parameters based on the alias configuration:\n     ```python\n     def normalize_parameters(params, handler_schema):\n         \"\"\"\n         Transform parameters based on aliases to match expected schema\n         while preserving original values when appropriate\n         \"\"\"\n         normalized = params.copy()\n         for expected_param, aliases in PARAMETER_ALIASES.items():\n             if expected_param not in normalized:\n                 for alias in aliases:\n                     if alias in normalized:\n                         normalized[expected_param] = normalized[alias]\n                         break\n         return normalized\n     ```\n\n4. **Schema Integrity**:\n   - Maintain strict schema advertising in API documentation\n   - Add warnings in logs when non-standard parameter names are used\n   - Consider adding deprecation notices for certain aliases to encourage standard usage\n\n5. **Integration**:\n   - Apply normalization before parameter validation\n   - Update all handler functions to use the normalized parameters\n   - Ensure backward compatibility with existing clients\n\n6. **Documentation**:\n   - Document the parameter aliasing system for developers\n   - Update API documentation to note accepted variations where appropriate",
      "testStrategy": "1. **Unit Tests for Normalization**:\n   - Create tests for the parameter normalization function with various input combinations\n   - Verify each alias correctly maps to its canonical parameter name\n   - Test edge cases like conflicting parameters or missing values\n\n2. **Handler Integration Tests**:\n   - For each MCP handler, create test cases that use alternative parameter names\n   - Verify the handler functions correctly with both standard and aliased parameters\n   - Example test cases:\n     ```python\n     def test_commit_handler_with_parameter_aliases():\n         # Test with standard parameters\n         result1 = commit_handler(path=\"/path/to/repo\", message=\"Test commit\")\n         \n         # Test with aliased parameters\n         result2 = commit_handler(project_path=\"/path/to/repo\", reflection=\"Test commit\")\n         \n         # Results should be equivalent\n         assert result1 == result2\n     ```\n\n3. **Backward Compatibility Tests**:\n   - Verify that existing code using standard parameter names continues to work\n   - Run the full test suite to ensure no regressions\n\n4. **MCP Inspector Tests**:\n   - Use the MCP inspector tool to verify parameter handling\n   - Test interactive parameter submission with various aliases\n   - Verify the inspector correctly shows normalized parameters\n\n5. **Error Handling Tests**:\n   - Test scenarios with invalid parameters to ensure proper error messages\n   - Verify that aliasing doesn't interfere with validation logic\n\n6. **Performance Tests**:\n   - Measure any performance impact from the additional normalization layer\n   - Ensure the overhead is minimal for standard parameter usage",
      "status": "pending",
      "dependencies": [
        26
      ],
      "priority": "low",
      "subtasks": []
    },
    {
      "id": 33,
      "title": "Remove All Console Output",
      "description": "Audit and remove all remaining stdout/print statements, replacing them with proper logging and return values to ensure clean operation for MCP clients.",
      "details": "This task involves a systematic audit and cleanup of all console output in the codebase:\n\n1. **Audit Phase**:\n   - Perform a comprehensive search for all `print()` statements, `sys.stdout.write()` calls, and any other direct console output\n   - Create an inventory of all console output locations with their purpose (debug, info, error, etc.)\n   - Categorize outputs as:\n     - Debug/development outputs (to be replaced with logging)\n     - CLI user feedback (to be preserved for human users)\n     - JSON/data outputs (to be converted to return values)\n\n2. **CLI Output Refactoring**:\n   - Modify `cli.py` to properly return values instead of printing JSON:\n   ```python\n   # Before:\n   def get_entries(date_range):\n       entries = journal.get_entries(date_range)\n       print(json.dumps(entries))\n   \n   # After:\n   def get_entries(date_range):\n       entries = journal.get_entries(date_range)\n       return entries  # Click will handle JSON serialization\n   ```\n   - Preserve human-readable help text and error messages in CLI interface\n   - Implement proper exit codes for CLI operations\n\n3. **Logging Implementation**:\n   - Replace all debug/info print statements with appropriate logging calls:\n   ```python\n   # Before:\n   print(f\"Processing commit {commit_id}\")\n   \n   # After:\n   logger.debug(f\"Processing commit {commit_id}\")\n   ```\n   - Ensure all logging uses the file-based logger implemented in Task 28\n   - Add appropriate log levels (DEBUG, INFO, WARNING, ERROR) based on message importance\n\n4. **Return Value Standardization**:\n   - Ensure all functions return proper values instead of printing results\n   - Implement consistent return structures (dictionaries, objects, etc.)\n   - For functions that previously printed status updates, consider adding a callback parameter for progress reporting\n\n5. **MCP Server Cleanup**:\n   - Special focus on MCP server handlers to ensure they never write to stdout\n   - Verify all handlers return proper JSON responses rather than printing them\n   - Implement proper error handling that logs errors but returns appropriate error responses\n\n6. **Exception Handling**:\n   - Review all exception handling to ensure exceptions are logged but not printed\n   - Implement structured error responses for API functions\n\n7. **Documentation Update**:\n   - Update documentation to reflect the new logging approach\n   - Document the return value structures for all public functions",
      "testStrategy": "1. **Automated Output Capture Test**:\n   - Create a test that captures stdout during execution of all major functions\n   - Verify no unexpected output is produced\n   ```python\n   import io\n   import sys\n   from contextlib import redirect_stdout\n   \n   def test_no_stdout_output():\n       f = io.StringIO()\n       with redirect_stdout(f):\n           # Run various operations\n           client.create_entry(commit_id=\"abc123\")\n           client.generate_summary(period=\"day\")\n       \n       output = f.getvalue()\n       assert output == \"\", f\"Unexpected stdout output: {output}\"\n   ```\n\n2. **CLI Command Testing**:\n   - Test all CLI commands with various flags and options\n   - Verify help text is still displayed correctly\n   - Verify error messages are properly shown to users\n   - For commands that should return data, verify the data is correctly returned\n\n3. **Log File Verification**:\n   - Run operations that previously generated console output\n   - Verify appropriate log entries are created in the log file\n   - Check log levels are appropriate for the message content\n\n4. **MCP Client Integration Test**:\n   - Create a test MCP client that consumes the server's responses\n   - Verify the client receives proper return values and not stdout text\n   - Test error conditions to ensure they're properly communicated via return values\n\n5. **Edge Case Testing**:\n   - Test with verbose/debug flags enabled to ensure they affect logging but not stdout\n   - Test with various error conditions to verify errors are logged but not printed\n   - Test concurrent operations to ensure no race conditions in logging\n\n6. **Manual Review**:\n   - Perform a final manual code review to catch any remaining print statements\n   - Run the application with stdout redirected to a file to verify no unexpected output",
      "status": "pending",
      "dependencies": [
        28,
        26
      ],
      "priority": "medium",
      "subtasks": []
    },
    {
      "id": 34,
      "title": "Performance Validation - Auto-test Journal Generation on Every Commit",
      "description": "Create a test harness that automatically measures and validates journal generation performance on every commit, providing continuous feedback on performance metrics and alerting developers when thresholds are exceeded.",
      "details": "Implement a comprehensive performance testing system with the following components:\n\n1. Create `scripts/test_performance.py` that:\n   - Retrieves current commit information using existing git utilities\n   - Implements logic to skip journal-only commits by leveraging existing recursion prevention logic\n   - Creates realistic test fixtures for GitContext, ChatHistory, and TerminalContext objects\n   - Sequentially calls all 8 `generate_*_section` functions from journal.py\n   - Times each function individually using Python's `time` module\n   - Measures memory usage before and after using `psutil` or similar library\n   - Generates a structured performance report containing:\n     - Individual function execution times\n     - Total generation time\n     - Memory usage delta\n     - Context size metrics\n     - Pass/fail status based on defined thresholds\n\n2. Install as a git post-commit hook:\n   ```bash\n   #!/bin/bash\n   # Run performance test in background\n   python scripts/test_performance.py &\n   ```\n   - Ensure the hook runs after every commit in the background\n   - Configure output to `.performance-test-results/YYYY-MM-DD-HH-MM-SS-{commit-hash}.json`\n   - Print a summary to console for immediate developer feedback\n   - Implement warning logic for total time > 15 seconds\n   - Implement error reporting for total time > 30 seconds\n\n3. Define performance thresholds:\n   ```python\n   THRESHOLDS = {\n       \"total_time\": {\n           \"good\": 15,  # seconds\n           \"warning\": 30  # seconds\n       },\n       \"memory_increase\": {\n           \"warning\": 100 * 1024 * 1024,  # 100MB\n           \"critical\": 500 * 1024 * 1024  # 500MB\n       }\n   }\n   ```\n\n4. Create visualization script `scripts/show_performance_trends.py`:\n   - Read all test results from `.performance-test-results/` directory\n   - Generate time-series plots showing performance trends\n   - Identify and highlight the slowest functions\n   - Calculate moving averages to smooth out variations\n   - Provide command-line options for filtering and analysis\n\n5. Integration with existing codebase:\n   - Import all generation functions from journal.py\n   - Use existing context collection functions\n   - Respect journal path configuration from config files\n   - Leverage git utilities for commit analysis\n\n6. Update `.gitignore` to exclude performance test results:\n   ```\n   .performance-test-results/\n   ```",
      "testStrategy": "1. Manual Testing:\n   - Create commits of various sizes and verify the performance test runs automatically\n   - Confirm the script correctly identifies and skips journal-only commits\n   - Validate that performance measurements are accurate by comparing with manual timing\n   - Ensure the hook doesn't interfere with normal git operations or slow down the commit process\n\n2. Functionality Testing:\n   - Verify all 8 `generate_*_section` functions are properly timed individually\n   - Confirm memory measurements are accurate by comparing with system monitoring tools\n   - Test that performance reports are correctly written to the `.performance-test-results/` directory\n   - Validate console output provides clear and accurate performance summaries\n\n3. Threshold Testing:\n   - Artificially create performance bottlenecks to trigger warnings and errors\n   - Verify warning messages appear when total time exceeds 15 seconds\n   - Confirm error messages appear when total time exceeds 30 seconds\n   - Test memory increase warnings at 100MB and 500MB thresholds\n\n4. Visualization Testing:\n   - Generate multiple test results across different commits\n   - Run the visualization script and verify it correctly displays performance trends\n   - Confirm the script accurately identifies the slowest functions\n   - Test filtering options and ensure they work as expected\n\n5. Integration Testing:\n   - Verify the performance test correctly uses existing context collection functions\n   - Confirm it respects the journal path configuration\n   - Test that git utilities are properly used for commit analysis\n   - Ensure the system works end-to-end from commit to performance report",
      "status": "pending",
      "dependencies": [
        27
      ],
      "priority": "critical",
      "subtasks": []
    }
  ]
}