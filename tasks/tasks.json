{
  "tasks": [
    {
      "id": 11,
      "title": "Implement Summary Generation",
      "description": "Create the functionality to generate daily, weekly, monthly, quarterly, and yearly summaries of journal entries, with special emphasis on manual reflections.",
      "status": "pending",
      "dependencies": [],
      "priority": "medium",
      "details": "Implement summary generation in the MCP server with the following features:\n\n1. Date range utilities:\n```python\ndef get_date_range(period, date=None):\n    \"\"\"Get start and end dates for a period\"\"\"\n    if date is None:\n        date = datetime.now().date()\n    elif isinstance(date, str):\n        date = datetime.strptime(date, \"%Y-%m-%d\").date()\n    \n    if period == \"day\":\n        return date, date\n    elif period == \"week\":\n        # Start of week (Monday)\n        start = date - timedelta(days=date.weekday())\n        end = start + timedelta(days=6)\n        return start, end\n    elif period == \"month\":\n        start = date.replace(day=1)\n        # Last day of month\n        next_month = date.replace(day=28) + timedelta(days=4)\n        end = next_month - timedelta(days=next_month.day)\n        return start, end\n    elif period == \"quarter\":\n        # Determine which quarter the date falls in\n        quarter = (date.month - 1) // 3 + 1\n        # Start of quarter (first day of first month in quarter)\n        start_month = (quarter - 1) * 3 + 1\n        start = date.replace(month=start_month, day=1)\n        # End of quarter (last day of last month in quarter)\n        end_month = quarter * 3\n        end_day = 31 if end_month in [3, 12] else 30 if end_month in [6, 9] else 28\n        if end_month == 2 and (date.year % 4 == 0 and (date.year % 100 != 0 or date.year % 400 == 0)):\n            end_day = 29  # Leap year\n        end = date.replace(month=end_month, day=end_day)\n        return start, end\n    elif period == \"year\":\n        start = date.replace(month=1, day=1)\n        end = date.replace(month=12, day=31)\n        return start, end\n    else:\n        raise ValueError(f\"Unknown period: {period}\")\n```\n\n2. Journal file collection:\n```python\ndef get_journal_files_in_range(start_date, end_date, config):\n    \"\"\"Get journal files in date range\"\"\"\n    files = []\n    current = start_date\n    while current <= end_date:\n        file_path = Path(config[\"journal\"][\"path\"]) / \"daily\" / f\"{current.strftime('%Y-%m-%d')}.md\"\n        if file_path.exists():\n            files.append(file_path)\n        current += timedelta(days=1)\n    return files\n```\n\n3. Summary generation with manual reflection prioritization:\n```python\ndef generate_summary(files, period, config):\n    \"\"\"Generate summary from journal files\"\"\"\n    # Extract content from files\n    entries = []\n    manual_reflections = []\n    \n    for file_path in files:\n        with open(file_path, \"r\") as f:\n            content = f.read()\n            # Extract entries and reflections\n            # Extract manual reflections from special sections\n            reflection_sections = extract_manual_reflections(content, file_path.stem)\n            if reflection_sections:\n                manual_reflections.extend(reflection_sections)\n            # Extract regular entries\n            # Implementation\n    \n    # Analyze entries for significance/complexity\n    weighted_entries = []\n    for entry in entries:\n        # Determine entry significance based on factors like:\n        # - Length/detail of the entry\n        # - Presence of technical terms or complex concepts\n        # - Keywords indicating substantial work (\"implemented\", \"designed\", \"solved\")\n        # - Absence of trivial indicators (\"minor fix\", \"typo\", \"small change\")\n        significance_score = calculate_entry_significance(entry)\n        weighted_entries.append((entry, significance_score))\n    \n    # Sort entries by significance score to prioritize important work\n    weighted_entries.sort(key=lambda x: x[1], reverse=True)\n    \n    # Generate summary sections\n    summary = []\n    \n    # Add manual reflections section first - always prioritized\n    if manual_reflections:\n        summary.append(\"# 📝 Manual Reflections\\n\")\n        summary.append(\"*These are your own reflections from the period, presented verbatim.*\\n\")\n        formatted_reflections = []\n        for date, reflection in manual_reflections:\n            formatted_reflections.append(f\"## {date}\\n\\n{reflection}\\n\")\n        summary.append(\"\\n\".join(formatted_reflections))\n    \n    # Add other sections\n    summary.append(\"# Summary\\n\")\n    # Generate overall summary with emphasis on significant entries\n    \n    summary.append(\"# Key Accomplishments\\n\")\n    # Extract accomplishments, prioritizing substantial work\n    \n    summary.append(\"# Challenges\\n\")\n    # Extract challenges, focusing on complex problems\n    \n    summary.append(\"# Technical Decisions\\n\")\n    # Extract decisions, highlighting important architectural choices\n    \n    return \"\\n\\n\".join(summary)\n\ndef extract_manual_reflections(content, date_str):\n    \"\"\"Extract manual reflections from journal content\"\"\"\n    reflections = []\n    \n    # Look for reflection sections with patterns like:\n    # ## Reflection\n    # ## Daily Reflection\n    # ## Personal Reflection\n    # etc.\n    \n    reflection_patterns = [\n        r\"#+\\s*(?:Daily\\s*)?Reflection[s]?\\s*\\n([\\s\\S]*?)(?:\\n#+\\s|$)\",\n        r\"#+\\s*(?:Personal\\s*)?Thought[s]?\\s*\\n([\\s\\S]*?)(?:\\n#+\\s|$)\",\n        r\"#+\\s*(?:Manual\\s*)?Note[s]?\\s*\\n([\\s\\S]*?)(?:\\n#+\\s|$)\"\n    ]\n    \n    for pattern in reflection_patterns:\n        matches = re.finditer(pattern, content, re.MULTILINE)\n        for match in matches:\n            reflection_text = match.group(1).strip()\n            if reflection_text:  # Only add non-empty reflections\n                reflections.append((date_str, reflection_text))\n    \n    return reflections\n\ndef calculate_entry_significance(entry):\n    \"\"\"Calculate significance score for an entry to prioritize substantial work\"\"\"\n    score = 0\n    \n    # Base score from length (longer entries often indicate more substantial work)\n    score += min(len(entry) / 100, 5)  # Cap at 5 points for length\n    \n    # Keywords indicating substantial work\n    substantial_indicators = [\n        \"implement\", \"design\", \"architecture\", \"refactor\", \"optimize\", \n        \"solve\", \"complex\", \"challenge\", \"significant\", \"major\"\n    ]\n    \n    # Keywords indicating trivial work\n    trivial_indicators = [\n        \"typo\", \"minor fix\", \"small change\", \"tweak\", \"trivial\", \n        \"cosmetic\", \"rename\", \"formatting\"\n    ]\n    \n    # Add points for substantial work indicators\n    for word in substantial_indicators:\n        if word in entry.lower():\n            score += 2\n    \n    # Subtract points for trivial work indicators\n    for word in trivial_indicators:\n        if word in entry.lower():\n            score -= 1.5\n    \n    # Analyze for technical complexity\n    # (This could be enhanced with more sophisticated NLP in the future)\n    technical_terms = [\"algorithm\", \"database\", \"architecture\", \"performance\", \"security\"]\n    for term in technical_terms:\n        if term in entry.lower():\n            score += 1\n    \n    return max(score, 0)  # Ensure score doesn't go negative\n```\n\n4. Summary file saving:\n```python\ndef save_summary(content, period, date, config):\n    \"\"\"Save summary to appropriate file\"\"\"\n    if period == \"day\":\n        file_name = f\"{date.strftime('%Y-%m-%d')}-summary.md\"\n        dir_path = Path(config[\"journal\"][\"path\"]) / \"summaries\" / \"daily\"\n    elif period == \"week\":\n        # Get week number\n        week_num = date.isocalendar()[1]\n        file_name = f\"{date.strftime('%Y-%m')}-week{week_num}.md\"\n        dir_path = Path(config[\"journal\"][\"path\"]) / \"summaries\" / \"weekly\"\n    elif period == \"month\":\n        file_name = f\"{date.strftime('%Y-%m')}.md\"\n        dir_path = Path(config[\"journal\"][\"path\"]) / \"summaries\" / \"monthly\"\n    elif period == \"quarter\":\n        # Determine which quarter the date falls in\n        quarter = (date.month - 1) // 3 + 1\n        file_name = f\"{date.strftime('%Y')}-Q{quarter}.md\"\n        dir_path = Path(config[\"journal\"][\"path\"]) / \"summaries\" / \"quarterly\"\n    elif period == \"year\":\n        file_name = f\"{date.strftime('%Y')}.md\"\n        dir_path = Path(config[\"journal\"][\"path\"]) / \"summaries\" / \"yearly\"\n    else:\n        raise ValueError(f\"Unknown period: {period}\")\n    \n    # Create file path\n    file_path = dir_path / file_name\n    \n    # Ensure directory exists using on-demand directory creation pattern\n    ensure_journal_directory(dir_path)\n    \n    # Save file\n    with open(file_path, \"w\") as f:\n        f.write(content)\n    \n    return file_path\n```\n\n5. MCP handler implementation:\n```python\n@trace_operation(\"journal_summarize\")\nasync def handle_summarize(request):\n    \"\"\"Handle journal/summarize operation\"\"\"\n    period = request.get(\"period\", \"day\")\n    date = request.get(\"date\")\n    date_range = request.get(\"range\")\n    \n    # Load config\n    config = load_config()\n    \n    # Get date range\n    if date_range:\n        # Parse range (format: \"YYYY-MM-DD:YYYY-MM-DD\")\n        start_str, end_str = date_range.split(\":\")\n        start_date = datetime.strptime(start_str, \"%Y-%m-%d\").date()\n        end_date = datetime.strptime(end_str, \"%Y-%m-%d\").date()\n    else:\n        start_date, end_date = get_date_range(period, date)\n    \n    # Get journal files\n    files = get_journal_files_in_range(start_date, end_date, config)\n    if not files:\n        return {\"status\": \"error\", \"error\": \"No journal entries found in date range\"}\n    \n    # Generate summary\n    content = generate_summary(files, period, config)\n    \n    # Save summary\n    file_path = save_summary(content, period, start_date, config)\n    \n    return {\n        \"status\": \"success\",\n        \"file_path\": str(file_path),\n        \"content\": content\n    }\n```\n\n6. Directory creation utility:\n```python\ndef ensure_journal_directory(dir_path):\n    \"\"\"Ensure the journal directory exists, creating it if necessary\"\"\"\n    if not dir_path.exists():\n        dir_path.mkdir(parents=True, exist_ok=True)\n        logger.info(f\"Created directory: {dir_path}\")\n    return dir_path\n```\n\n7. On-demand directory creation pattern:\n- All summary file-writing operations must use the on-demand directory creation pattern\n- Directories should only be created when needed, not upfront\n- All summary-writing functions (including save_summary) must call ensure_journal_directory(file_path) before writing\n- See docs/on-demand-directory-pattern.md for implementation details and test patterns\n\n8. Manual reflection prioritization:\n- Manual reflections must be prominently displayed at the beginning of summaries\n- Use visual distinction (emoji, formatting) to highlight manual reflections\n- Include date context for each reflection\n- Preserve the original wording of manual reflections\n- Implement reflection extraction from common section patterns\n- Ensure manual reflections are always prioritized over inferred content\n\nNote: This implementation focuses solely on MCP/AI agent operations for summary generation. CLI functionality is limited to setup commands (journal-init, install-hook) only. Refer to updated documentation for details.",
      "testStrategy": "1. Unit tests for date range utilities\n   - Test all periods (day, week, month, quarter, year)\n   - Test edge cases like quarter boundaries\n   - Test leap year handling for February in quarterly calculations\n2. Tests for journal file collection\n3. Tests for summary generation\n4. Tests for summary file saving\n   - Test saving for all periods (daily, weekly, monthly, quarterly, yearly)\n   - Test correct file naming for quarterly summaries (YYYY-Q1, YYYY-Q2, etc.)\n5. Tests for MCP handler implementation\n6. Tests for handling different periods (day, week, month, quarter, year)\n7. Tests for handling date ranges\n8. Integration tests for full summary generation flow\n9. Tests for entry significance calculation\n10. Tests to verify that substantial work is properly prioritized in summaries\n11. Tests to verify that trivial entries are de-emphasized in summaries\n12. Tests with mixed entry types to ensure proper weighting in the final summary\n13. Tests for on-demand directory creation:\n    - Test that summary directories are created automatically when they don't exist\n    - Test that ensure_journal_directory() is called for all summary types (daily, weekly, monthly, quarterly, yearly)\n    - Test that directory creation works with nested paths\n    - Test that no errors occur when directories already exist\n    - Test that directories are only created when needed, not upfront\n    - Verify that all summary-writing functions call ensure_journal_directory() before writing\n    - Follow test patterns described in docs/on-demand-directory-pattern.md\n14. Tests to verify that summarization is available as an MCP operation\n15. Tests to verify that the AI agent can properly interact with the summarization functionality\n16. Verify that summary generation works correctly through the MCP interface only (not CLI)\n17. Test that the AI agent can request summaries for different time periods and date ranges\n18. Tests for manual reflection extraction:\n    - Test extraction from various section formats (## Reflection, ## Daily Reflection, etc.)\n    - Test with multiple reflection sections in a single file\n    - Test with reflection sections containing various formatting (lists, code blocks, etc.)\n    - Test with empty reflection sections\n    - Test with reflection sections at different positions in the file\n19. Tests for manual reflection prioritization:\n    - Verify that manual reflections appear at the beginning of summaries\n    - Verify that manual reflections are visually distinguished\n    - Verify that date context is included for each reflection\n    - Verify that original wording is preserved\n    - Test with mixed content (manual reflections and regular entries)\n    - Test with only manual reflections\n    - Test with no manual reflections\n20. Tests for quarterly summary generation:\n    - Test correct date range calculation for each quarter\n    - Test correct file naming (YYYY-Q1, YYYY-Q2, etc.)\n    - Test with entries spanning multiple months within a quarter\n    - Test with entries at quarter boundaries",
      "subtasks": [
        {
          "id": "11.1",
          "title": "Implement entry significance calculation",
          "description": "Create the algorithm to analyze journal entries and assign significance scores based on content analysis.",
          "status": "pending"
        },
        {
          "id": "11.2",
          "title": "Modify summary generation to prioritize significant entries",
          "description": "Update the summary generation logic to give more narrative weight to entries with higher significance scores.",
          "status": "pending"
        },
        {
          "id": "11.3",
          "title": "Create test cases for entry significance calculation",
          "description": "Develop test cases with various types of entries (substantial, trivial, mixed) to verify proper significance scoring.",
          "status": "pending"
        },
        {
          "id": "11.4",
          "title": "Test summary prioritization with real-world examples",
          "description": "Test the summary generation with a set of real-world journal entries to ensure meaningful work is properly highlighted.",
          "status": "pending"
        },
        {
          "id": "11.5",
          "title": "Implement ensure_journal_directory utility",
          "description": "Create the utility function to ensure journal directories exist, creating them on-demand if necessary.",
          "status": "pending"
        },
        {
          "id": "11.6",
          "title": "Update save_summary to use ensure_journal_directory",
          "description": "Modify the save_summary function to use the ensure_journal_directory utility for all summary types.",
          "status": "pending"
        },
        {
          "id": "11.7",
          "title": "Add tests for directory creation functionality",
          "description": "Create tests to verify that summary directories are created automatically when they don't exist and that the ensure_journal_directory utility works correctly.",
          "status": "pending"
        },
        {
          "id": "11.8",
          "title": "Implement on-demand directory creation pattern",
          "description": "Update all summary file-writing operations to follow the on-demand directory creation pattern as described in docs/on-demand-directory-pattern.md.",
          "status": "pending"
        },
        {
          "id": "11.9",
          "title": "Add tests for on-demand directory creation",
          "description": "Create tests to verify that directories are only created when needed, not upfront, and that all summary-writing functions call ensure_journal_directory() before writing.",
          "status": "pending"
        },
        {
          "id": "11.10",
          "title": "Review and update all file-writing operations",
          "description": "Review all file-writing operations in the codebase to ensure they follow the on-demand directory creation pattern.",
          "status": "pending"
        },
        {
          "id": "11.11",
          "title": "Verify MCP operation for summarization",
          "description": "Ensure that summarization is properly implemented as an MCP operation and accessible to the AI agent.",
          "status": "pending"
        },
        {
          "id": "11.12",
          "title": "Test AI agent interaction with summarization",
          "description": "Create tests to verify that the AI agent can properly request and process summary generation through the MCP server.",
          "status": "pending"
        },
        {
          "id": "11.13",
          "title": "Ensure summary generation is MCP-only",
          "description": "Verify that summary generation functionality is only available through the MCP interface and not through CLI commands.",
          "status": "pending"
        },
        {
          "id": "11.14",
          "title": "Update documentation to reflect MCP-only approach",
          "description": "Update relevant documentation to clarify that summary generation is only available through the MCP/AI agent interface, not through CLI commands.",
          "status": "pending"
        },
        {
          "id": "11.15",
          "title": "Implement manual reflection extraction",
          "description": "Create functionality to extract manual reflections from journal entries using pattern matching for common section headers.",
          "status": "pending"
        },
        {
          "id": "11.16",
          "title": "Implement manual reflection prioritization in summaries",
          "description": "Update summary generation to display manual reflections prominently at the beginning with visual distinction and date context.",
          "status": "pending"
        },
        {
          "id": "11.17",
          "title": "Add tests for manual reflection extraction",
          "description": "Create tests to verify that manual reflections are correctly extracted from various section formats and positions.",
          "status": "pending"
        },
        {
          "id": "11.18",
          "title": "Add tests for manual reflection prioritization",
          "description": "Create tests to verify that manual reflections appear at the beginning of summaries with proper visual distinction and preserved wording.",
          "status": "pending"
        },
        {
          "id": "11.19",
          "title": "Implement quarterly summary support",
          "description": "Add support for generating quarterly summaries, including date range calculation and file naming conventions.",
          "status": "pending"
        },
        {
          "id": "11.20",
          "title": "Create tests for quarterly summary generation",
          "description": "Develop tests to verify correct date range calculation, file naming, and content generation for quarterly summaries.",
          "status": "pending"
        },
        {
          "id": "11.21",
          "title": "Update documentation to include quarterly summaries",
          "description": "Update relevant documentation to include information about quarterly summary generation and usage.",
          "status": "pending"
        }
      ]
    },
    {
      "id": 12,
      "title": "Implement Blog Post Generation",
      "description": "Create the functionality to convert journal entries and summaries into blog post format for storytelling.",
      "details": "Implement blog post generation in both the MCP server and CLI with the following features:\n\n1. Blog post generation:\n```python\ndef generate_blog_post(files, config):\n    \"\"\"Generate blog post from journal files\"\"\"\n    # Extract content from files\n    entries = []\n    \n    for file_path in files:\n        with open(file_path, \"r\") as f:\n            content = f.read()\n            # Extract entries\n            # Implementation\n    \n    # Generate blog post sections\n    blog_post = []\n    \n    # Add title and introduction\n    blog_post.append(\"# Project Journey: From Idea to Implementation\\n\")\n    blog_post.append(\"*An engineering story based on journal entries*\\n\")\n    \n    # Add narrative sections\n    blog_post.append(\"## The Challenge\\n\")\n    # Generate challenge narrative\n    \n    blog_post.append(\"## The Approach\\n\")\n    # Generate approach narrative\n    \n    blog_post.append(\"## Key Decisions\\n\")\n    # Extract and narrate decisions\n    \n    blog_post.append(\"## Lessons Learned\\n\")\n    # Extract and narrate lessons\n    \n    blog_post.append(\"## Conclusion\\n\")\n    # Generate conclusion\n    \n    return \"\\n\\n\".join(blog_post)\n```\n\n2. Blog post file saving:\n```python\ndef save_blog_post(content, title, config):\n    \"\"\"Save blog post to file\"\"\"\n    # Create directory if needed\n    dir_path = Path(config[\"journal\"][\"path\"]) / \"blog_posts\"\n    dir_path.mkdir(parents=True, exist_ok=True)\n    \n    # Generate file name from title\n    file_name = title.lower().replace(\" \", \"-\") + \".md\"\n    file_path = dir_path / file_name\n    \n    # Save file\n    with open(file_path, \"w\") as f:\n        f.write(content)\n    \n    return file_path\n```\n\n3. MCP handler implementation:\n```python\n@trace_operation(\"journal_blogify\")\nasync def handle_blogify(request):\n    \"\"\"Handle journal/blogify operation\"\"\"\n    files = request.get(\"files\", [])\n    title = request.get(\"title\", \"Engineering Journey\")\n    \n    if not files:\n        return {\"status\": \"error\", \"error\": \"No files provided\"}\n    \n    # Load config\n    config = load_config()\n    \n    # Convert file paths to Path objects\n    file_paths = [Path(f) for f in files]\n    \n    # Check if files exist\n    missing = [str(f) for f in file_paths if not f.exists()]\n    if missing:\n        return {\"status\": \"error\", \"error\": f\"Files not found: {', '.join(missing)}\"}\n    \n    # Generate blog post\n    content = generate_blog_post(file_paths, config)\n    \n    # Save blog post\n    file_path = save_blog_post(content, title, config)\n    \n    return {\n        \"status\": \"success\",\n        \"file_path\": str(file_path),\n        \"content\": content\n    }\n```\n\n4. CLI command implementation:\n```python\n@cli.command()\n@click.argument(\"files\", nargs=-1, type=click.Path(exists=True))\n@click.option(\"--title\", default=\"Engineering Journey\", help=\"Blog post title\")\n@click.option(\"--debug\", is_flag=True, help=\"Show debug information\")\ndef blogify(files, title, debug):\n    \"\"\"Convert journal entries to blog post\"\"\"\n    try:\n        if not files:\n            click.echo(\"No files provided\")\n            return\n        \n        # Load config\n        config = load_config()\n        \n        # Convert file paths to Path objects\n        file_paths = [Path(f) for f in files]\n        \n        # Generate blog post\n        content = generate_blog_post(file_paths, config)\n        \n        # Save blog post\n        file_path = save_blog_post(content, title, config)\n        \n        click.echo(f\"Blog post saved to {file_path}\")\n    except Exception as e:\n        if debug:\n            click.echo(f\"Error: {e}\")\n            traceback.print_exc()\n        else:\n            click.echo(f\"Error: {e}\")\n```",
      "testStrategy": "1. Unit tests for blog post generation\n2. Tests for blog post file saving\n3. Tests for MCP handler implementation\n4. Tests for CLI command implementation\n5. Tests for handling multiple input files\n6. Tests for narrative generation\n7. Integration tests for full blog post generation flow",
      "priority": "low",
      "dependencies": [],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 13,
      "title": "Implement Backfill Functionality",
      "description": "Create the functionality to detect and generate journal entries for missed commits.",
      "details": "Implement backfill functionality in both the MCP server and CLI with the following features:\n\n1. Missed commit detection:\n```python\ndef get_missed_commits(repo, config):\n    \"\"\"Get commits that don't have journal entries\"\"\"\n    # Get journal directory\n    journal_path = Path(config[\"journal\"][\"path\"])\n    \n    # Get all commits\n    commits = list(repo.iter_commits())\n    \n    # Get all journal files\n    journal_files = list(journal_path.glob(\"daily/*.md\"))\n    \n    # Extract commit hashes from journal files\n    journal_commits = set()\n    for file_path in journal_files:\n        with open(file_path, \"r\") as f:\n            content = f.read()\n            # Extract commit hashes using regex\n            # Implementation\n    \n    # Find commits not in journal\n    missed_commits = []\n    for commit in commits:\n        if commit.hexsha not in journal_commits and not is_journal_only_commit(commit, config[\"journal\"][\"path\"]):\n            missed_commits.append(commit)\n    \n    return missed_commits\n```\n\n2. Backfill processing:\n```python\ndef process_backfill(commits, config, debug=False):\n    \"\"\"Process backfill for missed commits\"\"\"\n    results = []\n    \n    # Sort commits by date\n    commits.sort(key=lambda c: c.committed_date)\n    \n    for commit in commits:\n        # Generate entry\n        entry = generate_journal_entry(commit, config, debug)\n        if not entry:\n            continue\n        \n        # Mark as backfilled\n        entry.is_backfilled = True\n        \n        # Save entry\n        file_path = save_journal_entry(entry, config)\n        \n        results.append({\n            \"commit\": commit.hexsha,\n            \"file_path\": str(file_path)\n        })\n    \n    return results\n```\n\n3. MCP handler implementation:\n```python\n@trace_operation(\"journal_backfill\")\nasync def handle_backfill(request):\n    \"\"\"Handle journal/backfill operation\"\"\"\n    debug = request.get(\"debug\", False)\n    \n    # Load config\n    config = load_config()\n    \n    # Get repo\n    repo = get_repo()\n    \n    # Get missed commits\n    missed_commits = get_missed_commits(repo, config)\n    if not missed_commits:\n        return {\"status\": \"success\", \"message\": \"No missed commits found\"}\n    \n    # Process backfill\n    results = process_backfill(missed_commits, config, debug)\n    \n    return {\n        \"status\": \"success\",\n        \"count\": len(results),\n        \"entries\": results\n    }\n```\n\n4. CLI command implementation:\n```python\n@cli.command()\n@click.option(\"--debug\", is_flag=True, help=\"Show debug information\")\ndef backfill(debug):\n    \"\"\"Check for missed commits and create entries\"\"\"\n    try:\n        # Load config\n        config = load_config()\n        \n        # Get repo\n        repo = get_repo()\n        \n        # Get missed commits\n        missed_commits = get_missed_commits(repo, config)\n        if not missed_commits:\n            click.echo(\"No missed commits found\")\n            return\n        \n        # Process backfill\n        results = process_backfill(missed_commits, config, debug)\n        \n        click.echo(f\"Created {len(results)} journal entries for missed commits\")\n        for result in results:\n            click.echo(f\"  - {result['commit'][:8]}: {result['file_path']}\")\n    except Exception as e:\n        if debug:\n            click.echo(f\"Error: {e}\")\n            traceback.print_exc()\n        else:\n            click.echo(f\"Error: {e}\")\n```",
      "testStrategy": "1. Unit tests for missed commit detection\n2. Tests for backfill processing\n3. Tests for MCP handler implementation\n4. Tests for CLI command implementation\n5. Tests for handling journal-only commits\n6. Tests for chronological ordering of backfilled entries\n7. Integration tests for full backfill flow",
      "priority": "medium",
      "dependencies": [],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 15,
      "title": "Create Comprehensive Tests and Documentation",
      "description": "Develop comprehensive tests for all components and create documentation for the project.",
      "status": "pending",
      "dependencies": [
        11,
        12,
        13
      ],
      "priority": "high",
      "details": "Create comprehensive tests and documentation with the following features:\n\n1. Test fixtures:\n```python\n@pytest.fixture\ndef mock_git_repo():\n    \"\"\"Create temporary git repo with test commits\"\"\"\n    # Implementation\n\n@pytest.fixture\ndef sample_journal_entries():\n    \"\"\"Load sample journal files\"\"\"\n    # Implementation\n\n@pytest.fixture\ndef mock_terminal_history():\n    \"\"\"Provide test terminal command history\"\"\"\n    # Implementation\n\n@pytest.fixture\ndef mock_chat_history():\n    \"\"\"Provide test chat history\"\"\"\n    # Implementation\n\n@pytest.fixture\ndef mock_telemetry_exporter():\n    \"\"\"Provide a test exporter that captures telemetry events\"\"\"\n    # Implementation\n```\n\n2. Unit tests:\n```python\ndef test_config_loading():\n    \"\"\"Test configuration loading\"\"\"\n    # Implementation\n\ndef test_git_utils():\n    \"\"\"Test git utilities\"\"\"\n    # Implementation\n\ndef test_journal_entry_generation():\n    \"\"\"Test journal entry generation\"\"\"\n    # Implementation\n\ndef test_telemetry():\n    \"\"\"Test telemetry integration\"\"\"\n    # Implementation\n\n# Additional unit tests for all components\n```\n\n3. Integration tests:\n```python\ndef test_cli_init():\n    \"\"\"Test CLI init command\"\"\"\n    # Implementation\n\ndef test_cli_new_entry():\n    \"\"\"Test CLI new-entry command\"\"\"\n    # Implementation\n\ndef test_mcp_server():\n    \"\"\"Test MCP server operations\"\"\"\n    # Implementation\n\n# Additional integration tests for all workflows\n```\n\n4. Documentation:\n   - README.md with project overview, installation, and usage\n   - Configuration documentation\n   - CLI command reference\n   - MCP server API reference\n   - Development guide\n   - Examples and tutorials\n\n5. Test coverage:\n   - Configure pytest-cov for coverage reporting\n   - Ensure >90% test coverage\n   - Add coverage reporting to CI pipeline\n\n6. Documentation structure:\n```\nREADME.md\ndocs/\n├── configuration.md\n├── cli.md\n├── mcp-server.md\n├── development.md\n└── examples/\n    ├── basic-usage.md\n    ├── custom-configuration.md\n    └── integration-examples.md\n```",
      "testStrategy": "1. Verify test coverage meets >90% threshold\n2. Ensure all components have unit tests\n3. Verify integration tests cover all workflows\n4. Test documentation for accuracy and completeness\n5. Verify examples work as documented\n6. Test installation and usage instructions\n7. Verify CI pipeline runs all tests\n8. Ensure telemetry system is thoroughly tested with both unit and integration tests",
      "subtasks": [
        {
          "id": "15.1",
          "title": "Implement telemetry-specific tests",
          "description": "Create comprehensive tests for the telemetry system implemented in task 4",
          "status": "pending",
          "details": "Develop unit and integration tests for the telemetry infrastructure including:\n1. Test telemetry event generation\n2. Test telemetry data collection\n3. Test telemetry exporters\n4. Test telemetry configuration options\n5. Test telemetry integration with other components"
        },
        {
          "id": "15.2",
          "title": "Document telemetry system",
          "description": "Create documentation for the telemetry system",
          "status": "pending",
          "details": "Add telemetry documentation including:\n1. Overview of telemetry capabilities\n2. Configuration options for telemetry\n3. How to extend telemetry with custom exporters\n4. Privacy considerations\n5. Add a telemetry.md file to the docs directory"
        }
      ]
    },
    {
      "id": 19,
      "title": "Document MCP Server Configuration and Integration",
      "description": "Ensure the MCP server launch/discovery/configuration requirements are documented in the PRD, README, and codebase. The MCP server must be launchable as a standalone process, expose the required journal operations, and be discoverable by compatible clients. The method for launching the MCP server is not prescribed; it may be started via CLI, Python entry point, etc.",
      "details": "",
      "testStrategy": "",
      "status": "pending",
      "dependencies": [],
      "priority": "medium",
      "subtasks": [
        {
          "id": 1,
          "title": "Provide generic client/editor config block example",
          "description": "Add a JSON example of a configuration block for connecting to the MCP server, showing command, args, and optional env vars.",
          "details": "",
          "status": "pending",
          "dependencies": [],
          "parentTaskId": 19
        },
        {
          "id": 2,
          "title": "Clarify API key/env var requirements",
          "description": "Document that API keys or environment variables are only required if the underlying SDK or provider needs them, not for all deployments.",
          "details": "",
          "status": "pending",
          "dependencies": [],
          "parentTaskId": 19
        },
        {
          "id": 3,
          "title": "Ensure separation of MCP server config from journal config",
          "description": "Make sure documentation clearly distinguishes between MCP server configuration and the journal system's .mcp-journalrc.yaml.",
          "details": "",
          "status": "pending",
          "dependencies": [],
          "parentTaskId": 19
        },
        {
          "id": 4,
          "title": "Review and update README/docs",
          "description": "Review and update the README.md and other documentation to reflect changes made in this task. Ensure documentation is clear, accurate, and up to date.",
          "details": "",
          "status": "pending",
          "dependencies": [
            "19.1",
            "19.2",
            "19.3"
          ],
          "parentTaskId": 19
        }
      ]
    },
    {
      "id": 21,
      "title": "Integrate Codecov for Test Coverage Reporting",
      "description": "Set up Codecov integration with the GitHub repository to track and report test coverage metrics, culminating in a functional coverage badge in the README.",
      "details": "This task involves establishing a connection between the repository and Codecov to enable automated test coverage reporting. Implementation steps include:\n\n1. Create a Codecov account if not already available and link it to the organization's GitHub account\n2. Add the repository to Codecov's dashboard\n3. Generate a Codecov token for secure communication between CI and Codecov\n4. Update the CI pipeline configuration (GitHub Actions, CircleCI, etc.) to:\n   - Install necessary coverage tools (e.g., pytest-cov for Python)\n   - Run tests with coverage collection enabled\n   - Upload coverage reports to Codecov using the token\n5. Add a `.codecov.yml` configuration file to the repository root to customize coverage settings (thresholds, exclusions, etc.)\n6. Uncomment or add the Codecov badge in the README.md file using the format provided by Codecov\n7. Verify the badge displays the actual coverage percentage after the first successful upload\n\nConsider setting coverage thresholds to maintain code quality and potentially configure PR comments from Codecov to highlight coverage changes in code reviews.",
      "testStrategy": "To verify successful completion of this task:\n\n1. Manually trigger a CI build and confirm the coverage report is generated and uploaded to Codecov\n2. Check the Codecov dashboard to ensure:\n   - The repository appears with correct coverage data\n   - Historical data begins tracking from the first upload\n   - Coverage reports include all relevant files (no critical omissions)\n3. Verify the Codecov badge in the README:\n   - Badge is properly displayed (not broken)\n   - Badge shows an actual percentage value (not \"unknown\" or \"N/A\")\n   - The percentage matches what's shown in the Codecov dashboard\n4. Create a test PR with code changes that would affect coverage (both positively and negatively) to confirm:\n   - Codecov reports the coverage change in the PR\n   - The badge updates accordingly after merging\n5. Document the integration process in the project documentation for future reference\n6. Have another team member verify they can access the Codecov dashboard for the repository",
      "status": "pending",
      "dependencies": [],
      "priority": "medium",
      "subtasks": []
    },
    {
      "id": 22,
      "title": "Implement Remaining MCP Server Handlers",
      "description": "Add the remaining non-MVP MCP tool handlers to complete the full feature set after their backend dependencies are implemented.",
      "status": "pending",
      "dependencies": [
        11,
        12,
        13
      ],
      "priority": "medium",
      "details": "Implement the remaining MCP server tool handlers in `src/mcp_commit_story/server.py` to complete the full feature set:\n\n1. **journal/summarize** handler:\n   - Depends on Task 11 (Summary Generation)\n   - Handle daily, weekly, monthly, yearly summary requests\n   - Return summary content and file paths\n   - Must use on-demand directory creation pattern\n\n2. **journal/blogify** handler:\n   - Depends on Task 12 (Blog Post Generation)\n   - Convert journal entries to blog post format\n   - Accept multiple file inputs\n   - Must use on-demand directory creation pattern\n\n3. **journal/backfill** handler:\n   - Depends on Task 13 (Backfill Functionality)\n   - Detect and create entries for missed commits\n   - Return list of created entries\n   - Must use on-demand directory creation pattern\n\n4. **journal/add-reflection** handler:\n   - Add reflection content to existing journal entries\n   - Accept entry path and reflection content\n   - Must use on-demand directory creation pattern\n\nAll handlers should:\n- Use existing `@handle_mcp_error` decorator\n- Follow TypedDict patterns established in Tasks 6.3-6.4\n- Include proper async/await support\n- Integrate with existing backend logic from their dependency tasks\n- Include comprehensive error handling and validation\n- Call ensure_journal_directory(file_path) before writing any files\n- Never create directories upfront - only on demand when needed\n- Implement as MCP operations only (no CLI commands required)\n- Focus exclusively on MCP/AI agent operations for file-writing handlers\n\nNote that the CLI functionality is limited to setup commands (journal-init, install-hook) only. All file-writing functionality must be implemented as MCP operations. Refer to the updated engineering spec and README.md for implementation details and test patterns.",
      "testStrategy": "1. Unit tests for each new handler\n2. Integration tests with backend logic\n3. Error handling validation\n4. End-to-end workflow testing\n5. Backward compatibility with existing handlers\n6. Verify on-demand directory creation pattern is used correctly\n7. Test that directories are only created when files are actually written\n8. Verify ensure_journal_directory() is called before file writes\n9. Verify all file-writing functionality is accessible via MCP operations only\n10. Test the journal/add-reflection handler functionality as an MCP operation",
      "subtasks": []
    },
    {
      "id": 26,
      "title": "Create Packaging Strategy and Release Process for MVP Launch",
      "description": "Develop a comprehensive packaging and distribution strategy for the MCP Commit Story MVP, including PyPI publishing, version management, installation methods, and release processes.",
      "status": "pending",
      "dependencies": [],
      "priority": "high",
      "details": "This task involves creating a complete packaging strategy and implementation plan for the MCP Commit Story MVP launch:\n\n1. **Distribution Strategy**:\n   - Set up PyPI package configuration with appropriate metadata in setup.py/pyproject.toml\n   - Implement semantic versioning (MAJOR.MINOR.PATCH) with version tracking in a dedicated file\n   - Configure CI/CD pipeline for automated releases using GitHub Actions or similar\n   - Define package dependencies with appropriate version constraints\n   - Create package structure with proper namespacing\n\n2. **Installation Methods**:\n   - Implement standard pip installation: `pip install mcp-commit-story`\n   - Create development installation process: `pip install -e .` with dev dependencies\n   - Document MCP server deployment options (standalone, Docker, etc.)\n   - Write detailed configuration guides for different environments\n\n3. **Release Process**:\n   - Implement automated version tagging and changelog generation\n   - Create pre-release testing checklist and validation procedures\n   - Set up documentation update workflow tied to releases\n   - Document rollback procedures for failed releases\n   - Establish release branch strategy (e.g., release/v1.0.0)\n   - Integrate with the Release Preparation Script (Task 30)\n\n4. **User Experience Documentation**:\n   - Write comprehensive getting started guide\n   - Create integration examples for VSCode, PyCharm, and command line\n   - Develop troubleshooting guide with common issues and solutions\n   - Set up community support channels (GitHub Discussions, Discord, etc.)\n   - Document the MCP Info Command functionality (Task 29)\n\n5. **Technical Implementation**:\n   - Define package structure with clear entry points\n   - Implement dependency management with compatibility matrices\n   - Create environment testing matrix (OS, Python versions)\n   - Document performance benchmarks and minimum requirements\n   - Ensure journal entry functionality is properly packaged and accessible\n   - Verify proper integration with the File-Based Logging System (Task 28)\n\nImplementation should follow Python packaging best practices and ensure the journal entry creation functionality from Task 9, daily summary git hook trigger from Task 27, logging system from Task 28, info command from Task 29, and release preparation script from Task 30 are all properly exposed and documented in the package.",
      "testStrategy": "To verify the packaging strategy and release process:\n\n1. **Package Structure Testing**:\n   - Validate package structure using `check-manifest`\n   - Verify all necessary files are included in the distribution\n   - Test package installation in a clean virtual environment\n   - Confirm entry points work as expected after installation\n\n2. **Release Process Validation**:\n   - Perform a test release to TestPyPI\n   - Verify version bumping and changelog generation\n   - Test the release automation pipeline with a pre-release version\n   - Validate rollback procedures with a simulated failed release\n   - Test the Release Preparation Script (Task 30) integration\n\n3. **Installation Testing**:\n   - Test pip installation on different operating systems (Windows, macOS, Linux)\n   - Verify development installation for contributors\n   - Test MCP server deployment using the documented methods\n   - Validate configuration options work as described\n\n4. **Documentation Review**:\n   - Conduct user testing with the getting started guide\n   - Review integration examples for accuracy and completeness\n   - Verify troubleshooting documentation addresses common issues\n   - Test community support channels are properly set up\n   - Verify MCP Info Command (Task 29) documentation is accurate\n\n5. **Functionality Testing**:\n   - Verify journal entry creation (from Task 9) works correctly after package installation\n   - Test daily summary git hook trigger (from Task 27) functions properly\n   - Validate the File-Based Logging System (Task 28) works as expected\n   - Test the MCP Info Command (Task 29) functionality\n   - Verify the Release Preparation Script (Task 30) executes correctly\n   - Test all documented features are accessible through the package\n   - Validate performance meets the documented benchmarks\n   - Ensure compatibility with all supported Python versions and environments\n\nThe packaging strategy is considered complete when a test user can successfully install and use the package following only the provided documentation.",
      "subtasks": [
        {
          "id": "26.1",
          "title": "Package Structure and Basic Configuration",
          "description": "Set up the foundational package structure with proper entry points, dependencies, and basic metadata in pyproject.toml",
          "status": "pending",
          "dependencies": [],
          "details": "**Objective**: Set up the foundational package structure with proper entry points, dependencies, and basic metadata in pyproject.toml\n\n### TDD Steps:\n1. **WRITE TESTS FIRST**\n   - Create `tests/unit/test_package_structure.py`\n   - Test `check_package_installability()` function\n   - Test cases: package can be installed via pip, entry points are accessible, required dependencies are installed, package metadata is correct, module imports work properly\n   - **RUN TESTS - VERIFY THEY FAIL**\n\n2. **GET APPROVAL FOR DESIGN CHOICES**\n   - **PAUSE FOR MANUAL APPROVAL**: Package name decision (mcp-commit-story vs alternatives)\n   - **PAUSE FOR MANUAL APPROVAL**: Entry point structure and CLI command naming\n   - **PAUSE FOR MANUAL APPROVAL**: Dependency version constraints strategy (pinned vs flexible)\n\n3. **IMPLEMENT FUNCTIONALITY**\n   - Implement proper pyproject.toml configuration with metadata, dependencies, and entry points\n   - Create package structure with `__init__.py` files and proper imports\n   - Set up entry points for CLI and MCP server functionality\n   - Handle all error cases identified in tests\n   - **RUN TESTS - VERIFY THEY PASS**\n\n4. **DOCUMENT AND COMPLETE**\n   - Add documentation IF NEEDED in three places:\n     1. **Docs directory**: Create installation.md with package structure documentation\n     2. **PRD**: Update product requirements to reflect packaging strategy and installation methods\n     3. **Engineering Spec**: Update technical implementation details for package architecture and make sure TOC is current\n   - **Do not remove existing information unless it's incorrect**\n   - **No approval needed** - make documentation edits directly\n   - **Run the entire test suite and make sure all tests are passing**\n   - **Make sure pyproject.toml is updated as needed**\n   - Double check all subtask requirements are met before marking this subtask as complete\n   - **MARK COMPLETE**"
        },
        {
          "id": "26.2",
          "title": "Version Management and PyPI Metadata",
          "description": "Implement semantic versioning system and complete PyPI package metadata for public distribution",
          "status": "pending",
          "dependencies": [],
          "details": "**Objective**: Implement semantic versioning system and complete PyPI package metadata for public distribution\n\n### TDD Steps:\n1. **WRITE TESTS FIRST**\n   - Create `tests/unit/test_version_management.py`\n   - Test `get_version()`, `validate_version_format()`, and `increment_version()` functions\n   - Test cases: version extraction from pyproject.toml, semantic version validation, version comparison, changelog integration, PyPI metadata validation\n   - **RUN TESTS - VERIFY THEY FAIL**\n\n2. **GET APPROVAL FOR DESIGN CHOICES**\n   - **PAUSE FOR MANUAL APPROVAL**: Initial version number for MVP (0.1.0 vs 1.0.0 vs other)\n   - **PAUSE FOR MANUAL APPROVAL**: PyPI package description, keywords, and classifiers\n   - **PAUSE FOR MANUAL APPROVAL**: Author information and project URLs structure\n\n3. **IMPLEMENT FUNCTIONALITY**\n   - Implement version management utilities in `src/mcp_commit_story/version.py`\n   - Complete PyPI metadata in pyproject.toml with description, classifiers, urls\n   - Create version validation and update mechanisms\n   - Handle all error cases identified in tests\n   - **RUN TESTS - VERIFY THEY PASS**\n\n4. **DOCUMENT AND COMPLETE**\n   - Add documentation IF NEEDED in three places:\n     1. **Docs directory**: Update installation.md with version information and PyPI details\n     2. **PRD**: Update product requirements to reflect versioning strategy and public availability\n     3. **Engineering Spec**: Update technical implementation details for version management and make sure TOC is current\n   - **Do not remove existing information unless it's incorrect**\n   - **No approval needed** - make documentation edits directly\n   - **Run the entire test suite and make sure all tests are passing**\n   - **Make sure pyproject.toml is updated as needed**\n   - Double check all subtask requirements are met before marking this subtask as complete\n   - **MARK COMPLETE**"
        },
        {
          "id": "26.3",
          "title": "CI/CD Pipeline and Automated Release Process",
          "description": "Create GitHub Actions workflow for automated testing, building, and PyPI publishing with proper release automation",
          "status": "pending",
          "dependencies": [],
          "details": "**Objective**: Create GitHub Actions workflow for automated testing, building, and PyPI publishing with proper release automation\n\n### TDD Steps:\n1. **WRITE TESTS FIRST**\n   - Create `tests/integration/test_release_pipeline.py`\n   - Test `validate_release_process()` and `test_package_build()` functions\n   - Test cases: package builds successfully, tests pass in clean environment, version tagging works, PyPI upload simulation, rollback procedures, multi-platform compatibility\n   - **RUN TESTS - VERIFY THEY FAIL**\n\n2. **GET APPROVAL FOR DESIGN CHOICES**\n   - **PAUSE FOR MANUAL APPROVAL**: CI/CD platform choice (GitHub Actions vs alternatives)\n   - **PAUSE FOR MANUAL APPROVAL**: Release trigger strategy (manual vs automatic on tag)\n   - **PAUSE FOR MANUAL APPROVAL**: Testing matrix (Python versions, OS combinations)\n   - **PAUSE FOR MANUAL APPROVAL**: PyPI vs TestPyPI initial release strategy\n\n3. **IMPLEMENT FUNCTIONALITY**\n   - Implement GitHub Actions workflow in `.github/workflows/release.yml`\n   - Create automated testing pipeline with multiple Python versions and OS\n   - Set up PyPI publishing with proper secrets and authentication\n   - Create release automation scripts and rollback procedures\n   - Handle all error cases identified in tests\n   - **RUN TESTS - VERIFY THEY PASS**\n\n4. **DOCUMENT AND COMPLETE**\n   - Add documentation IF NEEDED in three places:\n     1. **Docs directory**: Create release-process.md with CI/CD documentation and contributor guidelines\n     2. **PRD**: Update product requirements to reflect automated release capabilities\n     3. **Engineering Spec**: Update technical implementation details for CI/CD architecture and make sure TOC is current\n   - **Do not remove existing information unless it's incorrect**\n   - **No approval needed** - make documentation edits directly\n   - **Run the entire test suite and make sure all tests are passing**\n   - **Make sure pyproject.toml is updated as needed**\n   - Double check all subtask requirements are met before marking this subtask as complete\n   - **MARK COMPLETE**"
        },
        {
          "id": "26.4",
          "title": "Installation Methods and Development Setup",
          "description": "Implement and test multiple installation methods including pip install, development setup, and MCP server deployment options",
          "status": "pending",
          "dependencies": [],
          "details": "**Objective**: Implement and test multiple installation methods including pip install, development setup, and MCP server deployment options\n\n### TDD Steps:\n1. **WRITE TESTS FIRST**\n   - Create `tests/integration/test_installation_methods.py`\n   - Test `test_pip_installation()`, `test_dev_installation()`, and `test_mcp_deployment()` functions\n   - Test cases: standard pip install works, development install with editable mode, MCP server starts properly, configuration detection, dependency resolution, uninstall cleanup\n   - **RUN TESTS - VERIFY THEY FAIL**\n\n2. **GET APPROVAL FOR DESIGN CHOICES**\n   - **PAUSE FOR MANUAL APPROVAL**: MCP server deployment strategy (standalone, Docker, systemd service)\n   - **PAUSE FOR MANUAL APPROVAL**: Development setup complexity vs ease of use trade-offs\n   - **PAUSE FOR MANUAL APPROVAL**: Configuration file locations and discovery methods\n\n3. **IMPLEMENT FUNCTIONALITY**\n   - Implement installation validation scripts and MCP server deployment helpers\n   - Create development setup automation and configuration templates\n   - Set up proper package entry points and command line interfaces\n   - Create deployment documentation and configuration examples\n   - Handle all error cases identified in tests\n   - **RUN TESTS - VERIFY THEY PASS**\n\n4. **DOCUMENT AND COMPLETE**\n   - Add documentation IF NEEDED in three places:\n     1. **Docs directory**: Create deployment.md and development.md with comprehensive setup guides\n     2. **PRD**: Update product requirements to reflect deployment options and developer experience\n     3. **Engineering Spec**: Update technical implementation details for installation architecture and make sure TOC is current\n   - **Do not remove existing information unless it's incorrect**\n   - **No approval needed** - make documentation edits directly\n   - **Run the entire test suite and make sure all tests are passing**\n   - **Make sure pyproject.toml is updated as needed**\n   - Double check all subtask requirements are met before marking this subtask as complete\n   - **MARK COMPLETE**"
        },
        {
          "id": "26.5",
          "title": "User Documentation and Getting Started Guide",
          "description": "Create comprehensive user-facing documentation including getting started guide, integration examples, and troubleshooting resources",
          "status": "pending",
          "dependencies": [],
          "details": "**Objective**: Create comprehensive user-facing documentation including getting started guide, integration examples, and troubleshooting resources\n\n### TDD Steps:\n1. **WRITE TESTS FIRST**\n   - Create `tests/integration/test_user_documentation.py`\n   - Test `validate_documentation_examples()` and `test_integration_guides()` functions\n   - Test cases: getting started examples work as written, VSCode integration examples function properly, command line examples produce expected output, troubleshooting steps resolve common issues, all code snippets are valid\n   - **RUN TESTS - VERIFY THEY FAIL**\n\n2. **GET APPROVAL FOR DESIGN CHOICES**\n   - **PAUSE FOR MANUAL APPROVAL**: Documentation structure and organization approach\n   - **PAUSE FOR MANUAL APPROVAL**: Target audience definition (developers vs end users)\n   - **PAUSE FOR MANUAL APPROVAL**: Integration example priorities (which editors/tools to focus on)\n   - **PAUSE FOR MANUAL APPROVAL**: Community support channel setup (GitHub Discussions, Discord, etc.)\n\n3. **IMPLEMENT FUNCTIONALITY**\n   - Implement comprehensive getting started guide with step-by-step instructions\n   - Create integration examples for VSCode, PyCharm, and command line usage\n   - Develop troubleshooting guide with common issues and solutions\n   - Set up community support infrastructure and documentation\n   - Handle all error cases identified in tests\n   - **RUN TESTS - VERIFY THEY PASS**\n\n4. **DOCUMENT AND COMPLETE**\n   - Add documentation IF NEEDED in three places:\n     1. **Docs directory**: Create getting-started.md, integrations.md, and troubleshooting.md\n     2. **PRD**: Update product requirements to reflect user experience and support capabilities\n     3. **Engineering Spec**: Update technical implementation details for documentation architecture and make sure TOC is current\n   - **Do not remove existing information unless it's incorrect**\n   - **No approval needed** - make documentation edits directly\n   - **Run the entire test suite and make sure all tests are passing**\n   - **Make sure pyproject.toml is updated as needed**\n   - Double check all subtask requirements are met before marking this subtask as complete\n   - **MARK COMPLETE**"
        },
        {
          "id": "26.6",
          "title": "Final MVP Release and Launch Validation",
          "description": "Execute the complete MVP release process with final testing, PyPI publishing, and post-launch validation",
          "status": "pending",
          "dependencies": [],
          "details": "**Objective**: Execute the complete MVP release process with final testing, PyPI publishing, and post-launch validation\n\n### TDD Steps:\n1. **WRITE TESTS FIRST**\n   - Create `tests/integration/test_mvp_release.py`\n   - Test `validate_mvp_readiness()`, `test_production_release()`, and `verify_post_launch()` functions\n   - Test cases: all features work in production environment, PyPI package installs correctly, documentation is accessible, performance meets requirements, error handling works properly, rollback procedures function if needed\n   - **RUN TESTS - VERIFY THEY FAIL**\n\n2. **GET APPROVAL FOR DESIGN CHOICES**\n   - **PAUSE FOR MANUAL APPROVAL**: Final release version number and release notes content\n   - **PAUSE FOR MANUAL APPROVAL**: Launch timing and announcement strategy\n   - **PAUSE FOR MANUAL APPROVAL**: Post-launch monitoring and support plan\n   - **PAUSE FOR MANUAL APPROVAL**: Success criteria definition for MVP launch\n\n3. **IMPLEMENT FUNCTIONALITY**\n   - Implement final release checklist validation and automation\n   - Execute production PyPI release with proper version tagging\n   - Verify all installation methods work in clean environments\n   - Set up post-launch monitoring and feedback collection\n   - Handle all error cases identified in tests\n   - **RUN TESTS - VERIFY THEY PASS**\n\n4. **DOCUMENT AND COMPLETE**\n   - Add documentation IF NEEDED in three places:\n     1. **Docs directory**: Create release-notes.md and update all guides with final version information\n     2. **PRD**: Update product requirements to reflect MVP completion and next phase planning\n     3. **Engineering Spec**: Update technical implementation details for production architecture and make sure TOC is current\n   - **Do not remove existing information unless it's incorrect**\n   - **No approval needed** - make documentation edits directly\n   - **Run the entire test suite and make sure all tests are passing**\n   - **Make sure pyproject.toml is updated as needed**\n   - Double check all subtask requirements are met before marking this subtask as complete\n   - **MARK COMPLETE**"
        }
      ]
    },
    {
      "id": 29,
      "title": "Implement MCP Info Command for Diagnostics",
      "description": "Add a new 'info' tool to the MCP server that provides diagnostic information to help users troubleshoot issues, including version, telemetry status, configuration details, and dependency availability.",
      "details": "Implement the MCP info command in `src/mcp_commit_story/server.py` with the following features:\n\n1. Create a new tool handler using the `@server.tool()` decorator:\n```python\n@server.tool()\nasync def info(request):\n    \"\"\"Return diagnostic information about the MCP server.\"\"\"\n    try:\n        # Get version from pyproject.toml\n        version = get_version_from_pyproject()\n        \n        # Get telemetry status\n        telemetry_status = get_telemetry_status()\n        \n        # Get active configuration path\n        config_path = get_active_config_path()\n        \n        # Get log file location from the logging system\n        log_file = get_log_file_location()\n        \n        # Check dependency availability\n        dependencies = {\n            \"git\": check_git_availability(),\n            \"opentelemetry\": check_opentelemetry_availability()\n        }\n        \n        # Validate configuration\n        config_validation = validate_configuration()\n        \n        return {\n            \"version\": version,\n            \"telemetry_status\": telemetry_status,\n            \"config_path\": str(config_path),\n            \"log_file\": str(log_file),\n            \"dependencies\": dependencies,\n            \"config_validation\": config_validation\n        }\n    except Exception as e:\n        logger.error(f\"Error in info command: {str(e)}\")\n        return {\"error\": str(e)}\n```\n\n2. Implement helper functions for retrieving diagnostic information:\n\n```python\ndef get_version_from_pyproject():\n    \"\"\"Extract version from pyproject.toml.\"\"\"\n    try:\n        import tomli\n        from pathlib import Path\n        \n        # Find the pyproject.toml file (traverse up from current file if needed)\n        current_dir = Path(__file__).parent\n        pyproject_path = None\n        \n        # Look up to 3 levels up for pyproject.toml\n        for i in range(4):\n            check_path = current_dir / (\"../\" * i) / \"pyproject.toml\"\n            if check_path.resolve().exists():\n                pyproject_path = check_path.resolve()\n                break\n        \n        if not pyproject_path:\n            return \"unknown\"\n        \n        with open(pyproject_path, \"rb\") as f:\n            pyproject_data = tomli.load(f)\n            \n        return pyproject_data.get(\"project\", {}).get(\"version\", \"unknown\")\n    except Exception as e:\n        logger.error(f\"Error getting version: {str(e)}\")\n        return \"unknown\"\n\ndef get_telemetry_status():\n    \"\"\"Get the current telemetry status.\"\"\"\n    # Check if telemetry is enabled in configuration\n    config = get_config()\n    return {\n        \"enabled\": config.get(\"telemetry\", {}).get(\"enabled\", False),\n        \"endpoint\": config.get(\"telemetry\", {}).get(\"endpoint\", \"\")\n    }\n\ndef get_active_config_path():\n    \"\"\"Get the path to the active configuration file.\"\"\"\n    # Return the path to the currently loaded config file\n    return get_config_path()\n\ndef get_log_file_location():\n    \"\"\"Get the path to the current log file.\"\"\"\n    # This should use the logging system implemented in Task 28\n    from mcp_commit_story.logging import get_log_file_path\n    return get_log_file_path()\n\ndef check_git_availability():\n    \"\"\"Check if git is available and return version info.\"\"\"\n    try:\n        import subprocess\n        result = subprocess.run([\"git\", \"--version\"], capture_output=True, text=True, check=True)\n        return {\n            \"available\": True,\n            \"version\": result.stdout.strip()\n        }\n    except Exception:\n        return {\n            \"available\": False,\n            \"version\": None\n        }\n\ndef check_opentelemetry_availability():\n    \"\"\"Check if OpenTelemetry is available.\"\"\"\n    try:\n        import opentelemetry\n        return {\n            \"available\": True,\n            \"version\": getattr(opentelemetry, \"__version__\", \"unknown\")\n        }\n    except ImportError:\n        return {\n            \"available\": False,\n            \"version\": None\n        }\n\ndef validate_configuration():\n    \"\"\"Validate the current configuration.\"\"\"\n    # Perform validation checks on the current configuration\n    config = get_config()\n    validation_results = {}\n    \n    # Check for required configuration sections\n    required_sections = [\"journal\", \"git\", \"server\"]\n    for section in required_sections:\n        validation_results[f\"{section}_section\"] = section in config\n    \n    # Check for required paths\n    if \"journal\" in config:\n        journal_path = Path(config[\"journal\"].get(\"path\", \"\"))\n        validation_results[\"journal_path_exists\"] = journal_path.exists()\n    \n    return validation_results\n```\n\n3. Update the MCP server documentation to include the new info command:\n```python\n# In the server documentation string\n\"\"\"\nMCP Server Tools:\n...\n- info: Returns diagnostic information about the MCP server\n\"\"\"\n```\n\n4. Ensure the info command is properly registered with the MCP server and accessible through the standard MCP protocol.",
      "testStrategy": "1. Unit tests for the info command:\n```python\ndef test_info_command():\n    \"\"\"Test that the info command returns all required fields.\"\"\"\n    # Setup mock server\n    server = MockMCPServer()\n    \n    # Call info command\n    response = server.call_tool(\"info\", {})\n    \n    # Verify all required fields are present\n    assert \"version\" in response\n    assert \"telemetry_status\" in response\n    assert \"config_path\" in response\n    assert \"log_file\" in response\n    assert \"dependencies\" in response\n    assert \"config_validation\" in response\n    \n    # Verify dependencies contains required checks\n    assert \"git\" in response[\"dependencies\"]\n    assert \"opentelemetry\" in response[\"dependencies\"]\n\ndef test_version_from_pyproject():\n    \"\"\"Test that version is dynamically read from pyproject.toml.\"\"\"\n    # Create a temporary pyproject.toml with a known version\n    with tempfile.TemporaryDirectory() as tmpdir:\n        temp_path = Path(tmpdir) / \"pyproject.toml\"\n        with open(temp_path, \"w\") as f:\n            f.write('[project]\\nversion = \"1.2.3\"\\n')\n        \n        # Mock the file resolution to return our temporary file\n        with patch(\"pathlib.Path.resolve\", return_value=temp_path):\n            with patch(\"pathlib.Path.exists\", return_value=True):\n                version = get_version_from_pyproject()\n                assert version == \"1.2.3\"\n\ndef test_info_with_various_configs():\n    \"\"\"Test info command with various configuration states.\"\"\"\n    # Test with missing configuration\n    with patch(\"mcp_commit_story.server.get_config\", return_value={}):\n        response = server.call_tool(\"info\", {})\n        assert response[\"config_validation\"][\"journal_section\"] is False\n    \n    # Test with valid configuration\n    valid_config = {\n        \"journal\": {\"path\": \"/tmp/journal\"},\n        \"git\": {\"repo_path\": \"/tmp/repo\"},\n        \"server\": {\"port\": 8000}\n    }\n    with patch(\"mcp_commit_story.server.get_config\", return_value=valid_config):\n        with patch(\"pathlib.Path.exists\", return_value=True):\n            response = server.call_tool(\"info\", {})\n            assert response[\"config_validation\"][\"journal_section\"] is True\n            assert response[\"config_validation\"][\"journal_path_exists\"] is True\n\ndef test_info_through_mcp_protocol():\n    \"\"\"Test that info command works through the MCP protocol.\"\"\"\n    # Start a real MCP server\n    server_process = start_test_server()\n    try:\n        # Connect to the server using the MCP client\n        client = MCPClient(\"localhost\", 8000)\n        \n        # Call the info command\n        response = client.call(\"info\", {})\n        \n        # Verify response\n        assert \"version\" in response\n        assert \"telemetry_status\" in response\n        assert \"config_path\" in response\n        assert \"log_file\" in response\n    finally:\n        # Clean up\n        server_process.terminate()\n```\n\n2. Integration tests:\n   - Test the info command through the MCP protocol from a real client\n   - Verify that all diagnostic information is correctly reported\n   - Test with different configuration states (missing config, invalid paths, etc.)\n   - Verify that the log file location matches the actual log file being used\n\n3. Manual testing:\n   - Call the info command from the CLI client\n   - Verify that all information is displayed correctly\n   - Intentionally break dependencies (e.g., rename git executable) and verify the command correctly reports their unavailability\n   - Test with telemetry enabled and disabled to ensure correct reporting",
      "status": "pending",
      "dependencies": [],
      "priority": "high",
      "subtasks": []
    },
    {
      "id": 30,
      "title": "Create Release Preparation Script",
      "description": "Develop an automated release validation script that performs a series of quality checks before publishing to ensure the package meets all requirements.",
      "details": "Implement a comprehensive release preparation script (`scripts/prepare_release.py`) that performs sequential validation checks before publishing:\n\n1. **Git Status Checks**:\n   ```python\n   def check_git_status():\n       \"\"\"Verify git repository is in a clean state for release\"\"\"\n       # Check current branch is main/master\n       branch = subprocess.check_output([\"git\", \"branch\", \"--show-current\"]).decode().strip()\n       if branch not in [\"main\", \"master\"]:\n           return False, f\"Not on main/master branch (current: {branch})\"\n       \n       # Check for uncommitted changes\n       status = subprocess.check_output([\"git\", \"status\", \"--porcelain\"]).decode().strip()\n       if status:\n           return False, \"Uncommitted changes detected\"\n       \n       # Check if local is in sync with remote\n       subprocess.check_call([\"git\", \"fetch\", \"origin\"])\n       local = subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"]).decode().strip()\n       remote = subprocess.check_output([\"git\", \"rev-parse\", f\"origin/{branch}\"]).decode().strip()\n       if local != remote:\n           return False, f\"Local {branch} is not in sync with origin/{branch}\"\n       \n       return True, \"Git status checks passed\"\n   ```\n\n2. **Version Checks**:\n   ```python\n   def check_version():\n       \"\"\"Verify version is consistent and not already published\"\"\"\n       # Get version from pyproject.toml\n       with open(\"pyproject.toml\", \"r\") as f:\n           pyproject = toml.load(f)\n       version = pyproject[\"project\"][\"version\"]\n       \n       # Check version format (semantic versioning)\n       if not re.match(r\"^\\d+\\.\\d+\\.\\d+$\", version):\n           return False, f\"Version {version} does not follow semantic versioning\"\n       \n       # Check if version already exists on PyPI\n       try:\n           response = requests.get(f\"https://pypi.org/pypi/mcp-commit-story/{version}/json\")\n           if response.status_code == 200:\n               return False, f\"Version {version} already exists on PyPI\"\n       except Exception as e:\n           pass  # Connection error is not a failure\n       \n       # Check version consistency across files\n       # (Add checks for other files that might contain version info)\n       \n       return True, f\"Version checks passed: {version}\"\n   ```\n\n3. **Code Quality Checks**:\n   ```python\n   def check_code_quality():\n       \"\"\"Run tests, linting, and security checks\"\"\"\n       # Run tests\n       try:\n           subprocess.check_call([\"pytest\", \"-xvs\"])\n       except subprocess.CalledProcessError:\n           return False, \"Tests failed\"\n       \n       # Run linting\n       try:\n           subprocess.check_call([\"flake8\"])\n       except subprocess.CalledProcessError:\n           return False, \"Linting failed\"\n       \n       # Run security audit\n       try:\n           subprocess.check_call([\"bandit\", \"-r\", \"src\"])\n       except subprocess.CalledProcessError:\n           return False, \"Security audit failed\"\n       \n       return True, \"Code quality checks passed\"\n   ```\n\n4. **Package Validation**:\n   ```python\n   def validate_package():\n       \"\"\"Build and validate the package\"\"\"\n       # Clean previous builds\n       if os.path.exists(\"dist\"):\n           shutil.rmtree(\"dist\")\n       \n       # Build package\n       try:\n           subprocess.check_call([\"python\", \"-m\", \"build\"])\n       except subprocess.CalledProcessError:\n           return False, \"Package build failed\"\n       \n       # Check package size\n       wheel_file = glob.glob(\"dist/*.whl\")[0]\n       size_mb = os.path.getsize(wheel_file) / (1024 * 1024)\n       if size_mb > 10:  # Example threshold\n           return False, f\"Package too large: {size_mb:.2f}MB (max 10MB)\"\n       \n       # Validate package structure\n       try:\n           subprocess.check_call([\"twine\", \"check\", \"dist/*\"])\n       except subprocess.CalledProcessError:\n           return False, \"Package validation failed\"\n       \n       return True, \"Package validation passed\"\n   ```\n\n5. **Main Script Structure**:\n   ```python\n   def main():\n       \"\"\"Run all release preparation checks\"\"\"\n       checks = [\n           (\"Git Status\", check_git_status),\n           (\"Version\", check_version),\n           (\"Code Quality\", check_code_quality),\n           (\"Package Validation\", validate_package)\n       ]\n       \n       for name, check_func in checks:\n           print(f\"Running {name} checks...\")\n           success, message = check_func()\n           if not success:\n               print(f\"❌ {name} check failed: {message}\")\n               sys.exit(1)\n           print(f\"✅ {message}\")\n       \n       print(\"✅ All checks passed! Ready for release.\")\n   \n   if __name__ == \"__main__\":\n       main()\n   ```\n\n6. **Add PyProject.toml Script Entry**:\n   Update `pyproject.toml` to include:\n   ```toml\n   [project.scripts]\n   prepare-release = \"scripts.prepare_release:main\"\n   ```\n\nThe script should be designed to fail fast, stopping at the first check that fails with a clear error message. Each check should be modular and return both a success status and a message explaining the result.",
      "testStrategy": "To verify the release preparation script works correctly:\n\n1. **Test Failure Scenarios**:\n   - Create a git repository with uncommitted changes and verify the script fails with the appropriate error message\n   - Create a version that already exists on PyPI and verify the script detects this\n   - Introduce a failing test and verify the script catches it\n   - Create an invalid package structure and verify the script detects it\n\n2. **Test Error Handling**:\n   - Verify the script provides clear, actionable error messages\n   - Confirm the script exits with non-zero status code on failure\n   - Ensure the script stops at the first failure without continuing\n\n3. **Test Success Path**:\n   - Set up a clean environment that meets all requirements\n   - Run the script and verify it completes successfully\n   - Confirm all checks are executed in the correct order\n\n4. **Integration Testing**:\n   - Test the script in a CI environment to ensure it works in automated contexts\n   - Verify the script can be run via the PyProject.toml entry point\n\n5. **Specific Test Cases**:\n   ```bash\n   # Test git status check failure\n   echo \"test\" > temp.txt\n   ./scripts/prepare_release.py  # Should fail with uncommitted changes message\n   git add temp.txt\n   git commit -m \"temp commit\"\n   ./scripts/prepare_release.py  # Should fail with branch sync message\n   \n   # Test version check\n   # (modify version to match existing PyPI version)\n   ./scripts/prepare_release.py  # Should fail with version exists message\n   \n   # Test successful run\n   git checkout main\n   git pull\n   # (ensure clean state and valid version)\n   ./scripts/prepare_release.py  # Should succeed\n   ```\n\nDocument all test scenarios and expected outcomes to ensure comprehensive coverage of the script's functionality.",
      "status": "pending",
      "dependencies": [
        15,
        26
      ],
      "priority": "high",
      "subtasks": []
    },
    {
      "id": 31,
      "title": "Refactor Large Modules for Improved Maintainability",
      "description": "Split large files into smaller, more focused modules to improve maintainability while preserving backward compatibility, following MCP best practices of keeping files under 500 lines of code.",
      "details": "This task involves refactoring large modules in the codebase to improve maintainability while ensuring backward compatibility:\n\n1. **Telemetry Module Refactoring**:\n   - Split the current telemetry.py (1800+ lines) into:\n     - `telemetry/core.py`: Core functionality and base classes\n     - `telemetry/decorators.py`: All telemetry-related decorators\n     - `telemetry/metrics.py`: Metric collection and processing\n     - `telemetry/config.py`: Configuration handling for telemetry\n   - Create appropriate `__init__.py` to re-export all public APIs\n\n2. **Journal Module Refactoring**:\n   - Split journal.py into:\n     - `journal/core.py`: Core journal functionality\n     - `journal/generators.py`: Entry generation logic\n     - `journal/serializers.py`: Serialization/deserialization logic\n   - Create appropriate `__init__.py` to re-export all public APIs\n\n3. **Backward Compatibility**:\n   - Ensure all public APIs are maintained\n   - Use `__init__.py` files to re-export all public functions and classes\n   - Example pattern:\n     ```python\n     # In telemetry/__init__.py\n     from .core import TelemetryManager, initialize_telemetry\n     from .decorators import track_usage, measure_performance\n     from .metrics import MetricCollector, send_metrics\n     from .config import TelemetryConfig, load_config\n\n     # Re-export everything to maintain the original API\n     __all__ = [\n         'TelemetryManager', 'initialize_telemetry',\n         'track_usage', 'measure_performance',\n         'MetricCollector', 'send_metrics',\n         'TelemetryConfig', 'load_config'\n     ]\n     ```\n\n4. **Update Import References**:\n   - Scan the entire codebase for imports from the original modules\n   - Update all import statements to reference the new module structure\n   - Use tools like `grep` or IDE search functionality to find all references\n\n5. **Code Organization Guidelines**:\n   - Follow single responsibility principle for each module\n   - Keep related functionality together\n   - Aim for <500 lines of code per file\n   - Add appropriate docstrings to clarify module purpose\n\n6. **Documentation Updates**:\n   - Update any documentation that references the original module structure\n   - Add module-level docstrings explaining the purpose of each new file",
      "testStrategy": "The refactoring will be verified through the following testing approach:\n\n1. **Baseline Test Execution**:\n   - Run the full test suite before starting refactoring to establish a baseline\n   - Document any existing test failures for reference\n\n2. **Incremental Testing**:\n   - Test each module after refactoring, before moving to the next\n   - Run the specific tests related to each module after refactoring\n\n3. **Full Test Suite Verification**:\n   - Run the complete test suite after all refactoring is complete\n   - Ensure all tests pass with the same results as the baseline\n\n4. **Import Compatibility Testing**:\n   - Create specific tests to verify that all public APIs are still accessible\n   - Test both direct imports and from-imports:\n     ```python\n     # Test direct imports still work\n     import telemetry\n     telemetry.initialize_telemetry()\n     \n     # Test specific imports work\n     from telemetry import initialize_telemetry\n     initialize_telemetry()\n     ```\n\n5. **Integration Testing**:\n   - Verify that components using these modules continue to function correctly\n   - Test the full application workflow to ensure no regressions\n\n6. **Manual Verification**:\n   - Manually verify that all modules are under 500 lines of code\n   - Review import statements across the codebase to ensure they've been updated\n\n7. **Documentation Testing**:\n   - Verify that documentation builds correctly with the new module structure\n   - Test any code examples in documentation to ensure they still work",
      "status": "pending",
      "dependencies": [
        26
      ],
      "priority": "medium",
      "subtasks": []
    },
    {
      "id": 32,
      "title": "Implement Parameter Parsing Leniency for MCP Handlers",
      "description": "Create a flexible parameter parsing system for MCP handlers that accepts common variations in parameter names while maintaining schema integrity.",
      "details": "This task involves implementing a parameter normalization layer to make MCP parameter parsing more flexible:\n\n1. **Parameter Normalization Layer**:\n   - Create a middleware or wrapper function that normalizes incoming parameters before they reach handler functions\n   - Implement in the core MCP request processing pipeline\n   - Design a consistent approach that works across all handlers\n\n2. **Parameter Aliasing Configuration**:\n   - Create a configuration system for parameter aliases with mappings like:\n     ```python\n     PARAMETER_ALIASES = {\n       \"path\": [\"project_path\", \"filepath\", \"file_path\"],\n       \"text\": [\"reflection\", \"content\", \"message\"],\n       \"commit_id\": [\"commit\", \"sha\", \"hash\"],\n       # Add other common variations\n     }\n     ```\n   - Ensure the configuration is extensible and documented\n\n3. **Normalization Logic**:\n   - Implement a function that transforms incoming parameters based on the alias configuration:\n     ```python\n     def normalize_parameters(params, handler_schema):\n         \"\"\"\n         Transform parameters based on aliases to match expected schema\n         while preserving original values when appropriate\n         \"\"\"\n         normalized = params.copy()\n         for expected_param, aliases in PARAMETER_ALIASES.items():\n             if expected_param not in normalized:\n                 for alias in aliases:\n                     if alias in normalized:\n                         normalized[expected_param] = normalized[alias]\n                         break\n         return normalized\n     ```\n\n4. **Schema Integrity**:\n   - Maintain strict schema advertising in API documentation\n   - Add warnings in logs when non-standard parameter names are used\n   - Consider adding deprecation notices for certain aliases to encourage standard usage\n\n5. **Integration**:\n   - Apply normalization before parameter validation\n   - Update all handler functions to use the normalized parameters\n   - Ensure backward compatibility with existing clients\n\n6. **Documentation**:\n   - Document the parameter aliasing system for developers\n   - Update API documentation to note accepted variations where appropriate",
      "testStrategy": "1. **Unit Tests for Normalization**:\n   - Create tests for the parameter normalization function with various input combinations\n   - Verify each alias correctly maps to its canonical parameter name\n   - Test edge cases like conflicting parameters or missing values\n\n2. **Handler Integration Tests**:\n   - For each MCP handler, create test cases that use alternative parameter names\n   - Verify the handler functions correctly with both standard and aliased parameters\n   - Example test cases:\n     ```python\n     def test_commit_handler_with_parameter_aliases():\n         # Test with standard parameters\n         result1 = commit_handler(path=\"/path/to/repo\", message=\"Test commit\")\n         \n         # Test with aliased parameters\n         result2 = commit_handler(project_path=\"/path/to/repo\", reflection=\"Test commit\")\n         \n         # Results should be equivalent\n         assert result1 == result2\n     ```\n\n3. **Backward Compatibility Tests**:\n   - Verify that existing code using standard parameter names continues to work\n   - Run the full test suite to ensure no regressions\n\n4. **MCP Inspector Tests**:\n   - Use the MCP inspector tool to verify parameter handling\n   - Test interactive parameter submission with various aliases\n   - Verify the inspector correctly shows normalized parameters\n\n5. **Error Handling Tests**:\n   - Test scenarios with invalid parameters to ensure proper error messages\n   - Verify that aliasing doesn't interfere with validation logic\n\n6. **Performance Tests**:\n   - Measure any performance impact from the additional normalization layer\n   - Ensure the overhead is minimal for standard parameter usage",
      "status": "pending",
      "dependencies": [
        26
      ],
      "priority": "low",
      "subtasks": []
    },
    {
      "id": 33,
      "title": "Remove All Console Output",
      "description": "Audit and remove all remaining stdout/print statements, replacing them with proper logging and return values to ensure clean operation for MCP clients.",
      "details": "This task involves a systematic audit and cleanup of all console output in the codebase:\n\n1. **Audit Phase**:\n   - Perform a comprehensive search for all `print()` statements, `sys.stdout.write()` calls, and any other direct console output\n   - Create an inventory of all console output locations with their purpose (debug, info, error, etc.)\n   - Categorize outputs as:\n     - Debug/development outputs (to be replaced with logging)\n     - CLI user feedback (to be preserved for human users)\n     - JSON/data outputs (to be converted to return values)\n\n2. **CLI Output Refactoring**:\n   - Modify `cli.py` to properly return values instead of printing JSON:\n   ```python\n   # Before:\n   def get_entries(date_range):\n       entries = journal.get_entries(date_range)\n       print(json.dumps(entries))\n   \n   # After:\n   def get_entries(date_range):\n       entries = journal.get_entries(date_range)\n       return entries  # Click will handle JSON serialization\n   ```\n   - Preserve human-readable help text and error messages in CLI interface\n   - Implement proper exit codes for CLI operations\n\n3. **Logging Implementation**:\n   - Replace all debug/info print statements with appropriate logging calls:\n   ```python\n   # Before:\n   print(f\"Processing commit {commit_id}\")\n   \n   # After:\n   logger.debug(f\"Processing commit {commit_id}\")\n   ```\n   - Ensure all logging uses the file-based logger implemented in Task 28\n   - Add appropriate log levels (DEBUG, INFO, WARNING, ERROR) based on message importance\n\n4. **Return Value Standardization**:\n   - Ensure all functions return proper values instead of printing results\n   - Implement consistent return structures (dictionaries, objects, etc.)\n   - For functions that previously printed status updates, consider adding a callback parameter for progress reporting\n\n5. **MCP Server Cleanup**:\n   - Special focus on MCP server handlers to ensure they never write to stdout\n   - Verify all handlers return proper JSON responses rather than printing them\n   - Implement proper error handling that logs errors but returns appropriate error responses\n\n6. **Exception Handling**:\n   - Review all exception handling to ensure exceptions are logged but not printed\n   - Implement structured error responses for API functions\n\n7. **Documentation Update**:\n   - Update documentation to reflect the new logging approach\n   - Document the return value structures for all public functions",
      "testStrategy": "1. **Automated Output Capture Test**:\n   - Create a test that captures stdout during execution of all major functions\n   - Verify no unexpected output is produced\n   ```python\n   import io\n   import sys\n   from contextlib import redirect_stdout\n   \n   def test_no_stdout_output():\n       f = io.StringIO()\n       with redirect_stdout(f):\n           # Run various operations\n           client.create_entry(commit_id=\"abc123\")\n           client.generate_summary(period=\"day\")\n       \n       output = f.getvalue()\n       assert output == \"\", f\"Unexpected stdout output: {output}\"\n   ```\n\n2. **CLI Command Testing**:\n   - Test all CLI commands with various flags and options\n   - Verify help text is still displayed correctly\n   - Verify error messages are properly shown to users\n   - For commands that should return data, verify the data is correctly returned\n\n3. **Log File Verification**:\n   - Run operations that previously generated console output\n   - Verify appropriate log entries are created in the log file\n   - Check log levels are appropriate for the message content\n\n4. **MCP Client Integration Test**:\n   - Create a test MCP client that consumes the server's responses\n   - Verify the client receives proper return values and not stdout text\n   - Test error conditions to ensure they're properly communicated via return values\n\n5. **Edge Case Testing**:\n   - Test with verbose/debug flags enabled to ensure they affect logging but not stdout\n   - Test with various error conditions to verify errors are logged but not printed\n   - Test concurrent operations to ensure no race conditions in logging\n\n6. **Manual Review**:\n   - Perform a final manual code review to catch any remaining print statements\n   - Run the application with stdout redirected to a file to verify no unexpected output",
      "status": "pending",
      "dependencies": [
        26
      ],
      "priority": "medium",
      "subtasks": []
    },
    {
      "id": 36,
      "title": "Implement Cursor Chat Database Integration for Journal Context Collection",
      "description": "Enhance the journal context collection by implementing direct SQLite integration with the Cursor chat database to access complete conversation history instead of limited recent messages.",
      "status": "pending",
      "dependencies": [],
      "priority": "high",
      "details": "This task involves significant improvements to the context collection system by directly accessing the Cursor chat database, implemented in four distinct phases:\n\n## Phase 1: Core SQLite Integration (MVP)\n\n1. Remove the current limited `collect_ai_chat_context` function from `context_collection.py` that only captures recent messages.\n\n2. Add a new SQLite reader function to `context_collection.py` that can:\n   - Use Python's built-in sqlite3 module (no external dependencies)\n   - Locate and connect to the Cursor chat database at `~/Library/Application Support/Cursor/User/workspaceStorage/[MD5_HASH]/state.vscdb`\n   - Implement proper error handling for missing or inaccessible databases\n\n3. Implement a direct database query function that:\n   - Queries the `ItemTable` where `key='aiService.prompts'` to extract the complete conversation history\n   - Parses the returned JSON format into a structured conversation history\n   - Includes appropriate error handling and logging\n\n## Phase 2: Context Integration\n\n4. Update the downstream components in the `generate_journal_entry` functions to:\n   - Process FOUR distinct context sources: git, terminal, cursor chat database, and synthesized summary\n   - Handle cases where some context sources might be unavailable\n   - Maintain backward compatibility with existing journal generation\n\n5. Implement chat boundary detection logic:\n   - Strategy to be determined during implementation\n   - Consider options: all history, since last commit, configurable limit\n\n## Phase 3: Enhancement Features\n\n6. Create a new synthesized summary collection function that:\n   - Uses AI prompts to generate high-level summaries of conversations\n   - Provides configuration options for summary detail level and focus areas\n   - Handles rate limiting and API errors gracefully\n\n7. Implement caching mechanisms for performance:\n   - Avoid regenerating summaries unnecessarily\n   - Optimize database query performance\n\n## Phase 4: Production Readiness\n\n8. Implement cross-platform support considerations:\n   - Handle path variations across operating systems\n   - Test on Windows, macOS, and Linux\n\n9. Address packaging considerations:\n   - Zero external dependencies (using built-in sqlite3)\n   - Document installation requirements for end users\n   - Provide fallback mechanisms when database access is not available\n\n## Additional Requirements\n\n10. Add comprehensive telemetry as defined in docs/telemetry.md\n\n11. Implement user-friendly diagnostics:\n    - Check if Cursor workspace is accessible\n    - Validate chat data availability\n    - Provide helpful error messages\n    - Include troubleshooting guidance\n\n12. Update documentation to reflect the new capabilities and requirements.",
      "testStrategy": "1. Unit Testing:\n   - Create unit tests for the new SQLite reader function with mock database responses\n   - Test the database query function with various sample data structures\n   - Verify the synthesized summary function produces expected outputs\n   - Test error handling for all new functions with various failure scenarios\n   - Verify telemetry implementation with mock events\n\n2. Integration Testing:\n   - Test the complete context collection pipeline with a real Cursor chat database\n   - Verify that all four context sources are properly integrated in journal generation\n   - Test across different operating systems to ensure path handling works correctly\n   - Verify backward compatibility with existing journal entries\n   - Test diagnostic features with various error conditions\n\n3. Performance Testing:\n   - Measure and optimize query performance for large chat histories\n   - Test memory usage when processing extensive conversation data\n   - Benchmark summary generation time and resource usage\n   - Verify caching mechanisms improve performance as expected\n\n4. User Acceptance Testing:\n   - Verify the quality and relevance of journal entries with the enhanced context\n   - Compare journal entries generated with and without the new context sources\n   - Gather feedback on summary quality and completeness\n   - Test user experience with diagnostic messages\n\n5. Phased Implementation Verification:\n   - Verify each phase can be deployed independently\n   - Test MVP functionality in isolation\n   - Ensure enhancements build properly on core functionality\n\n6. Documentation Review:\n   - Ensure all new features are properly documented\n   - Verify troubleshooting guidance is clear and helpful\n   - Confirm telemetry events are documented according to standards",
      "subtasks": []
    }
  ]
}