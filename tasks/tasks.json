{
  "tasks": [
    {
      "id": 7,
      "title": "Implement CLI Interface",
      "description": "Create the command-line interface using Click to provide setup functionality for the journal. This is a necessary foundation component for the MVP and other tasks.",
      "status": "pending",
      "dependencies": [],
      "priority": "medium",
      "details": "Implement the CLI interface in `src/mcp_journal/cli.py` with the following features:\n\n1. CLI setup:\n```python\nimport click\n\n@click.group()\ndef cli():\n    \"\"\"MCP Journal - Engineering journal for Git repositories\"\"\"\n    pass\n```\n\n2. Setup command implementations:\n```python\n@cli.command()\n@click.option(\"--debug\", is_flag=True, help=\"Show debug information\")\ndef journal_init(debug):\n    \"\"\"Initialize journal in current repository\"\"\"\n    # Implementation\n\n@cli.command()\n@click.option(\"--debug\", is_flag=True, help=\"Show debug information\")\ndef install_hook(debug):\n    \"\"\"Install Git hook to connect with MCP server\"\"\"\n    # Implementation\n```\n\n3. Global options:\n```python\n@click.option(\"--config\", help=\"Override config file location\")\n@click.option(\"--dry-run\", is_flag=True, help=\"Preview operations without writing files\")\n@click.option(\"--verbose\", is_flag=True, help=\"Detailed output for debugging\")\n```\n\n4. Main entry point:\n```python\ndef main():\n    \"\"\"Main entry point for CLI\"\"\"\n    try:\n        cli()\n    except Exception as e:\n        click.echo(f\"Error: {e}\", err=True)\n        sys.exit(1)\n\nif __name__ == \"__main__\":\n    main()\n```\n\nNote: This CLI is focused primarily on setup commands (journal-init, install-hook), but is a necessary foundation for the MVP as it's a blocking dependency for tasks 11, 12, 13, and 15, and has subtasks from MVP Task 9 that require CLI functionality. Most operational tasks (journal entry creation, reflection addition, summarization, etc.) are handled by the MCP server and AI agents, not through this CLI.",
      "testStrategy": "1. Unit tests for setup CLI commands (journal-init, install-hook)\n2. Tests for command options and arguments\n3. Tests for error handling\n4. Tests for global options\n5. Integration tests for setup commands\n6. Tests for exit codes and error messages",
      "subtasks": []
    },
    {
      "id": 9,
      "title": "Implement Journal Entry Creation",
      "description": "Create the functionality to generate and save journal entries for Git commits, including context collection and formatting.",
      "status": "pending",
      "dependencies": [],
      "priority": "high",
      "details": "Implement journal entry creation in the MCP server with the following features:\n\n1. Entry generation workflow:\n```python\ndef generate_journal_entry(commit, config, debug=False):\n    \"\"\"Generate a journal entry for a commit\"\"\"\n    # Skip if journal-only commit\n    if is_journal_only_commit(commit, config[\"journal\"][\"path\"]):\n        if debug:\n            print(\"Skipping journal-only commit\")\n        return None\n    \n    # Collect context\n    context = {}\n    if config[\"journal\"][\"include_terminal\"]:\n        try:\n            context[\"terminal\"] = collect_terminal_history(commit.committed_date)\n        except Exception as e:\n            if debug:\n                print(f\"Error collecting terminal history: {e}\")\n    \n    if config[\"journal\"][\"include_chat\"]:\n        try:\n            context[\"chat\"] = collect_chat_history(commit)\n        except Exception as e:\n            if debug:\n                print(f\"Error collecting chat history: {e}\")\n    \n    # Generate entry\n    entry = JournalEntry(commit, config)\n    entry.generate_sections(context)\n    \n    return entry\n```\n\n2. File saving:\n```python\ndef save_journal_entry(entry, config):\n    \"\"\"Save journal entry to appropriate file\"\"\"\n    date = entry.timestamp.strftime(\"%Y-%m-%d\")\n    file_path = Path(config[\"journal\"][\"path\"]) / \"daily\" / f\"{date}.md\"\n    \n    # Create directory if needed\n    file_path.parent.mkdir(parents=True, exist_ok=True)\n    \n    # Append to file\n    with open(file_path, \"a\") as f:\n        f.write(\"\\n\\n\" + entry.to_markdown())\n    \n    return file_path\n```\n\n3. MCP handler implementation:\n```python\n@trace_operation(\"journal_entry_creation\")\nasync def handle_journal_entry_creation(request):\n    \"\"\"Handle journal entry creation operation\"\"\"\n    debug = request.get(\"debug\", False)\n    \n    # Load config\n    config = load_config()\n    \n    # Get current commit\n    repo = get_repo()\n    commit = get_current_commit(repo)\n    \n    # Generate entry\n    entry = generate_journal_entry(commit, config, debug)\n    if not entry:\n        return {\"status\": \"skipped\", \"reason\": \"Journal-only commit\"}\n    \n    # Save entry\n    file_path = save_journal_entry(entry, config)\n    \n    # Check for auto-summarize\n    if config[\"journal\"][\"auto_summarize\"][\"daily\"]:\n        # Check if first commit of day\n        # Implementation\n    \n    return {\n        \"status\": \"success\",\n        \"file_path\": str(file_path),\n        \"entry\": entry.to_markdown()\n    }\n```\n\nNote: All operational journal entry and reflection tasks are handled by the MCP server and AI agent. The CLI commands are limited to setup functionality (journal-init, install-hook). The post-commit hook will call the MCP server endpoint for journal entry creation, which will be handled by the AI agent.",
      "testStrategy": "1. Unit tests for entry generation workflow\n2. Tests for file saving\n3. Tests for MCP handler implementation\n4. Tests for journal-only commit detection\n5. Tests for context collection\n6. Integration tests for full entry creation flow via MCP server\n7. Tests for post-commit hook functionality",
      "subtasks": [
        {
          "id": 1,
          "title": "Implement Journal Entry Generation Workflow",
          "description": "Create generate_journal_entry() function that orchestrates all context collection and section generation functions",
          "details": "# Task 9: Implement Journal Entry Creation - Detailed Subtask Plan\n\n## Subtask 9.1: Implement Journal Entry Generation Workflow\n**Objective**: Create generate_journal_entry() function that orchestrates all context collection and section generation functions to build complete journal entries from commit data.\n\n### TDD Steps:\n1. **WRITE TESTS FIRST**\n   - Create `tests/unit/test_journal_entry_generation.py`\n   - Test `generate_journal_entry(commit, config, debug=False)` function\n   - Test cases: successful entry generation with all sections, journal-only commit detection and skipping, context collection integration (collect_chat_history, collect_ai_terminal_commands, collect_git_context), section generation integration (all 8 generate_*_section functions), graceful degradation when individual functions fail, configuration-driven section inclusion/exclusion, debug mode output validation\n   - Test `is_journal_only_commit(commit, journal_path)` helper function\n   - Test integration with existing JournalEntry and JournalContext classes\n   - **RUN TESTS - VERIFY THEY FAIL**\n\n2. **GET APPROVAL FOR DESIGN CHOICES**\n   - **PAUSE FOR MANUAL APPROVAL**: Header format consistency (match reflection headers or create new format)\n   - **PAUSE FOR MANUAL APPROVAL**: Section ordering and organization in final journal entry\n   - **PAUSE FOR MANUAL APPROVAL**: Configuration schema for enabling/disabling individual sections\n   - **PAUSE FOR MANUAL APPROVAL**: Graceful degradation strategy (skip failed sections vs include error placeholder)\n   - **PAUSE FOR MANUAL APPROVAL**: Journal-only commit detection criteria\n\n3. **IMPLEMENT FUNCTIONALITY**\n   - Implement `generate_journal_entry()` in `src/mcp_commit_story/journal.py`\n   - Create `is_journal_only_commit()` helper function\n   - Integrate ALL context collection functions: collect_chat_history(), collect_ai_terminal_commands(), collect_git_context()\n   - Orchestrate ALL section generators: generate_summary_section(), generate_technical_synopsis_section(), generate_accomplishments_section(), generate_frustrations_section(), generate_tone_mood_section(), generate_discussion_notes_section(), generate_terminal_commands_section(), generate_commit_metadata_section()\n   - Implement graceful degradation: catch individual function errors, log them, continue with other sections\n   - Build complete JournalContext from all collected context\n   - Ensure compatibility with existing JournalEntry class\n   - **RUN TESTS - VERIFY THEY PASS**\n\n4. **DOCUMENT AND COMPLETE**\n   - Add documentation IF NEEDED in three places:\n     1. **Docs directory**: Update journal entry generation workflow documentation\n     2. **PRD**: Update automated journal creation features\n     3. **Engineering Spec**: Update workflow implementation details and make sure TOC is current\n   - **Do not remove existing information unless it's incorrect**\n   - **No approval needed** - make documentation edits directly\n   - **Run the entire test suite and make sure all tests are passing**\n   - **Make sure pyproject.toml is updated as needed**\n   - Double check all subtask requirements are met before marking this subtask as complete\n   - **MARK COMPLETE**\n<info added on 2025-06-03T21:22:39.418Z>\n# Task 9.2: Implement Journal Entry File Operations\n\n## Objective\nCreate functions to handle file operations for journal entries, including saving entries to disk, managing file paths, and handling file-related errors.\n\n### TDD Steps:\n1. **WRITE TESTS FIRST**\n   - Create `tests/unit/test_journal_file_operations.py`\n   - Test `save_journal_entry(entry, config, debug=False)` function\n   - Test `get_journal_file_path(commit_hash, config)` helper function\n   - Test `ensure_journal_directory_exists(config)` helper function\n   - Test cases: successful file saving, directory creation, path generation with various configurations, error handling for file operations, debug mode behavior\n   - **RUN TESTS - VERIFY THEY FAIL**\n\n2. **GET APPROVAL FOR DESIGN CHOICES**\n   - **PAUSE FOR MANUAL APPROVAL**: File naming convention (date-based vs. commit-hash-based)\n   - **PAUSE FOR MANUAL APPROVAL**: Directory structure for journal entries\n   - **PAUSE FOR MANUAL APPROVAL**: File format (markdown vs. other formats)\n   - **PAUSE FOR MANUAL APPROVAL**: Error handling strategy for file operations\n   - **PAUSE FOR MANUAL APPROVAL**: Backup strategy for existing files\n\n3. **IMPLEMENT FUNCTIONALITY**\n   - Implement `save_journal_entry()` in appropriate module (likely journal_workflow.py)\n   - Create `get_journal_file_path()` helper function\n   - Create `ensure_journal_directory_exists()` helper function\n   - Implement file operation error handling with appropriate logging\n   - Ensure compatibility with the JournalEntry class from subtask 9.1\n   - Integrate with configuration system for customizable paths\n   - **RUN TESTS - VERIFY THEY PASS**\n\n4. **DOCUMENT AND COMPLETE**\n   - Add documentation IF NEEDED in three places:\n     1. **Docs directory**: Update journal file operations documentation\n     2. **PRD**: Update file handling features\n     3. **Engineering Spec**: Update file operations implementation details\n   - **Do not remove existing information unless it's incorrect**\n   - **No approval needed** - make documentation edits directly\n   - **Run the entire test suite and make sure all tests are passing**\n   - **Make sure pyproject.toml is updated as needed**\n   - Double check all subtask requirements are met before marking this subtask as complete\n   - **MARK COMPLETE**\n</info added on 2025-06-03T21:22:39.418Z>",
          "status": "done",
          "dependencies": [],
          "parentTaskId": 9
        },
        {
          "id": 2,
          "title": "Implement Journal Entry File Operations",
          "description": "Create save_journal_entry() function for writing journal entries to daily files using existing utilities",
          "details": "## Subtask 9.2: Implement Journal Entry File Operations\n**Objective**: Create save_journal_entry() function that handles writing journal entries to daily journal files using existing utilities.\n\n### TDD Steps:\n1. **WRITE TESTS FIRST**\n   - Create `tests/unit/test_journal_file_operations.py`\n   - Test `save_journal_entry(entry, config)` function\n   - Test cases: successful save to daily file, file creation when doesn't exist, proper entry formatting and separation, integration with existing append_to_journal_file() utility, error handling for file permission issues, directory creation when needed\n   - Test daily file naming convention (YYYY-MM-DD.md format)\n   - Test entry separation and formatting consistency\n   - **RUN TESTS - VERIFY THEY FAIL**\n\n2. **GET APPROVAL FOR DESIGN CHOICES**\n   - **PAUSE FOR MANUAL APPROVAL**: File naming convention consistency with existing journal files\n   - **PAUSE FOR MANUAL APPROVAL**: Entry separation format (newlines, headers, dividers)\n   - **PAUSE FOR MANUAL APPROVAL**: Directory structure and organization\n   - **PAUSE FOR MANUAL APPROVAL**: Integration approach with existing append_to_journal_file() function\n\n3. **IMPLEMENT FUNCTIONALITY**\n   - Implement `save_journal_entry()` in `src/mcp_commit_story/journal.py`\n   - Use existing `append_to_journal_file()` utility from journal.py\n   - Handle daily file management with proper formatting\n   - Ensure consistent entry separation and structure\n   - Add proper error handling for file operations\n   - Create directories as needed\n   - **RUN TESTS - VERIFY THEY PASS**\n\n4. **DOCUMENT AND COMPLETE**\n   - Add documentation IF NEEDED in three places:\n     1. **Docs directory**: Update journal file operations documentation\n     2. **PRD**: Update file management features\n     3. **Engineering Spec**: Update file operation implementation details and make sure TOC is current\n   - **Do not remove existing information unless it's incorrect**\n   - **No approval needed** - make documentation edits directly\n   - **Run the entire test suite and make sure all tests are passing**\n   - **Make sure pyproject.toml is updated as needed**\n   - Double check all subtask requirements are met before marking this subtask as complete\n   - **MARK COMPLETE**",
          "status": "pending",
          "dependencies": [],
          "parentTaskId": 9
        },
        {
          "id": 3,
          "title": "Implement MCP Journal Entry Creation Handler",
          "description": "Create handle_journal_entry_creation() MCP function that integrates generation and file operations",
          "details": "## Subtask 9.3: Implement MCP Journal Entry Creation Handler\n**Objective**: Create handle_journal_entry_creation() function that integrates journal entry generation and file operations into the MCP server.\n\n### TDD Steps:\n1. **WRITE TESTS FIRST**\n   - Create `tests/unit/test_mcp_journal_handler.py`\n   - Test `handle_journal_entry_creation(request)` function with @trace_operation decorator\n   - Test cases: successful journal entry creation end-to-end, MCP request schema validation, git operations integration, auto-summarize integration when configured, error handling for missing commits, telemetry integration with proper span creation\n   - Test MCP response format compliance\n   - Test integration with existing git utilities\n   - **RUN TESTS - VERIFY THEY FAIL**\n\n2. **GET APPROVAL FOR DESIGN CHOICES**\n   - **PAUSE FOR MANUAL APPROVAL**: MCP request schema structure (commit_hash, config overrides, etc.)\n   - **PAUSE FOR MANUAL APPROVAL**: MCP response format (success/error structure, included data)\n   - **PAUSE FOR MANUAL APPROVAL**: Auto-summarize integration approach and configuration\n   - **PAUSE FOR MANUAL APPROVAL**: Telemetry span naming and attribute structure\n\n3. **IMPLEMENT FUNCTIONALITY**\n   - Implement `handle_journal_entry_creation()` in appropriate MCP handler module\n   - Add @trace_operation decorator for telemetry integration\n   - Integrate with generate_journal_entry() from subtask 9.1\n   - Integrate with save_journal_entry() from subtask 9.2\n   - Add proper MCP request/response handling\n   - Implement git operations integration\n   - Add auto-summarize integration when configured\n   - Ensure proper error handling and status reporting\n   - **RUN TESTS - VERIFY THEY PASS**\n\n4. **DOCUMENT AND COMPLETE**\n   - Add documentation IF NEEDED in three places:\n     1. **Docs directory**: Update MCP integration documentation\n     2. **PRD**: Update journal creation workflow features\n     3. **Engineering Spec**: Update MCP handler implementation details and make sure TOC is current\n   - **Do not remove existing information unless it's incorrect**\n   - **No approval needed** - make documentation edits directly\n   - **Run the entire test suite and make sure all tests are passing**\n   - **Make sure pyproject.toml is updated as needed**\n   - Double check all subtask requirements are met before marking this subtask as complete\n   - **MARK COMPLETE**",
          "status": "pending",
          "dependencies": [
            "9.1",
            "9.2"
          ],
          "parentTaskId": 9
        }
      ]
    },
    {
      "id": 11,
      "title": "Implement Summary Generation",
      "description": "Create the functionality to generate daily, weekly, monthly, and yearly summaries of journal entries, with special emphasis on manual reflections.",
      "status": "pending",
      "dependencies": [
        7
      ],
      "priority": "medium",
      "details": "Implement summary generation in the MCP server with the following features:\n\n1. Date range utilities:\n```python\ndef get_date_range(period, date=None):\n    \"\"\"Get start and end dates for a period\"\"\"\n    if date is None:\n        date = datetime.now().date()\n    elif isinstance(date, str):\n        date = datetime.strptime(date, \"%Y-%m-%d\").date()\n    \n    if period == \"day\":\n        return date, date\n    elif period == \"week\":\n        # Start of week (Monday)\n        start = date - timedelta(days=date.weekday())\n        end = start + timedelta(days=6)\n        return start, end\n    elif period == \"month\":\n        start = date.replace(day=1)\n        # Last day of month\n        next_month = date.replace(day=28) + timedelta(days=4)\n        end = next_month - timedelta(days=next_month.day)\n        return start, end\n    elif period == \"year\":\n        start = date.replace(month=1, day=1)\n        end = date.replace(month=12, day=31)\n        return start, end\n    else:\n        raise ValueError(f\"Unknown period: {period}\")\n```\n\n2. Journal file collection:\n```python\ndef get_journal_files_in_range(start_date, end_date, config):\n    \"\"\"Get journal files in date range\"\"\"\n    files = []\n    current = start_date\n    while current <= end_date:\n        file_path = Path(config[\"journal\"][\"path\"]) / \"daily\" / f\"{current.strftime('%Y-%m-%d')}.md\"\n        if file_path.exists():\n            files.append(file_path)\n        current += timedelta(days=1)\n    return files\n```\n\n3. Summary generation with manual reflection prioritization:\n```python\ndef generate_summary(files, period, config):\n    \"\"\"Generate summary from journal files\"\"\"\n    # Extract content from files\n    entries = []\n    manual_reflections = []\n    \n    for file_path in files:\n        with open(file_path, \"r\") as f:\n            content = f.read()\n            # Extract entries and reflections\n            # Extract manual reflections from special sections\n            reflection_sections = extract_manual_reflections(content, file_path.stem)\n            if reflection_sections:\n                manual_reflections.extend(reflection_sections)\n            # Extract regular entries\n            # Implementation\n    \n    # Analyze entries for significance/complexity\n    weighted_entries = []\n    for entry in entries:\n        # Determine entry significance based on factors like:\n        # - Length/detail of the entry\n        # - Presence of technical terms or complex concepts\n        # - Keywords indicating substantial work (\"implemented\", \"designed\", \"solved\")\n        # - Absence of trivial indicators (\"minor fix\", \"typo\", \"small change\")\n        significance_score = calculate_entry_significance(entry)\n        weighted_entries.append((entry, significance_score))\n    \n    # Sort entries by significance score to prioritize important work\n    weighted_entries.sort(key=lambda x: x[1], reverse=True)\n    \n    # Generate summary sections\n    summary = []\n    \n    # Add manual reflections section first - always prioritized\n    if manual_reflections:\n        summary.append(\"# 📝 Manual Reflections\\n\")\n        summary.append(\"*These are your own reflections from the period, presented verbatim.*\\n\")\n        formatted_reflections = []\n        for date, reflection in manual_reflections:\n            formatted_reflections.append(f\"## {date}\\n\\n{reflection}\\n\")\n        summary.append(\"\\n\".join(formatted_reflections))\n    \n    # Add other sections\n    summary.append(\"# Summary\\n\")\n    # Generate overall summary with emphasis on significant entries\n    \n    summary.append(\"# Key Accomplishments\\n\")\n    # Extract accomplishments, prioritizing substantial work\n    \n    summary.append(\"# Challenges\\n\")\n    # Extract challenges, focusing on complex problems\n    \n    summary.append(\"# Technical Decisions\\n\")\n    # Extract decisions, highlighting important architectural choices\n    \n    return \"\\n\\n\".join(summary)\n\ndef extract_manual_reflections(content, date_str):\n    \"\"\"Extract manual reflections from journal content\"\"\"\n    reflections = []\n    \n    # Look for reflection sections with patterns like:\n    # ## Reflection\n    # ## Daily Reflection\n    # ## Personal Reflection\n    # etc.\n    \n    reflection_patterns = [\n        r\"#+\\s*(?:Daily\\s*)?Reflection[s]?\\s*\\n([\\s\\S]*?)(?:\\n#+\\s|$)\",\n        r\"#+\\s*(?:Personal\\s*)?Thought[s]?\\s*\\n([\\s\\S]*?)(?:\\n#+\\s|$)\",\n        r\"#+\\s*(?:Manual\\s*)?Note[s]?\\s*\\n([\\s\\S]*?)(?:\\n#+\\s|$)\"\n    ]\n    \n    for pattern in reflection_patterns:\n        matches = re.finditer(pattern, content, re.MULTILINE)\n        for match in matches:\n            reflection_text = match.group(1).strip()\n            if reflection_text:  # Only add non-empty reflections\n                reflections.append((date_str, reflection_text))\n    \n    return reflections\n\ndef calculate_entry_significance(entry):\n    \"\"\"Calculate significance score for an entry to prioritize substantial work\"\"\"\n    score = 0\n    \n    # Base score from length (longer entries often indicate more substantial work)\n    score += min(len(entry) / 100, 5)  # Cap at 5 points for length\n    \n    # Keywords indicating substantial work\n    substantial_indicators = [\n        \"implement\", \"design\", \"architecture\", \"refactor\", \"optimize\", \n        \"solve\", \"complex\", \"challenge\", \"significant\", \"major\"\n    ]\n    \n    # Keywords indicating trivial work\n    trivial_indicators = [\n        \"typo\", \"minor fix\", \"small change\", \"tweak\", \"trivial\", \n        \"cosmetic\", \"rename\", \"formatting\"\n    ]\n    \n    # Add points for substantial work indicators\n    for word in substantial_indicators:\n        if word in entry.lower():\n            score += 2\n    \n    # Subtract points for trivial work indicators\n    for word in trivial_indicators:\n        if word in entry.lower():\n            score -= 1.5\n    \n    # Analyze for technical complexity\n    # (This could be enhanced with more sophisticated NLP in the future)\n    technical_terms = [\"algorithm\", \"database\", \"architecture\", \"performance\", \"security\"]\n    for term in technical_terms:\n        if term in entry.lower():\n            score += 1\n    \n    return max(score, 0)  # Ensure score doesn't go negative\n```\n\n4. Summary file saving:\n```python\ndef save_summary(content, period, date, config):\n    \"\"\"Save summary to appropriate file\"\"\"\n    if period == \"day\":\n        file_name = f\"{date.strftime('%Y-%m-%d')}-summary.md\"\n        dir_path = Path(config[\"journal\"][\"path\"]) / \"summaries\" / \"daily\"\n    elif period == \"week\":\n        # Get week number\n        week_num = date.isocalendar()[1]\n        file_name = f\"{date.strftime('%Y-%m')}-week{week_num}.md\"\n        dir_path = Path(config[\"journal\"][\"path\"]) / \"summaries\" / \"weekly\"\n    elif period == \"month\":\n        file_name = f\"{date.strftime('%Y-%m')}.md\"\n        dir_path = Path(config[\"journal\"][\"path\"]) / \"summaries\" / \"monthly\"\n    elif period == \"year\":\n        file_name = f\"{date.strftime('%Y')}.md\"\n        dir_path = Path(config[\"journal\"][\"path\"]) / \"summaries\" / \"yearly\"\n    else:\n        raise ValueError(f\"Unknown period: {period}\")\n    \n    # Create file path\n    file_path = dir_path / file_name\n    \n    # Ensure directory exists using on-demand directory creation pattern\n    ensure_journal_directory(dir_path)\n    \n    # Save file\n    with open(file_path, \"w\") as f:\n        f.write(content)\n    \n    return file_path\n```\n\n5. MCP handler implementation:\n```python\n@trace_operation(\"journal_summarize\")\nasync def handle_summarize(request):\n    \"\"\"Handle journal/summarize operation\"\"\"\n    period = request.get(\"period\", \"day\")\n    date = request.get(\"date\")\n    date_range = request.get(\"range\")\n    \n    # Load config\n    config = load_config()\n    \n    # Get date range\n    if date_range:\n        # Parse range (format: \"YYYY-MM-DD:YYYY-MM-DD\")\n        start_str, end_str = date_range.split(\":\")\n        start_date = datetime.strptime(start_str, \"%Y-%m-%d\").date()\n        end_date = datetime.strptime(end_str, \"%Y-%m-%d\").date()\n    else:\n        start_date, end_date = get_date_range(period, date)\n    \n    # Get journal files\n    files = get_journal_files_in_range(start_date, end_date, config)\n    if not files:\n        return {\"status\": \"error\", \"error\": \"No journal entries found in date range\"}\n    \n    # Generate summary\n    content = generate_summary(files, period, config)\n    \n    # Save summary\n    file_path = save_summary(content, period, start_date, config)\n    \n    return {\n        \"status\": \"success\",\n        \"file_path\": str(file_path),\n        \"content\": content\n    }\n```\n\n6. Directory creation utility:\n```python\ndef ensure_journal_directory(dir_path):\n    \"\"\"Ensure the journal directory exists, creating it if necessary\"\"\"\n    if not dir_path.exists():\n        dir_path.mkdir(parents=True, exist_ok=True)\n        logger.info(f\"Created directory: {dir_path}\")\n    return dir_path\n```\n\n7. On-demand directory creation pattern:\n- All summary file-writing operations must use the on-demand directory creation pattern\n- Directories should only be created when needed, not upfront\n- All summary-writing functions (including save_summary) must call ensure_journal_directory(file_path) before writing\n- See docs/on-demand-directory-pattern.md for implementation details and test patterns\n\n8. Manual reflection prioritization:\n- Manual reflections must be prominently displayed at the beginning of summaries\n- Use visual distinction (emoji, formatting) to highlight manual reflections\n- Include date context for each reflection\n- Preserve the original wording of manual reflections\n- Implement reflection extraction from common section patterns\n- Ensure manual reflections are always prioritized over inferred content\n\nNote: This implementation focuses solely on MCP/AI agent operations for summary generation. CLI functionality is limited to setup commands (journal-init, install-hook) only. Refer to updated documentation for details.",
      "testStrategy": "1. Unit tests for date range utilities\n2. Tests for journal file collection\n3. Tests for summary generation\n4. Tests for summary file saving\n5. Tests for MCP handler implementation\n6. Tests for handling different periods (day, week, month, year)\n7. Tests for handling date ranges\n8. Integration tests for full summary generation flow\n9. Tests for entry significance calculation\n10. Tests to verify that substantial work is properly prioritized in summaries\n11. Tests to verify that trivial entries are de-emphasized in summaries\n12. Tests with mixed entry types to ensure proper weighting in the final summary\n13. Tests for on-demand directory creation:\n    - Test that summary directories are created automatically when they don't exist\n    - Test that ensure_journal_directory() is called for all summary types (daily, weekly, monthly, yearly)\n    - Test that directory creation works with nested paths\n    - Test that no errors occur when directories already exist\n    - Test that directories are only created when needed, not upfront\n    - Verify that all summary-writing functions call ensure_journal_directory() before writing\n    - Follow test patterns described in docs/on-demand-directory-pattern.md\n14. Tests to verify that summarization is available as an MCP operation\n15. Tests to verify that the AI agent can properly interact with the summarization functionality\n16. Verify that summary generation works correctly through the MCP interface only (not CLI)\n17. Test that the AI agent can request summaries for different time periods and date ranges\n18. Tests for manual reflection extraction:\n    - Test extraction from various section formats (## Reflection, ## Daily Reflection, etc.)\n    - Test with multiple reflection sections in a single file\n    - Test with reflection sections containing various formatting (lists, code blocks, etc.)\n    - Test with empty reflection sections\n    - Test with reflection sections at different positions in the file\n19. Tests for manual reflection prioritization:\n    - Verify that manual reflections appear at the beginning of summaries\n    - Verify that manual reflections are visually distinguished\n    - Verify that date context is included for each reflection\n    - Verify that original wording is preserved\n    - Test with mixed content (manual reflections and regular entries)\n    - Test with only manual reflections\n    - Test with no manual reflections",
      "subtasks": [
        {
          "id": "11.1",
          "title": "Implement entry significance calculation",
          "description": "Create the algorithm to analyze journal entries and assign significance scores based on content analysis.",
          "status": "pending"
        },
        {
          "id": "11.2",
          "title": "Modify summary generation to prioritize significant entries",
          "description": "Update the summary generation logic to give more narrative weight to entries with higher significance scores.",
          "status": "pending"
        },
        {
          "id": "11.3",
          "title": "Create test cases for entry significance calculation",
          "description": "Develop test cases with various types of entries (substantial, trivial, mixed) to verify proper significance scoring.",
          "status": "pending"
        },
        {
          "id": "11.4",
          "title": "Test summary prioritization with real-world examples",
          "description": "Test the summary generation with a set of real-world journal entries to ensure meaningful work is properly highlighted.",
          "status": "pending"
        },
        {
          "id": "11.5",
          "title": "Implement ensure_journal_directory utility",
          "description": "Create the utility function to ensure journal directories exist, creating them on-demand if necessary.",
          "status": "pending"
        },
        {
          "id": "11.6",
          "title": "Update save_summary to use ensure_journal_directory",
          "description": "Modify the save_summary function to use the ensure_journal_directory utility for all summary types.",
          "status": "pending"
        },
        {
          "id": "11.7",
          "title": "Add tests for directory creation functionality",
          "description": "Create tests to verify that summary directories are created automatically when they don't exist and that the ensure_journal_directory utility works correctly.",
          "status": "pending"
        },
        {
          "id": "11.8",
          "title": "Implement on-demand directory creation pattern",
          "description": "Update all summary file-writing operations to follow the on-demand directory creation pattern as described in docs/on-demand-directory-pattern.md.",
          "status": "pending"
        },
        {
          "id": "11.9",
          "title": "Add tests for on-demand directory creation",
          "description": "Create tests to verify that directories are only created when needed, not upfront, and that all summary-writing functions call ensure_journal_directory() before writing.",
          "status": "pending"
        },
        {
          "id": "11.10",
          "title": "Review and update all file-writing operations",
          "description": "Review all file-writing operations in the codebase to ensure they follow the on-demand directory creation pattern.",
          "status": "pending"
        },
        {
          "id": "11.11",
          "title": "Verify MCP operation for summarization",
          "description": "Ensure that summarization is properly implemented as an MCP operation and accessible to the AI agent.",
          "status": "pending"
        },
        {
          "id": "11.12",
          "title": "Test AI agent interaction with summarization",
          "description": "Create tests to verify that the AI agent can properly request and process summary generation through the MCP server.",
          "status": "pending"
        },
        {
          "id": "11.13",
          "title": "Ensure summary generation is MCP-only",
          "description": "Verify that summary generation functionality is only available through the MCP interface and not through CLI commands.",
          "status": "pending"
        },
        {
          "id": "11.14",
          "title": "Update documentation to reflect MCP-only approach",
          "description": "Update relevant documentation to clarify that summary generation is only available through the MCP/AI agent interface, not through CLI commands.",
          "status": "pending"
        },
        {
          "id": "11.15",
          "title": "Implement manual reflection extraction",
          "description": "Create functionality to extract manual reflections from journal entries using pattern matching for common section headers.",
          "status": "pending"
        },
        {
          "id": "11.16",
          "title": "Implement manual reflection prioritization in summaries",
          "description": "Update summary generation to display manual reflections prominently at the beginning with visual distinction and date context.",
          "status": "pending"
        },
        {
          "id": "11.17",
          "title": "Add tests for manual reflection extraction",
          "description": "Create tests to verify that manual reflections are correctly extracted from various section formats and positions.",
          "status": "pending"
        },
        {
          "id": "11.18",
          "title": "Add tests for manual reflection prioritization",
          "description": "Create tests to verify that manual reflections appear at the beginning of summaries with proper visual distinction and preserved wording.",
          "status": "pending"
        }
      ]
    },
    {
      "id": 12,
      "title": "Implement Blog Post Generation",
      "description": "Create the functionality to convert journal entries and summaries into blog post format for storytelling.",
      "details": "Implement blog post generation in both the MCP server and CLI with the following features:\n\n1. Blog post generation:\n```python\ndef generate_blog_post(files, config):\n    \"\"\"Generate blog post from journal files\"\"\"\n    # Extract content from files\n    entries = []\n    \n    for file_path in files:\n        with open(file_path, \"r\") as f:\n            content = f.read()\n            # Extract entries\n            # Implementation\n    \n    # Generate blog post sections\n    blog_post = []\n    \n    # Add title and introduction\n    blog_post.append(\"# Project Journey: From Idea to Implementation\\n\")\n    blog_post.append(\"*An engineering story based on journal entries*\\n\")\n    \n    # Add narrative sections\n    blog_post.append(\"## The Challenge\\n\")\n    # Generate challenge narrative\n    \n    blog_post.append(\"## The Approach\\n\")\n    # Generate approach narrative\n    \n    blog_post.append(\"## Key Decisions\\n\")\n    # Extract and narrate decisions\n    \n    blog_post.append(\"## Lessons Learned\\n\")\n    # Extract and narrate lessons\n    \n    blog_post.append(\"## Conclusion\\n\")\n    # Generate conclusion\n    \n    return \"\\n\\n\".join(blog_post)\n```\n\n2. Blog post file saving:\n```python\ndef save_blog_post(content, title, config):\n    \"\"\"Save blog post to file\"\"\"\n    # Create directory if needed\n    dir_path = Path(config[\"journal\"][\"path\"]) / \"blog_posts\"\n    dir_path.mkdir(parents=True, exist_ok=True)\n    \n    # Generate file name from title\n    file_name = title.lower().replace(\" \", \"-\") + \".md\"\n    file_path = dir_path / file_name\n    \n    # Save file\n    with open(file_path, \"w\") as f:\n        f.write(content)\n    \n    return file_path\n```\n\n3. MCP handler implementation:\n```python\n@trace_operation(\"journal_blogify\")\nasync def handle_blogify(request):\n    \"\"\"Handle journal/blogify operation\"\"\"\n    files = request.get(\"files\", [])\n    title = request.get(\"title\", \"Engineering Journey\")\n    \n    if not files:\n        return {\"status\": \"error\", \"error\": \"No files provided\"}\n    \n    # Load config\n    config = load_config()\n    \n    # Convert file paths to Path objects\n    file_paths = [Path(f) for f in files]\n    \n    # Check if files exist\n    missing = [str(f) for f in file_paths if not f.exists()]\n    if missing:\n        return {\"status\": \"error\", \"error\": f\"Files not found: {', '.join(missing)}\"}\n    \n    # Generate blog post\n    content = generate_blog_post(file_paths, config)\n    \n    # Save blog post\n    file_path = save_blog_post(content, title, config)\n    \n    return {\n        \"status\": \"success\",\n        \"file_path\": str(file_path),\n        \"content\": content\n    }\n```\n\n4. CLI command implementation:\n```python\n@cli.command()\n@click.argument(\"files\", nargs=-1, type=click.Path(exists=True))\n@click.option(\"--title\", default=\"Engineering Journey\", help=\"Blog post title\")\n@click.option(\"--debug\", is_flag=True, help=\"Show debug information\")\ndef blogify(files, title, debug):\n    \"\"\"Convert journal entries to blog post\"\"\"\n    try:\n        if not files:\n            click.echo(\"No files provided\")\n            return\n        \n        # Load config\n        config = load_config()\n        \n        # Convert file paths to Path objects\n        file_paths = [Path(f) for f in files]\n        \n        # Generate blog post\n        content = generate_blog_post(file_paths, config)\n        \n        # Save blog post\n        file_path = save_blog_post(content, title, config)\n        \n        click.echo(f\"Blog post saved to {file_path}\")\n    except Exception as e:\n        if debug:\n            click.echo(f\"Error: {e}\")\n            traceback.print_exc()\n        else:\n            click.echo(f\"Error: {e}\")\n```",
      "testStrategy": "1. Unit tests for blog post generation\n2. Tests for blog post file saving\n3. Tests for MCP handler implementation\n4. Tests for CLI command implementation\n5. Tests for handling multiple input files\n6. Tests for narrative generation\n7. Integration tests for full blog post generation flow",
      "priority": "low",
      "dependencies": [
        7
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 13,
      "title": "Implement Backfill Functionality",
      "description": "Create the functionality to detect and generate journal entries for missed commits.",
      "details": "Implement backfill functionality in both the MCP server and CLI with the following features:\n\n1. Missed commit detection:\n```python\ndef get_missed_commits(repo, config):\n    \"\"\"Get commits that don't have journal entries\"\"\"\n    # Get journal directory\n    journal_path = Path(config[\"journal\"][\"path\"])\n    \n    # Get all commits\n    commits = list(repo.iter_commits())\n    \n    # Get all journal files\n    journal_files = list(journal_path.glob(\"daily/*.md\"))\n    \n    # Extract commit hashes from journal files\n    journal_commits = set()\n    for file_path in journal_files:\n        with open(file_path, \"r\") as f:\n            content = f.read()\n            # Extract commit hashes using regex\n            # Implementation\n    \n    # Find commits not in journal\n    missed_commits = []\n    for commit in commits:\n        if commit.hexsha not in journal_commits and not is_journal_only_commit(commit, config[\"journal\"][\"path\"]):\n            missed_commits.append(commit)\n    \n    return missed_commits\n```\n\n2. Backfill processing:\n```python\ndef process_backfill(commits, config, debug=False):\n    \"\"\"Process backfill for missed commits\"\"\"\n    results = []\n    \n    # Sort commits by date\n    commits.sort(key=lambda c: c.committed_date)\n    \n    for commit in commits:\n        # Generate entry\n        entry = generate_journal_entry(commit, config, debug)\n        if not entry:\n            continue\n        \n        # Mark as backfilled\n        entry.is_backfilled = True\n        \n        # Save entry\n        file_path = save_journal_entry(entry, config)\n        \n        results.append({\n            \"commit\": commit.hexsha,\n            \"file_path\": str(file_path)\n        })\n    \n    return results\n```\n\n3. MCP handler implementation:\n```python\n@trace_operation(\"journal_backfill\")\nasync def handle_backfill(request):\n    \"\"\"Handle journal/backfill operation\"\"\"\n    debug = request.get(\"debug\", False)\n    \n    # Load config\n    config = load_config()\n    \n    # Get repo\n    repo = get_repo()\n    \n    # Get missed commits\n    missed_commits = get_missed_commits(repo, config)\n    if not missed_commits:\n        return {\"status\": \"success\", \"message\": \"No missed commits found\"}\n    \n    # Process backfill\n    results = process_backfill(missed_commits, config, debug)\n    \n    return {\n        \"status\": \"success\",\n        \"count\": len(results),\n        \"entries\": results\n    }\n```\n\n4. CLI command implementation:\n```python\n@cli.command()\n@click.option(\"--debug\", is_flag=True, help=\"Show debug information\")\ndef backfill(debug):\n    \"\"\"Check for missed commits and create entries\"\"\"\n    try:\n        # Load config\n        config = load_config()\n        \n        # Get repo\n        repo = get_repo()\n        \n        # Get missed commits\n        missed_commits = get_missed_commits(repo, config)\n        if not missed_commits:\n            click.echo(\"No missed commits found\")\n            return\n        \n        # Process backfill\n        results = process_backfill(missed_commits, config, debug)\n        \n        click.echo(f\"Created {len(results)} journal entries for missed commits\")\n        for result in results:\n            click.echo(f\"  - {result['commit'][:8]}: {result['file_path']}\")\n    except Exception as e:\n        if debug:\n            click.echo(f\"Error: {e}\")\n            traceback.print_exc()\n        else:\n            click.echo(f\"Error: {e}\")\n```",
      "testStrategy": "1. Unit tests for missed commit detection\n2. Tests for backfill processing\n3. Tests for MCP handler implementation\n4. Tests for CLI command implementation\n5. Tests for handling journal-only commits\n6. Tests for chronological ordering of backfilled entries\n7. Integration tests for full backfill flow",
      "priority": "medium",
      "dependencies": [
        7
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 15,
      "title": "Create Comprehensive Tests and Documentation",
      "description": "Develop comprehensive tests for all components and create documentation for the project.",
      "status": "pending",
      "dependencies": [
        7,
        9,
        11,
        12,
        13
      ],
      "priority": "high",
      "details": "Create comprehensive tests and documentation with the following features:\n\n1. Test fixtures:\n```python\n@pytest.fixture\ndef mock_git_repo():\n    \"\"\"Create temporary git repo with test commits\"\"\"\n    # Implementation\n\n@pytest.fixture\ndef sample_journal_entries():\n    \"\"\"Load sample journal files\"\"\"\n    # Implementation\n\n@pytest.fixture\ndef mock_terminal_history():\n    \"\"\"Provide test terminal command history\"\"\"\n    # Implementation\n\n@pytest.fixture\ndef mock_chat_history():\n    \"\"\"Provide test chat history\"\"\"\n    # Implementation\n\n@pytest.fixture\ndef mock_telemetry_exporter():\n    \"\"\"Provide a test exporter that captures telemetry events\"\"\"\n    # Implementation\n```\n\n2. Unit tests:\n```python\ndef test_config_loading():\n    \"\"\"Test configuration loading\"\"\"\n    # Implementation\n\ndef test_git_utils():\n    \"\"\"Test git utilities\"\"\"\n    # Implementation\n\ndef test_journal_entry_generation():\n    \"\"\"Test journal entry generation\"\"\"\n    # Implementation\n\ndef test_telemetry():\n    \"\"\"Test telemetry integration\"\"\"\n    # Implementation\n\n# Additional unit tests for all components\n```\n\n3. Integration tests:\n```python\ndef test_cli_init():\n    \"\"\"Test CLI init command\"\"\"\n    # Implementation\n\ndef test_cli_new_entry():\n    \"\"\"Test CLI new-entry command\"\"\"\n    # Implementation\n\ndef test_mcp_server():\n    \"\"\"Test MCP server operations\"\"\"\n    # Implementation\n\n# Additional integration tests for all workflows\n```\n\n4. Documentation:\n   - README.md with project overview, installation, and usage\n   - Configuration documentation\n   - CLI command reference\n   - MCP server API reference\n   - Development guide\n   - Examples and tutorials\n\n5. Test coverage:\n   - Configure pytest-cov for coverage reporting\n   - Ensure >90% test coverage\n   - Add coverage reporting to CI pipeline\n\n6. Documentation structure:\n```\nREADME.md\ndocs/\n├── configuration.md\n├── cli.md\n├── mcp-server.md\n├── development.md\n└── examples/\n    ├── basic-usage.md\n    ├── custom-configuration.md\n    └── integration-examples.md\n```",
      "testStrategy": "1. Verify test coverage meets >90% threshold\n2. Ensure all components have unit tests\n3. Verify integration tests cover all workflows\n4. Test documentation for accuracy and completeness\n5. Verify examples work as documented\n6. Test installation and usage instructions\n7. Verify CI pipeline runs all tests\n8. Ensure telemetry system is thoroughly tested with both unit and integration tests",
      "subtasks": [
        {
          "id": "15.1",
          "title": "Implement telemetry-specific tests",
          "description": "Create comprehensive tests for the telemetry system implemented in task 4",
          "status": "pending",
          "details": "Develop unit and integration tests for the telemetry infrastructure including:\n1. Test telemetry event generation\n2. Test telemetry data collection\n3. Test telemetry exporters\n4. Test telemetry configuration options\n5. Test telemetry integration with other components"
        },
        {
          "id": "15.2",
          "title": "Document telemetry system",
          "description": "Create documentation for the telemetry system",
          "status": "pending",
          "details": "Add telemetry documentation including:\n1. Overview of telemetry capabilities\n2. Configuration options for telemetry\n3. How to extend telemetry with custom exporters\n4. Privacy considerations\n5. Add a telemetry.md file to the docs directory"
        }
      ]
    },
    {
      "id": 19,
      "title": "Document MCP Server Configuration and Integration",
      "description": "Ensure the MCP server launch/discovery/configuration requirements are documented in the PRD, README, and codebase. The MCP server must be launchable as a standalone process, expose the required journal operations, and be discoverable by compatible clients. The method for launching the MCP server is not prescribed; it may be started via CLI, Python entry point, etc.",
      "details": "",
      "testStrategy": "",
      "status": "pending",
      "dependencies": [],
      "priority": "medium",
      "subtasks": [
        {
          "id": 1,
          "title": "Provide generic client/editor config block example",
          "description": "Add a JSON example of a configuration block for connecting to the MCP server, showing command, args, and optional env vars.",
          "details": "",
          "status": "pending",
          "dependencies": [],
          "parentTaskId": 19
        },
        {
          "id": 2,
          "title": "Clarify API key/env var requirements",
          "description": "Document that API keys or environment variables are only required if the underlying SDK or provider needs them, not for all deployments.",
          "details": "",
          "status": "pending",
          "dependencies": [],
          "parentTaskId": 19
        },
        {
          "id": 3,
          "title": "Ensure separation of MCP server config from journal config",
          "description": "Make sure documentation clearly distinguishes between MCP server configuration and the journal system's .mcp-journalrc.yaml.",
          "details": "",
          "status": "pending",
          "dependencies": [],
          "parentTaskId": 19
        },
        {
          "id": 4,
          "title": "Review and update README/docs",
          "description": "Review and update the README.md and other documentation to reflect changes made in this task. Ensure documentation is clear, accurate, and up to date.",
          "details": "",
          "status": "pending",
          "dependencies": [
            "19.1",
            "19.2",
            "19.3"
          ],
          "parentTaskId": 19
        }
      ]
    },
    {
      "id": 21,
      "title": "Integrate Codecov for Test Coverage Reporting",
      "description": "Set up Codecov integration with the GitHub repository to track and report test coverage metrics, culminating in a functional coverage badge in the README.",
      "details": "This task involves establishing a connection between the repository and Codecov to enable automated test coverage reporting. Implementation steps include:\n\n1. Create a Codecov account if not already available and link it to the organization's GitHub account\n2. Add the repository to Codecov's dashboard\n3. Generate a Codecov token for secure communication between CI and Codecov\n4. Update the CI pipeline configuration (GitHub Actions, CircleCI, etc.) to:\n   - Install necessary coverage tools (e.g., pytest-cov for Python)\n   - Run tests with coverage collection enabled\n   - Upload coverage reports to Codecov using the token\n5. Add a `.codecov.yml` configuration file to the repository root to customize coverage settings (thresholds, exclusions, etc.)\n6. Uncomment or add the Codecov badge in the README.md file using the format provided by Codecov\n7. Verify the badge displays the actual coverage percentage after the first successful upload\n\nConsider setting coverage thresholds to maintain code quality and potentially configure PR comments from Codecov to highlight coverage changes in code reviews.",
      "testStrategy": "To verify successful completion of this task:\n\n1. Manually trigger a CI build and confirm the coverage report is generated and uploaded to Codecov\n2. Check the Codecov dashboard to ensure:\n   - The repository appears with correct coverage data\n   - Historical data begins tracking from the first upload\n   - Coverage reports include all relevant files (no critical omissions)\n3. Verify the Codecov badge in the README:\n   - Badge is properly displayed (not broken)\n   - Badge shows an actual percentage value (not \"unknown\" or \"N/A\")\n   - The percentage matches what's shown in the Codecov dashboard\n4. Create a test PR with code changes that would affect coverage (both positively and negatively) to confirm:\n   - Codecov reports the coverage change in the PR\n   - The badge updates accordingly after merging\n5. Document the integration process in the project documentation for future reference\n6. Have another team member verify they can access the Codecov dashboard for the repository",
      "status": "pending",
      "dependencies": [],
      "priority": "medium",
      "subtasks": []
    },
    {
      "id": 22,
      "title": "Implement Remaining MCP Server Handlers",
      "description": "Add the remaining non-MVP MCP tool handlers to complete the full feature set after their backend dependencies are implemented.",
      "status": "pending",
      "dependencies": [
        11,
        12,
        13
      ],
      "priority": "medium",
      "details": "Implement the remaining MCP server tool handlers in `src/mcp_commit_story/server.py` to complete the full feature set:\n\n1. **journal/summarize** handler:\n   - Depends on Task 11 (Summary Generation)\n   - Handle daily, weekly, monthly, yearly summary requests\n   - Return summary content and file paths\n   - Must use on-demand directory creation pattern\n\n2. **journal/blogify** handler:\n   - Depends on Task 12 (Blog Post Generation)\n   - Convert journal entries to blog post format\n   - Accept multiple file inputs\n   - Must use on-demand directory creation pattern\n\n3. **journal/backfill** handler:\n   - Depends on Task 13 (Backfill Functionality)\n   - Detect and create entries for missed commits\n   - Return list of created entries\n   - Must use on-demand directory creation pattern\n\n4. **journal/add-reflection** handler:\n   - Add reflection content to existing journal entries\n   - Accept entry path and reflection content\n   - Must use on-demand directory creation pattern\n\nAll handlers should:\n- Use existing `@handle_mcp_error` decorator\n- Follow TypedDict patterns established in Tasks 6.3-6.4\n- Include proper async/await support\n- Integrate with existing backend logic from their dependency tasks\n- Include comprehensive error handling and validation\n- Call ensure_journal_directory(file_path) before writing any files\n- Never create directories upfront - only on demand when needed\n- Implement as MCP operations only (no CLI commands required)\n- Focus exclusively on MCP/AI agent operations for file-writing handlers\n\nNote that the CLI functionality is limited to setup commands (journal-init, install-hook) only. All file-writing functionality must be implemented as MCP operations. Refer to the updated engineering spec and README.md for implementation details and test patterns.",
      "testStrategy": "1. Unit tests for each new handler\n2. Integration tests with backend logic\n3. Error handling validation\n4. End-to-end workflow testing\n5. Backward compatibility with existing handlers\n6. Verify on-demand directory creation pattern is used correctly\n7. Test that directories are only created when files are actually written\n8. Verify ensure_journal_directory() is called before file writes\n9. Verify all file-writing functionality is accessible via MCP operations only\n10. Test the journal/add-reflection handler functionality as an MCP operation",
      "subtasks": []
    },
    {
      "id": 26,
      "title": "Create Packaging Strategy and Release Process for MVP Launch",
      "description": "Develop a comprehensive packaging and distribution strategy for the MCP Commit Story MVP, including PyPI publishing, version management, installation methods, and release processes.",
      "details": "This task involves creating a complete packaging strategy and implementation plan for the MCP Commit Story MVP launch:\n\n1. **Distribution Strategy**:\n   - Set up PyPI package configuration with appropriate metadata in setup.py/pyproject.toml\n   - Implement semantic versioning (MAJOR.MINOR.PATCH) with version tracking in a dedicated file\n   - Configure CI/CD pipeline for automated releases using GitHub Actions or similar\n   - Define package dependencies with appropriate version constraints\n   - Create package structure with proper namespacing\n\n2. **Installation Methods**:\n   - Implement standard pip installation: `pip install mcp-commit-story`\n   - Create development installation process: `pip install -e .` with dev dependencies\n   - Document MCP server deployment options (standalone, Docker, etc.)\n   - Write detailed configuration guides for different environments\n\n3. **Release Process**:\n   - Implement automated version tagging and changelog generation\n   - Create pre-release testing checklist and validation procedures\n   - Set up documentation update workflow tied to releases\n   - Document rollback procedures for failed releases\n   - Establish release branch strategy (e.g., release/v1.0.0)\n\n4. **User Experience Documentation**:\n   - Write comprehensive getting started guide\n   - Create integration examples for VSCode, PyCharm, and command line\n   - Develop troubleshooting guide with common issues and solutions\n   - Set up community support channels (GitHub Discussions, Discord, etc.)\n\n5. **Technical Implementation**:\n   - Define package structure with clear entry points\n   - Implement dependency management with compatibility matrices\n   - Create environment testing matrix (OS, Python versions)\n   - Document performance benchmarks and minimum requirements\n   - Ensure journal entry functionality is properly packaged and accessible\n\nImplementation should follow Python packaging best practices and ensure the journal entry creation functionality from Task 9 is properly exposed and documented in the package.",
      "testStrategy": "To verify the packaging strategy and release process:\n\n1. **Package Structure Testing**:\n   - Validate package structure using `check-manifest`\n   - Verify all necessary files are included in the distribution\n   - Test package installation in a clean virtual environment\n   - Confirm entry points work as expected after installation\n\n2. **Release Process Validation**:\n   - Perform a test release to TestPyPI\n   - Verify version bumping and changelog generation\n   - Test the release automation pipeline with a pre-release version\n   - Validate rollback procedures with a simulated failed release\n\n3. **Installation Testing**:\n   - Test pip installation on different operating systems (Windows, macOS, Linux)\n   - Verify development installation for contributors\n   - Test MCP server deployment using the documented methods\n   - Validate configuration options work as described\n\n4. **Documentation Review**:\n   - Conduct user testing with the getting started guide\n   - Review integration examples for accuracy and completeness\n   - Verify troubleshooting documentation addresses common issues\n   - Test community support channels are properly set up\n\n5. **Functionality Testing**:\n   - Verify journal entry creation (from Task 9) works correctly after package installation\n   - Test all documented features are accessible through the package\n   - Validate performance meets the documented benchmarks\n   - Ensure compatibility with all supported Python versions and environments\n\nThe packaging strategy is considered complete when a test user can successfully install and use the package following only the provided documentation.",
      "status": "pending",
      "dependencies": [
        9,
        "27"
      ],
      "priority": "high",
      "subtasks": []
    },
    {
      "id": 27,
      "title": "Implement Daily Summary Git Hook Trigger",
      "description": "Create functionality that automatically generates a daily summary of journal entries from the previous day, triggered by a Git hook when the date changes.",
      "status": "pending",
      "dependencies": [
        9
      ],
      "priority": "high",
      "details": "This task involves implementing an automated daily summary generation system triggered by Git hooks:\n\n1. **Git Hook Implementation**:\n   ```bash\n   #!/bin/bash\n   # post-commit hook to check for date change\n   \n   # Get current date\n   CURRENT_DATE=$(date +%Y-%m-%d)\n   \n   # Get previous date from state file\n   STATE_FILE=\".commit-story-state\"\n   if [ -f \"$STATE_FILE\" ]; then\n     PREV_DATE=$(cat \"$STATE_FILE\")\n   else\n     PREV_DATE=\"\"\n   fi\n   \n   # Update state file with current date\n   echo \"$CURRENT_DATE\" > \"$STATE_FILE\"\n   \n   # If date changed, trigger summary generation\n   if [ \"$PREV_DATE\" != \"\" ] && [ \"$PREV_DATE\" != \"$CURRENT_DATE\" ]; then\n     # Call the summary generator for previous day\n     commit-story generate-summary --period day --date \"$PREV_DATE\"\n   fi\n   ```\n\n2. **Daily Summary Generation Function**:\n   ```python\n   def generate_daily_summary(date=None):\n       \"\"\"Generate summary for the specified date (defaults to yesterday)\"\"\"\n       if date is None:\n           # Default to yesterday\n           date = (datetime.now() - timedelta(days=1)).date()\n       elif isinstance(date, str):\n           date = datetime.strptime(date, \"%Y-%m-%d\").date()\n           \n       # Get all journal entries for the specified date\n       entries = get_journal_entries_for_date(date)\n       \n       if not entries:\n           logger.info(f\"No journal entries found for {date}\")\n           return None\n           \n       # Generate summary using existing summary generation logic\n       summary = synthesize_entries(entries, date)\n       \n       # Save summary to appropriate location\n       save_daily_summary(summary, date)\n       \n       return summary\n   ```\n\n3. **Entry Collection and Parsing**:\n   ```python\n   def get_journal_entries_for_date(date):\n       \"\"\"Retrieve all journal entries for the specified date\"\"\"\n       journal_path = get_journal_path()\n       date_str = date.strftime(\"%Y-%m-%d\")\n       \n       # Find all journal files for the date\n       entry_pattern = f\"{date_str}-*.md\"\n       entry_files = list(journal_path.glob(entry_pattern))\n       \n       entries = []\n       for file_path in entry_files:\n           with open(file_path, \"r\") as f:\n               content = f.read()\n               entries.append(parse_journal_entry(content, file_path))\n               \n       return entries\n   ```\n\n4. **Summary Synthesis**:\n   ```python\n   def synthesize_entries(entries, date):\n       \"\"\"Create a cohesive summary from multiple journal entries\"\"\"\n       # Sort entries by timestamp\n       entries.sort(key=lambda e: e.get('timestamp'))\n       \n       # Extract key information\n       commits = [e.get('commit_message') for e in entries if e.get('commit_message')]\n       reflections = [e.get('reflection') for e in entries if e.get('reflection')]\n       \n       # Prioritize manual reflections as a core requirement\n       manual_reflections = [r for r in reflections if r.get('is_manual', False)]\n       \n       # Generate summary template\n       summary = {\n           \"date\": date.strftime(\"%Y-%m-%d\"),\n           \"title\": f\"Daily Summary for {date.strftime('%B %d, %Y')}\",\n           \"manual_reflections\": manual_reflections,\n           \"commit_count\": len(commits),\n           \"commit_summary\": summarize_commits(commits),\n           \"key_achievements\": extract_key_achievements(entries),\n           \"challenges\": extract_challenges(entries),\n           \"next_steps\": suggest_next_steps(entries)\n       }\n       \n       return summary\n   ```\n\n5. **Summary Storage**:\n   ```python\n   def save_daily_summary(summary, date):\n       \"\"\"Save the generated summary to the appropriate location\"\"\"\n       journal_path = get_journal_path()\n       summaries_dir = journal_path / \"summaries\" / \"daily\"\n       \n       # Create directories if they don't exist\n       summaries_dir.mkdir(parents=True, exist_ok=True)\n       \n       # Create filename\n       filename = f\"{date.strftime('%Y-%m-%d')}-daily-summary.md\"\n       file_path = summaries_dir / filename\n       \n       # Format summary as markdown\n       content = format_summary_as_markdown(summary)\n       \n       # Save to file\n       with open(file_path, \"w\") as f:\n           f.write(content)\n           \n       logger.info(f\"Daily summary saved to {file_path}\")\n       return file_path\n   ```\n\n6. **Format Summary with Prioritized Manual Reflections**:\n   ```python\n   def format_summary_as_markdown(summary):\n       \"\"\"Format the summary as a markdown document with prioritized manual reflections\"\"\"\n       md_content = []\n       \n       # Add title\n       md_content.append(f\"# {summary['title']}\\n\")\n       \n       # Prominently display manual reflections at the beginning with visual distinction\n       if summary.get('manual_reflections'):\n           md_content.append(\"## 💭 Manual Reflections\\n\")\n           md_content.append(\"<div class='manual-reflections'>\\n\")\n           \n           for reflection in summary['manual_reflections']:\n               md_content.append(f\"### {reflection.get('title', 'Reflection')}\\n\")\n               md_content.append(f\"{reflection.get('content', '')}\\n\\n\")\n           \n           md_content.append(\"</div>\\n\")\n       \n       # Add commit summary\n       md_content.append(f\"## Commit Activity\\n\")\n       md_content.append(f\"Total commits: {summary['commit_count']}\\n\\n\")\n       md_content.append(f\"{summary['commit_summary']}\\n\")\n       \n       # Add key achievements\n       md_content.append(f\"## Key Achievements\\n\")\n       for achievement in summary['key_achievements']:\n           md_content.append(f\"- {achievement}\\n\")\n       md_content.append(\"\\n\")\n       \n       # Add challenges\n       if summary.get('challenges'):\n           md_content.append(f\"## Challenges\\n\")\n           for challenge in summary['challenges']:\n               md_content.append(f\"- {challenge}\\n\")\n           md_content.append(\"\\n\")\n       \n       # Add next steps\n       md_content.append(f\"## Next Steps\\n\")\n       for step in summary['next_steps']:\n           md_content.append(f\"- {step}\\n\")\n       \n       return ''.join(md_content)\n   ```\n\n7. **CLI Integration**:\n   ```python\n   @click.command()\n   @click.option(\"--date\", help=\"Date to generate summary for (YYYY-MM-DD)\")\n   def generate_summary_command(date=None):\n       \"\"\"Generate a daily summary for the specified date\"\"\"\n       summary = generate_daily_summary(date)\n       if summary:\n           click.echo(f\"Summary generated for {summary['date']}\")\n       else:\n           click.echo(\"No entries found for the specified date\")\n   ```\n\n8. **Git Hook Installation**:\n   ```python\n   def install_git_hooks():\n       \"\"\"Install the necessary git hooks for commit-story\"\"\"\n       repo_path = get_git_repo_path()\n       hooks_dir = repo_path / \".git\" / \"hooks\"\n       \n       # Create post-commit hook\n       post_commit_path = hooks_dir / \"post-commit\"\n       \n       # Write hook content\n       with open(post_commit_path, \"w\") as f:\n           f.write(POST_COMMIT_HOOK_CONTENT)\n           \n       # Make hook executable\n       os.chmod(post_commit_path, 0o755)\n       \n       logger.info(f\"Installed post-commit hook at {post_commit_path}\")\n   ```\n\n9. **Configuration Integration**:\n   - Add configuration options for daily summary generation\n   - Allow customization of summary format and content\n   - Provide options to disable automatic triggering\n   - Include options for manual reflection styling and positioning\n\n10. **Error Handling and Logging**:\n   - Implement robust error handling for the Git hook\n   - Ensure failures don't disrupt normal Git operations\n   - Log summary generation attempts and results",
      "testStrategy": "To verify the correct implementation of the daily summary Git hook trigger:\n\n1. **Unit Tests for Summary Generation**:\n   ```python\n   def test_daily_summary_generation():\n       # Create mock journal entries for a specific date\n       mock_date = datetime.strptime(\"2023-05-15\", \"%Y-%m-%d\").date()\n       mock_entries = create_mock_journal_entries(mock_date, count=3)\n       \n       # Test summary generation\n       summary = synthesize_entries(mock_entries, mock_date)\n       \n       # Verify summary structure\n       assert summary[\"date\"] == \"2023-05-15\"\n       assert summary[\"commit_count\"] == 3\n       assert \"commit_summary\" in summary\n       assert \"key_achievements\" in summary\n   \n   def test_entry_collection():\n       # Create mock journal files for a specific date\n       mock_date = datetime.strptime(\"2023-05-16\", \"%Y-%m-%d\").date()\n       create_mock_journal_files(mock_date, count=4)\n       \n       # Test entry collection\n       entries = get_journal_entries_for_date(mock_date)\n       \n       # Verify entries were collected correctly\n       assert len(entries) == 4\n       for entry in entries:\n           assert \"timestamp\" in entry\n           assert \"content\" in entry\n   ```\n\n2. **Integration Test for Git Hook**:\n   ```python\n   def test_git_hook_trigger():\n       # Set up a test repository\n       repo_dir = setup_test_repo()\n       \n       # Install the git hook\n       install_git_hooks()\n       \n       # Create mock state file with yesterday's date\n       yesterday = (datetime.now() - timedelta(days=1)).date().strftime(\"%Y-%m-%d\")\n       with open(os.path.join(repo_dir, \".commit-story-state\"), \"w\") as f:\n           f.write(yesterday)\n       \n       # Create mock journal entries for yesterday\n       create_mock_journal_files(yesterday, count=2)\n       \n       # Make a commit to trigger the hook\n       make_test_commit(repo_dir, \"Test commit\")\n       \n       # Check if summary was generated\n       summary_path = get_expected_summary_path(yesterday)\n       assert os.path.exists(summary_path)\n       \n       # Verify summary content\n       with open(summary_path, \"r\") as f:\n           content = f.read()\n           assert yesterday in content\n           assert \"Daily Summary\" in content\n   ```\n\n3. **Test Manual Reflection Prioritization**:\n   ```python\n   def test_manual_reflection_prioritization():\n       # Create mock journal entries including manual reflections\n       mock_date = datetime.strptime(\"2023-05-18\", \"%Y-%m-%d\").date()\n       mock_entries = create_mock_journal_entries(mock_date, count=3)\n       \n       # Add manual reflections to one entry\n       mock_entries[1][\"reflection\"] = {\n           \"is_manual\": True,\n           \"title\": \"Test Reflection\",\n           \"content\": \"This is a manual reflection.\"\n       }\n       \n       # Generate summary\n       summary = synthesize_entries(mock_entries, mock_date)\n       \n       # Verify manual reflections are included and prioritized\n       assert \"manual_reflections\" in summary\n       assert len(summary[\"manual_reflections\"]) == 1\n       assert summary[\"manual_reflections\"][0][\"title\"] == \"Test Reflection\"\n       \n       # Test markdown formatting\n       markdown = format_summary_as_markdown(summary)\n       \n       # Verify manual reflections appear at the beginning with visual distinction\n       assert \"## 💭 Manual Reflections\" in markdown\n       assert \"<div class='manual-reflections'>\" in markdown\n       assert \"### Test Reflection\" in markdown\n       \n       # Verify manual reflections appear before other sections\n       manual_reflection_pos = markdown.find(\"## 💭 Manual Reflections\")\n       commit_activity_pos = markdown.find(\"## Commit Activity\")\n       assert manual_reflection_pos < commit_activity_pos\n   ```\n\n4. **Manual Testing Procedure**:\n   1. Install the application with the Git hook feature\n   2. Create several journal entries for \"yesterday\" (can be simulated by changing system date)\n   3. Include at least one manual reflection in the entries\n   4. Change the system date to \"today\"\n   5. Make a Git commit\n   6. Verify that a daily summary was generated for \"yesterday\"\n   7. Check that manual reflections are prominently displayed at the beginning\n   8. Verify the visual distinction of manual reflections\n   9. Check the summary content for accuracy and completeness\n\n5. **Edge Case Testing**:\n   ```python\n   def test_empty_day_handling():\n       # Test with a date that has no entries\n       empty_date = datetime.strptime(\"2000-01-01\", \"%Y-%m-%d\").date()\n       summary = generate_daily_summary(empty_date)\n       assert summary is None\n   \n   def test_malformed_entries():\n       # Create malformed journal entries\n       mock_date = datetime.strptime(\"2023-05-17\", \"%Y-%m-%d\").date()\n       create_malformed_journal_files(mock_date)\n       \n       # Test that the system handles malformed entries gracefully\n       try:\n           summary = generate_daily_summary(mock_date)\n           # Should either return a partial summary or None\n           if summary:\n               assert \"date\" in summary\n       except Exception as e:\n           assert False, f\"Should handle malformed entries without exception: {e}\"\n   ```\n\n6. **Performance Testing**:\n   - Test with a large number of journal entries (50+) for a single day\n   - Measure execution time and memory usage\n   - Ensure performance remains acceptable\n\n7. **Configuration Testing**:\n   - Test with different configuration settings\n   - Verify that customization options work as expected\n   - Test disabling the automatic trigger\n   - Test different styling options for manual reflections\n\n8. **Verification Checklist**:\n   - [ ] Git hook is properly installed during setup\n   - [ ] Hook correctly detects date changes\n   - [ ] Summary is generated for the correct date (previous day)\n   - [ ] Summary includes all journal entries from the target date\n   - [ ] Manual reflections are prioritized and displayed prominently at the beginning\n   - [ ] Manual reflections have visual distinction in the output\n   - [ ] Summary is saved to the expected location\n   - [ ] Error handling prevents Git operation disruption",
      "subtasks": []
    }
  ]
}